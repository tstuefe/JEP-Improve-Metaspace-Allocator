<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre>rev <a href="https://bugs.openjdk.java.net/browse/JDK-60221">60221</a> : imported patch big.patch</pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 2013, 2020, Red Hat, Inc. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "memory/allocation.hpp"
  27 #include "memory/universe.hpp"
  28 
  29 #include "gc/shared/gcArguments.hpp"
  30 #include "gc/shared/gcTimer.hpp"
  31 #include "gc/shared/gcTraceTime.inline.hpp"
  32 #include "gc/shared/locationPrinter.inline.hpp"
  33 #include "gc/shared/memAllocator.hpp"
  34 #include "gc/shared/oopStorageSet.hpp"
  35 #include "gc/shared/plab.hpp"
  36 
  37 #include "gc/shenandoah/shenandoahBarrierSet.hpp"
  38 #include "gc/shenandoah/shenandoahClosures.inline.hpp"
  39 #include "gc/shenandoah/shenandoahCollectionSet.hpp"
  40 #include "gc/shenandoah/shenandoahCollectorPolicy.hpp"
  41 #include "gc/shenandoah/shenandoahConcurrentMark.inline.hpp"
  42 #include "gc/shenandoah/shenandoahConcurrentRoots.hpp"
  43 #include "gc/shenandoah/shenandoahControlThread.hpp"
  44 #include "gc/shenandoah/shenandoahFreeSet.hpp"
  45 #include "gc/shenandoah/shenandoahPhaseTimings.hpp"
  46 #include "gc/shenandoah/shenandoahHeap.inline.hpp"
  47 #include "gc/shenandoah/shenandoahHeapRegion.inline.hpp"
  48 #include "gc/shenandoah/shenandoahHeapRegionSet.hpp"
  49 #include "gc/shenandoah/shenandoahInitLogger.hpp"
  50 #include "gc/shenandoah/shenandoahMarkCompact.hpp"
  51 #include "gc/shenandoah/shenandoahMarkingContext.inline.hpp"
  52 #include "gc/shenandoah/shenandoahMemoryPool.hpp"
  53 #include "gc/shenandoah/shenandoahMetrics.hpp"
  54 #include "gc/shenandoah/shenandoahMonitoringSupport.hpp"
  55 #include "gc/shenandoah/shenandoahOopClosures.inline.hpp"
  56 #include "gc/shenandoah/shenandoahPacer.inline.hpp"
  57 #include "gc/shenandoah/shenandoahPadding.hpp"
  58 #include "gc/shenandoah/shenandoahParallelCleaning.inline.hpp"
  59 #include "gc/shenandoah/shenandoahRootProcessor.inline.hpp"
  60 #include "gc/shenandoah/shenandoahStringDedup.hpp"
  61 #include "gc/shenandoah/shenandoahTaskqueue.hpp"
  62 #include "gc/shenandoah/shenandoahUtils.hpp"
  63 #include "gc/shenandoah/shenandoahVerifier.hpp"
  64 #include "gc/shenandoah/shenandoahCodeRoots.hpp"
  65 #include "gc/shenandoah/shenandoahVMOperations.hpp"
  66 #include "gc/shenandoah/shenandoahWorkGroup.hpp"
  67 #include "gc/shenandoah/shenandoahWorkerPolicy.hpp"
  68 #include "gc/shenandoah/mode/shenandoahIUMode.hpp"
  69 #include "gc/shenandoah/mode/shenandoahPassiveMode.hpp"
  70 #include "gc/shenandoah/mode/shenandoahSATBMode.hpp"
  71 #if INCLUDE_JFR
  72 #include "gc/shenandoah/shenandoahJfrSupport.hpp"
  73 #endif
  74 
<a name="1" id="anc1"></a><span class="changed">  75 </span>
<span class="changed">  76 #include "memory/metaspace/classLoaderMetaspace.hpp"</span>
<span class="changed">  77 #include "memory/metaspace/metaspaceEnums.hpp"</span>
  78 #include "oops/compressedOops.inline.hpp"
  79 #include "runtime/atomic.hpp"
  80 #include "runtime/globals.hpp"
  81 #include "runtime/interfaceSupport.inline.hpp"
  82 #include "runtime/orderAccess.hpp"
  83 #include "runtime/safepointMechanism.hpp"
  84 #include "runtime/vmThread.hpp"
  85 #include "services/mallocTracker.hpp"
  86 #include "utilities/powerOfTwo.hpp"
  87 
  88 #ifdef ASSERT
  89 template &lt;class T&gt;
  90 void ShenandoahAssertToSpaceClosure::do_oop_work(T* p) {
  91   T o = RawAccess&lt;&gt;::oop_load(p);
  92   if (! CompressedOops::is_null(o)) {
  93     oop obj = CompressedOops::decode_not_null(o);
  94     shenandoah_assert_not_forwarded(p, obj);
  95   }
  96 }
  97 
  98 void ShenandoahAssertToSpaceClosure::do_oop(narrowOop* p) { do_oop_work(p); }
  99 void ShenandoahAssertToSpaceClosure::do_oop(oop* p)       { do_oop_work(p); }
 100 #endif
 101 
 102 class ShenandoahPretouchHeapTask : public AbstractGangTask {
 103 private:
 104   ShenandoahRegionIterator _regions;
 105   const size_t _page_size;
 106 public:
 107   ShenandoahPretouchHeapTask(size_t page_size) :
 108     AbstractGangTask("Shenandoah Pretouch Heap"),
 109     _page_size(page_size) {}
 110 
 111   virtual void work(uint worker_id) {
 112     ShenandoahHeapRegion* r = _regions.next();
 113     while (r != NULL) {
 114       if (r-&gt;is_committed()) {
 115         os::pretouch_memory(r-&gt;bottom(), r-&gt;end(), _page_size);
 116       }
 117       r = _regions.next();
 118     }
 119   }
 120 };
 121 
 122 class ShenandoahPretouchBitmapTask : public AbstractGangTask {
 123 private:
 124   ShenandoahRegionIterator _regions;
 125   char* _bitmap_base;
 126   const size_t _bitmap_size;
 127   const size_t _page_size;
 128 public:
 129   ShenandoahPretouchBitmapTask(char* bitmap_base, size_t bitmap_size, size_t page_size) :
 130     AbstractGangTask("Shenandoah Pretouch Bitmap"),
 131     _bitmap_base(bitmap_base),
 132     _bitmap_size(bitmap_size),
 133     _page_size(page_size) {}
 134 
 135   virtual void work(uint worker_id) {
 136     ShenandoahHeapRegion* r = _regions.next();
 137     while (r != NULL) {
 138       size_t start = r-&gt;index()       * ShenandoahHeapRegion::region_size_bytes() / MarkBitMap::heap_map_factor();
 139       size_t end   = (r-&gt;index() + 1) * ShenandoahHeapRegion::region_size_bytes() / MarkBitMap::heap_map_factor();
 140       assert (end &lt;= _bitmap_size, "end is sane: " SIZE_FORMAT " &lt; " SIZE_FORMAT, end, _bitmap_size);
 141 
 142       if (r-&gt;is_committed()) {
 143         os::pretouch_memory(_bitmap_base + start, _bitmap_base + end, _page_size);
 144       }
 145 
 146       r = _regions.next();
 147     }
 148   }
 149 };
 150 
 151 jint ShenandoahHeap::initialize() {
 152   //
 153   // Figure out heap sizing
 154   //
 155 
 156   size_t init_byte_size = InitialHeapSize;
 157   size_t min_byte_size  = MinHeapSize;
 158   size_t max_byte_size  = MaxHeapSize;
 159   size_t heap_alignment = HeapAlignment;
 160 
 161   size_t reg_size_bytes = ShenandoahHeapRegion::region_size_bytes();
 162 
 163   Universe::check_alignment(max_byte_size,  reg_size_bytes, "Shenandoah heap");
 164   Universe::check_alignment(init_byte_size, reg_size_bytes, "Shenandoah heap");
 165 
 166   _num_regions = ShenandoahHeapRegion::region_count();
 167 
 168   // Now we know the number of regions, initialize the heuristics.
 169   initialize_heuristics();
 170 
 171   size_t num_committed_regions = init_byte_size / reg_size_bytes;
 172   num_committed_regions = MIN2(num_committed_regions, _num_regions);
 173   assert(num_committed_regions &lt;= _num_regions, "sanity");
 174   _initial_size = num_committed_regions * reg_size_bytes;
 175 
 176   size_t num_min_regions = min_byte_size / reg_size_bytes;
 177   num_min_regions = MIN2(num_min_regions, _num_regions);
 178   assert(num_min_regions &lt;= _num_regions, "sanity");
 179   _minimum_size = num_min_regions * reg_size_bytes;
 180 
 181   _committed = _initial_size;
 182 
 183   size_t heap_page_size   = UseLargePages ? (size_t)os::large_page_size() : (size_t)os::vm_page_size();
 184   size_t bitmap_page_size = UseLargePages ? (size_t)os::large_page_size() : (size_t)os::vm_page_size();
 185   size_t region_page_size = UseLargePages ? (size_t)os::large_page_size() : (size_t)os::vm_page_size();
 186 
 187   //
 188   // Reserve and commit memory for heap
 189   //
 190 
 191   ReservedHeapSpace heap_rs = Universe::reserve_heap(max_byte_size, heap_alignment);
 192   initialize_reserved_region(heap_rs);
 193   _heap_region = MemRegion((HeapWord*)heap_rs.base(), heap_rs.size() / HeapWordSize);
 194   _heap_region_special = heap_rs.special();
 195 
 196   assert((((size_t) base()) &amp; ShenandoahHeapRegion::region_size_bytes_mask()) == 0,
 197          "Misaligned heap: " PTR_FORMAT, p2i(base()));
 198 
 199 #if SHENANDOAH_OPTIMIZED_OBJTASK
 200   // The optimized ObjArrayChunkedTask takes some bits away from the full object bits.
 201   // Fail if we ever attempt to address more than we can.
 202   if ((uintptr_t)heap_rs.end() &gt;= ObjArrayChunkedTask::max_addressable()) {
 203     FormatBuffer&lt;512&gt; buf("Shenandoah reserved [" PTR_FORMAT ", " PTR_FORMAT") for the heap, \n"
 204                           "but max object address is " PTR_FORMAT ". Try to reduce heap size, or try other \n"
 205                           "VM options that allocate heap at lower addresses (HeapBaseMinAddress, AllocateHeapAt, etc).",
 206                 p2i(heap_rs.base()), p2i(heap_rs.end()), ObjArrayChunkedTask::max_addressable());
 207     vm_exit_during_initialization("Fatal Error", buf);
 208   }
 209 #endif
 210 
 211   ReservedSpace sh_rs = heap_rs.first_part(max_byte_size);
 212   if (!_heap_region_special) {
 213     os::commit_memory_or_exit(sh_rs.base(), _initial_size, heap_alignment, false,
 214                               "Cannot commit heap memory");
 215   }
 216 
 217   //
 218   // Reserve and commit memory for bitmap(s)
 219   //
 220 
 221   _bitmap_size = MarkBitMap::compute_size(heap_rs.size());
 222   _bitmap_size = align_up(_bitmap_size, bitmap_page_size);
 223 
 224   size_t bitmap_bytes_per_region = reg_size_bytes / MarkBitMap::heap_map_factor();
 225 
 226   guarantee(bitmap_bytes_per_region != 0,
 227             "Bitmap bytes per region should not be zero");
 228   guarantee(is_power_of_2(bitmap_bytes_per_region),
 229             "Bitmap bytes per region should be power of two: " SIZE_FORMAT, bitmap_bytes_per_region);
 230 
 231   if (bitmap_page_size &gt; bitmap_bytes_per_region) {
 232     _bitmap_regions_per_slice = bitmap_page_size / bitmap_bytes_per_region;
 233     _bitmap_bytes_per_slice = bitmap_page_size;
 234   } else {
 235     _bitmap_regions_per_slice = 1;
 236     _bitmap_bytes_per_slice = bitmap_bytes_per_region;
 237   }
 238 
 239   guarantee(_bitmap_regions_per_slice &gt;= 1,
 240             "Should have at least one region per slice: " SIZE_FORMAT,
 241             _bitmap_regions_per_slice);
 242 
 243   guarantee(((_bitmap_bytes_per_slice) % bitmap_page_size) == 0,
 244             "Bitmap slices should be page-granular: bps = " SIZE_FORMAT ", page size = " SIZE_FORMAT,
 245             _bitmap_bytes_per_slice, bitmap_page_size);
 246 
 247   ReservedSpace bitmap(_bitmap_size, bitmap_page_size);
 248   MemTracker::record_virtual_memory_type(bitmap.base(), mtGC);
 249   _bitmap_region = MemRegion((HeapWord*) bitmap.base(), bitmap.size() / HeapWordSize);
 250   _bitmap_region_special = bitmap.special();
 251 
 252   size_t bitmap_init_commit = _bitmap_bytes_per_slice *
 253                               align_up(num_committed_regions, _bitmap_regions_per_slice) / _bitmap_regions_per_slice;
 254   bitmap_init_commit = MIN2(_bitmap_size, bitmap_init_commit);
 255   if (!_bitmap_region_special) {
 256     os::commit_memory_or_exit((char *) _bitmap_region.start(), bitmap_init_commit, bitmap_page_size, false,
 257                               "Cannot commit bitmap memory");
 258   }
 259 
 260   _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions);
 261 
 262   if (ShenandoahVerify) {
 263     ReservedSpace verify_bitmap(_bitmap_size, bitmap_page_size);
 264     if (!verify_bitmap.special()) {
 265       os::commit_memory_or_exit(verify_bitmap.base(), verify_bitmap.size(), bitmap_page_size, false,
 266                                 "Cannot commit verification bitmap memory");
 267     }
 268     MemTracker::record_virtual_memory_type(verify_bitmap.base(), mtGC);
 269     MemRegion verify_bitmap_region = MemRegion((HeapWord *) verify_bitmap.base(), verify_bitmap.size() / HeapWordSize);
 270     _verification_bit_map.initialize(_heap_region, verify_bitmap_region);
 271     _verifier = new ShenandoahVerifier(this, &amp;_verification_bit_map);
 272   }
 273 
 274   // Reserve aux bitmap for use in object_iterate(). We don't commit it here.
 275   ReservedSpace aux_bitmap(_bitmap_size, bitmap_page_size);
 276   MemTracker::record_virtual_memory_type(aux_bitmap.base(), mtGC);
 277   _aux_bitmap_region = MemRegion((HeapWord*) aux_bitmap.base(), aux_bitmap.size() / HeapWordSize);
 278   _aux_bitmap_region_special = aux_bitmap.special();
 279   _aux_bit_map.initialize(_heap_region, _aux_bitmap_region);
 280 
 281   //
 282   // Create regions and region sets
 283   //
 284   size_t region_align = align_up(sizeof(ShenandoahHeapRegion), SHENANDOAH_CACHE_LINE_SIZE);
 285   size_t region_storage_size = align_up(region_align * _num_regions, region_page_size);
 286   region_storage_size = align_up(region_storage_size, os::vm_allocation_granularity());
 287 
 288   ReservedSpace region_storage(region_storage_size, region_page_size);
 289   MemTracker::record_virtual_memory_type(region_storage.base(), mtGC);
 290   if (!region_storage.special()) {
 291     os::commit_memory_or_exit(region_storage.base(), region_storage_size, region_page_size, false,
 292                               "Cannot commit region memory");
 293   }
 294 
 295   // Try to fit the collection set bitmap at lower addresses. This optimizes code generation for cset checks.
 296   // Go up until a sensible limit (subject to encoding constraints) and try to reserve the space there.
 297   // If not successful, bite a bullet and allocate at whatever address.
 298   {
 299     size_t cset_align = MAX2&lt;size_t&gt;(os::vm_page_size(), os::vm_allocation_granularity());
 300     size_t cset_size = align_up(((size_t) sh_rs.base() + sh_rs.size()) &gt;&gt; ShenandoahHeapRegion::region_size_bytes_shift(), cset_align);
 301 
 302     uintptr_t min = round_up_power_of_2(cset_align);
 303     uintptr_t max = (1u &lt;&lt; 30u);
 304 
 305     for (uintptr_t addr = min; addr &lt;= max; addr &lt;&lt;= 1u) {
 306       char* req_addr = (char*)addr;
 307       assert(is_aligned(req_addr, cset_align), "Should be aligned");
 308       ReservedSpace cset_rs(cset_size, cset_align, false, req_addr);
 309       if (cset_rs.is_reserved()) {
 310         assert(cset_rs.base() == req_addr, "Allocated where requested: " PTR_FORMAT ", " PTR_FORMAT, p2i(cset_rs.base()), addr);
 311         _collection_set = new ShenandoahCollectionSet(this, cset_rs, sh_rs.base());
 312         break;
 313       }
 314     }
 315 
 316     if (_collection_set == NULL) {
 317       ReservedSpace cset_rs(cset_size, cset_align, false);
 318       _collection_set = new ShenandoahCollectionSet(this, cset_rs, sh_rs.base());
 319     }
 320   }
 321 
 322   _regions = NEW_C_HEAP_ARRAY(ShenandoahHeapRegion*, _num_regions, mtGC);
 323   _free_set = new ShenandoahFreeSet(this, _num_regions);
 324 
 325   {
 326     ShenandoahHeapLocker locker(lock());
 327 
 328     for (size_t i = 0; i &lt; _num_regions; i++) {
 329       HeapWord* start = (HeapWord*)sh_rs.base() + ShenandoahHeapRegion::region_size_words() * i;
 330       bool is_committed = i &lt; num_committed_regions;
 331       void* loc = region_storage.base() + i * region_align;
 332 
 333       ShenandoahHeapRegion* r = new (loc) ShenandoahHeapRegion(start, i, is_committed);
 334       assert(is_aligned(r, SHENANDOAH_CACHE_LINE_SIZE), "Sanity");
 335 
 336       _marking_context-&gt;initialize_top_at_mark_start(r);
 337       _regions[i] = r;
 338       assert(!collection_set()-&gt;is_in(i), "New region should not be in collection set");
 339     }
 340 
 341     // Initialize to complete
 342     _marking_context-&gt;mark_complete();
 343 
 344     _free_set-&gt;rebuild();
 345   }
 346 
 347   if (AlwaysPreTouch) {
 348     // For NUMA, it is important to pre-touch the storage under bitmaps with worker threads,
 349     // before initialize() below zeroes it with initializing thread. For any given region,
 350     // we touch the region and the corresponding bitmaps from the same thread.
 351     ShenandoahPushWorkerScope scope(workers(), _max_workers, false);
 352 
 353     _pretouch_heap_page_size = heap_page_size;
 354     _pretouch_bitmap_page_size = bitmap_page_size;
 355 
 356 #ifdef LINUX
 357     // UseTransparentHugePages would madvise that backing memory can be coalesced into huge
 358     // pages. But, the kernel needs to know that every small page is used, in order to coalesce
 359     // them into huge one. Therefore, we need to pretouch with smaller pages.
 360     if (UseTransparentHugePages) {
 361       _pretouch_heap_page_size = (size_t)os::vm_page_size();
 362       _pretouch_bitmap_page_size = (size_t)os::vm_page_size();
 363     }
 364 #endif
 365 
 366     // OS memory managers may want to coalesce back-to-back pages. Make their jobs
 367     // simpler by pre-touching continuous spaces (heap and bitmap) separately.
 368 
 369     ShenandoahPretouchBitmapTask bcl(bitmap.base(), _bitmap_size, _pretouch_bitmap_page_size);
 370     _workers-&gt;run_task(&amp;bcl);
 371 
 372     ShenandoahPretouchHeapTask hcl(_pretouch_heap_page_size);
 373     _workers-&gt;run_task(&amp;hcl);
 374   }
 375 
 376   //
 377   // Initialize the rest of GC subsystems
 378   //
 379 
 380   _liveness_cache = NEW_C_HEAP_ARRAY(ShenandoahLiveData*, _max_workers, mtGC);
 381   for (uint worker = 0; worker &lt; _max_workers; worker++) {
 382     _liveness_cache[worker] = NEW_C_HEAP_ARRAY(ShenandoahLiveData, _num_regions, mtGC);
 383     Copy::fill_to_bytes(_liveness_cache[worker], _num_regions * sizeof(ShenandoahLiveData));
 384   }
 385 
 386   // There should probably be Shenandoah-specific options for these,
 387   // just as there are G1-specific options.
 388   {
 389     ShenandoahSATBMarkQueueSet&amp; satbqs = ShenandoahBarrierSet::satb_mark_queue_set();
 390     satbqs.set_process_completed_buffers_threshold(20); // G1SATBProcessCompletedThreshold
 391     satbqs.set_buffer_enqueue_threshold_percentage(60); // G1SATBBufferEnqueueingThresholdPercent
 392   }
 393 
 394   _monitoring_support = new ShenandoahMonitoringSupport(this);
 395   _phase_timings = new ShenandoahPhaseTimings(max_workers());
 396   ShenandoahStringDedup::initialize();
 397   ShenandoahCodeRoots::initialize();
 398 
 399   if (ShenandoahPacing) {
 400     _pacer = new ShenandoahPacer(this);
 401     _pacer-&gt;setup_for_idle();
 402   } else {
 403     _pacer = NULL;
 404   }
 405 
 406   _control_thread = new ShenandoahControlThread();
 407 
 408   _ref_proc_mt_processing = ParallelRefProcEnabled &amp;&amp; (ParallelGCThreads &gt; 1);
 409   _ref_proc_mt_discovery = _max_workers &gt; 1;
 410 
 411   ShenandoahInitLogger::print();
 412 
 413   return JNI_OK;
 414 }
 415 
 416 void ShenandoahHeap::initialize_heuristics() {
 417   if (ShenandoahGCMode != NULL) {
 418     if (strcmp(ShenandoahGCMode, "satb") == 0) {
 419       _gc_mode = new ShenandoahSATBMode();
 420     } else if (strcmp(ShenandoahGCMode, "iu") == 0) {
 421       _gc_mode = new ShenandoahIUMode();
 422     } else if (strcmp(ShenandoahGCMode, "passive") == 0) {
 423       _gc_mode = new ShenandoahPassiveMode();
 424     } else {
 425       vm_exit_during_initialization("Unknown -XX:ShenandoahGCMode option");
 426     }
 427   } else {
 428     ShouldNotReachHere();
 429   }
 430   _gc_mode-&gt;initialize_flags();
 431   if (_gc_mode-&gt;is_diagnostic() &amp;&amp; !UnlockDiagnosticVMOptions) {
 432     vm_exit_during_initialization(
 433             err_msg("GC mode \"%s\" is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.",
 434                     _gc_mode-&gt;name()));
 435   }
 436   if (_gc_mode-&gt;is_experimental() &amp;&amp; !UnlockExperimentalVMOptions) {
 437     vm_exit_during_initialization(
 438             err_msg("GC mode \"%s\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.",
 439                     _gc_mode-&gt;name()));
 440   }
 441 
 442   _heuristics = _gc_mode-&gt;initialize_heuristics();
 443 
 444   if (_heuristics-&gt;is_diagnostic() &amp;&amp; !UnlockDiagnosticVMOptions) {
 445     vm_exit_during_initialization(
 446             err_msg("Heuristics \"%s\" is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.",
 447                     _heuristics-&gt;name()));
 448   }
 449   if (_heuristics-&gt;is_experimental() &amp;&amp; !UnlockExperimentalVMOptions) {
 450     vm_exit_during_initialization(
 451             err_msg("Heuristics \"%s\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.",
 452                     _heuristics-&gt;name()));
 453   }
 454 }
 455 
 456 #ifdef _MSC_VER
 457 #pragma warning( push )
 458 #pragma warning( disable:4355 ) // 'this' : used in base member initializer list
 459 #endif
 460 
 461 ShenandoahHeap::ShenandoahHeap(ShenandoahCollectorPolicy* policy) :
 462   CollectedHeap(),
 463   _initial_size(0),
 464   _used(0),
 465   _committed(0),
 466   _bytes_allocated_since_gc_start(0),
 467   _max_workers(MAX2(ConcGCThreads, ParallelGCThreads)),
 468   _workers(NULL),
 469   _safepoint_workers(NULL),
 470   _heap_region_special(false),
 471   _num_regions(0),
 472   _regions(NULL),
 473   _update_refs_iterator(this),
 474   _control_thread(NULL),
 475   _shenandoah_policy(policy),
 476   _heuristics(NULL),
 477   _free_set(NULL),
 478   _scm(new ShenandoahConcurrentMark()),
 479   _full_gc(new ShenandoahMarkCompact()),
 480   _pacer(NULL),
 481   _verifier(NULL),
 482   _phase_timings(NULL),
 483   _monitoring_support(NULL),
 484   _memory_pool(NULL),
 485   _stw_memory_manager("Shenandoah Pauses", "end of GC pause"),
 486   _cycle_memory_manager("Shenandoah Cycles", "end of GC cycle"),
 487   _gc_timer(new (ResourceObj::C_HEAP, mtGC) ConcurrentGCTimer()),
 488   _soft_ref_policy(),
 489   _log_min_obj_alignment_in_bytes(LogMinObjAlignmentInBytes),
 490   _ref_processor(NULL),
 491   _marking_context(NULL),
 492   _bitmap_size(0),
 493   _bitmap_regions_per_slice(0),
 494   _bitmap_bytes_per_slice(0),
 495   _bitmap_region_special(false),
 496   _aux_bitmap_region_special(false),
 497   _liveness_cache(NULL),
 498   _collection_set(NULL)
 499 {
 500   BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this));
 501 
 502   _max_workers = MAX2(_max_workers, 1U);
 503   _workers = new ShenandoahWorkGang("Shenandoah GC Threads", _max_workers,
 504                             /* are_GC_task_threads */ true,
 505                             /* are_ConcurrentGC_threads */ true);
 506   if (_workers == NULL) {
 507     vm_exit_during_initialization("Failed necessary allocation.");
 508   } else {
 509     _workers-&gt;initialize_workers();
 510   }
 511 
 512   if (ParallelGCThreads &gt; 1) {
 513     _safepoint_workers = new ShenandoahWorkGang("Safepoint Cleanup Thread",
 514                                                 ParallelGCThreads,
 515                       /* are_GC_task_threads */ false,
 516                  /* are_ConcurrentGC_threads */ false);
 517     _safepoint_workers-&gt;initialize_workers();
 518   }
 519 }
 520 
 521 #ifdef _MSC_VER
 522 #pragma warning( pop )
 523 #endif
 524 
 525 class ShenandoahResetBitmapTask : public AbstractGangTask {
 526 private:
 527   ShenandoahRegionIterator _regions;
 528 
 529 public:
 530   ShenandoahResetBitmapTask() :
 531     AbstractGangTask("Parallel Reset Bitmap Task") {}
 532 
 533   void work(uint worker_id) {
 534     ShenandoahHeapRegion* region = _regions.next();
 535     ShenandoahHeap* heap = ShenandoahHeap::heap();
 536     ShenandoahMarkingContext* const ctx = heap-&gt;marking_context();
 537     while (region != NULL) {
 538       if (heap-&gt;is_bitmap_slice_committed(region)) {
 539         ctx-&gt;clear_bitmap(region);
 540       }
 541       region = _regions.next();
 542     }
 543   }
 544 };
 545 
 546 void ShenandoahHeap::reset_mark_bitmap() {
 547   assert_gc_workers(_workers-&gt;active_workers());
 548   mark_incomplete_marking_context();
 549 
 550   ShenandoahResetBitmapTask task;
 551   _workers-&gt;run_task(&amp;task);
 552 }
 553 
 554 void ShenandoahHeap::print_on(outputStream* st) const {
 555   st-&gt;print_cr("Shenandoah Heap");
 556   st-&gt;print_cr(" " SIZE_FORMAT "%s total, " SIZE_FORMAT "%s committed, " SIZE_FORMAT "%s used",
 557                byte_size_in_proper_unit(max_capacity()), proper_unit_for_byte_size(max_capacity()),
 558                byte_size_in_proper_unit(committed()),    proper_unit_for_byte_size(committed()),
 559                byte_size_in_proper_unit(used()),         proper_unit_for_byte_size(used()));
 560   st-&gt;print_cr(" " SIZE_FORMAT " x " SIZE_FORMAT"%s regions",
 561                num_regions(),
 562                byte_size_in_proper_unit(ShenandoahHeapRegion::region_size_bytes()),
 563                proper_unit_for_byte_size(ShenandoahHeapRegion::region_size_bytes()));
 564 
 565   st-&gt;print("Status: ");
 566   if (has_forwarded_objects())                 st-&gt;print("has forwarded objects, ");
 567   if (is_concurrent_mark_in_progress())        st-&gt;print("marking, ");
 568   if (is_evacuation_in_progress())             st-&gt;print("evacuating, ");
 569   if (is_update_refs_in_progress())            st-&gt;print("updating refs, ");
 570   if (is_degenerated_gc_in_progress())         st-&gt;print("degenerated gc, ");
 571   if (is_full_gc_in_progress())                st-&gt;print("full gc, ");
 572   if (is_full_gc_move_in_progress())           st-&gt;print("full gc move, ");
 573   if (is_concurrent_weak_root_in_progress())   st-&gt;print("concurrent weak roots, ");
 574   if (is_concurrent_strong_root_in_progress() &amp;&amp;
 575       !is_concurrent_weak_root_in_progress())  st-&gt;print("concurrent strong roots, ");
 576 
 577   if (cancelled_gc()) {
 578     st-&gt;print("cancelled");
 579   } else {
 580     st-&gt;print("not cancelled");
 581   }
 582   st-&gt;cr();
 583 
 584   st-&gt;print_cr("Reserved region:");
 585   st-&gt;print_cr(" - [" PTR_FORMAT ", " PTR_FORMAT ") ",
 586                p2i(reserved_region().start()),
 587                p2i(reserved_region().end()));
 588 
 589   ShenandoahCollectionSet* cset = collection_set();
 590   st-&gt;print_cr("Collection set:");
 591   if (cset != NULL) {
 592     st-&gt;print_cr(" - map (vanilla): " PTR_FORMAT, p2i(cset-&gt;map_address()));
 593     st-&gt;print_cr(" - map (biased):  " PTR_FORMAT, p2i(cset-&gt;biased_map_address()));
 594   } else {
 595     st-&gt;print_cr(" (NULL)");
 596   }
 597 
 598   st-&gt;cr();
 599   MetaspaceUtils::print_on(st);
 600 
 601   if (Verbose) {
 602     print_heap_regions_on(st);
 603   }
 604 }
 605 
 606 class ShenandoahInitWorkerGCLABClosure : public ThreadClosure {
 607 public:
 608   void do_thread(Thread* thread) {
 609     assert(thread != NULL, "Sanity");
 610     assert(thread-&gt;is_Worker_thread(), "Only worker thread expected");
 611     ShenandoahThreadLocalData::initialize_gclab(thread);
 612   }
 613 };
 614 
 615 void ShenandoahHeap::post_initialize() {
 616   CollectedHeap::post_initialize();
 617   MutexLocker ml(Threads_lock);
 618 
 619   ShenandoahInitWorkerGCLABClosure init_gclabs;
 620   _workers-&gt;threads_do(&amp;init_gclabs);
 621 
 622   // gclab can not be initialized early during VM startup, as it can not determinate its max_size.
 623   // Now, we will let WorkGang to initialize gclab when new worker is created.
 624   _workers-&gt;set_initialize_gclab();
 625 
 626   _scm-&gt;initialize(_max_workers);
 627   _full_gc-&gt;initialize(_gc_timer);
 628 
 629   ref_processing_init();
 630 
 631   _heuristics-&gt;initialize();
 632 
 633   JFR_ONLY(ShenandoahJFRSupport::register_jfr_type_serializers());
 634 }
 635 
 636 size_t ShenandoahHeap::used() const {
 637   return Atomic::load_acquire(&amp;_used);
 638 }
 639 
 640 size_t ShenandoahHeap::committed() const {
 641   OrderAccess::acquire();
 642   return _committed;
 643 }
 644 
 645 void ShenandoahHeap::increase_committed(size_t bytes) {
 646   shenandoah_assert_heaplocked_or_safepoint();
 647   _committed += bytes;
 648 }
 649 
 650 void ShenandoahHeap::decrease_committed(size_t bytes) {
 651   shenandoah_assert_heaplocked_or_safepoint();
 652   _committed -= bytes;
 653 }
 654 
 655 void ShenandoahHeap::increase_used(size_t bytes) {
 656   Atomic::add(&amp;_used, bytes);
 657 }
 658 
 659 void ShenandoahHeap::set_used(size_t bytes) {
 660   Atomic::release_store_fence(&amp;_used, bytes);
 661 }
 662 
 663 void ShenandoahHeap::decrease_used(size_t bytes) {
 664   assert(used() &gt;= bytes, "never decrease heap size by more than we've left");
 665   Atomic::sub(&amp;_used, bytes);
 666 }
 667 
 668 void ShenandoahHeap::increase_allocated(size_t bytes) {
 669   Atomic::add(&amp;_bytes_allocated_since_gc_start, bytes);
 670 }
 671 
 672 void ShenandoahHeap::notify_mutator_alloc_words(size_t words, bool waste) {
 673   size_t bytes = words * HeapWordSize;
 674   if (!waste) {
 675     increase_used(bytes);
 676   }
 677   increase_allocated(bytes);
 678   if (ShenandoahPacing) {
 679     control_thread()-&gt;pacing_notify_alloc(words);
 680     if (waste) {
 681       pacer()-&gt;claim_for_alloc(words, true);
 682     }
 683   }
 684 }
 685 
 686 size_t ShenandoahHeap::capacity() const {
 687   return committed();
 688 }
 689 
 690 size_t ShenandoahHeap::max_capacity() const {
 691   return _num_regions * ShenandoahHeapRegion::region_size_bytes();
 692 }
 693 
 694 size_t ShenandoahHeap::min_capacity() const {
 695   return _minimum_size;
 696 }
 697 
 698 size_t ShenandoahHeap::initial_capacity() const {
 699   return _initial_size;
 700 }
 701 
 702 bool ShenandoahHeap::is_in(const void* p) const {
 703   HeapWord* heap_base = (HeapWord*) base();
 704   HeapWord* last_region_end = heap_base + ShenandoahHeapRegion::region_size_words() * num_regions();
 705   return p &gt;= heap_base &amp;&amp; p &lt; last_region_end;
 706 }
 707 
 708 void ShenandoahHeap::op_uncommit(double shrink_before) {
 709   assert (ShenandoahUncommit, "should be enabled");
 710 
 711   // Application allocates from the beginning of the heap, and GC allocates at
 712   // the end of it. It is more efficient to uncommit from the end, so that applications
 713   // could enjoy the near committed regions. GC allocations are much less frequent,
 714   // and therefore can accept the committing costs.
 715 
 716   size_t count = 0;
 717   for (size_t i = num_regions(); i &gt; 0; i--) { // care about size_t underflow
 718     ShenandoahHeapRegion* r = get_region(i - 1);
 719     if (r-&gt;is_empty_committed() &amp;&amp; (r-&gt;empty_time() &lt; shrink_before)) {
 720       ShenandoahHeapLocker locker(lock());
 721       if (r-&gt;is_empty_committed()) {
 722         // Do not uncommit below minimal capacity
 723         if (committed() &lt; min_capacity() + ShenandoahHeapRegion::region_size_bytes()) {
 724           break;
 725         }
 726 
 727         r-&gt;make_uncommitted();
 728         count++;
 729       }
 730     }
 731     SpinPause(); // allow allocators to take the lock
 732   }
 733 
 734   if (count &gt; 0) {
 735     control_thread()-&gt;notify_heap_changed();
 736   }
 737 }
 738 
 739 HeapWord* ShenandoahHeap::allocate_from_gclab_slow(Thread* thread, size_t size) {
 740   // New object should fit the GCLAB size
 741   size_t min_size = MAX2(size, PLAB::min_size());
 742 
 743   // Figure out size of new GCLAB, looking back at heuristics. Expand aggressively.
 744   size_t new_size = ShenandoahThreadLocalData::gclab_size(thread) * 2;
 745   new_size = MIN2(new_size, PLAB::max_size());
 746   new_size = MAX2(new_size, PLAB::min_size());
 747 
 748   // Record new heuristic value even if we take any shortcut. This captures
 749   // the case when moderately-sized objects always take a shortcut. At some point,
 750   // heuristics should catch up with them.
 751   ShenandoahThreadLocalData::set_gclab_size(thread, new_size);
 752 
 753   if (new_size &lt; size) {
 754     // New size still does not fit the object. Fall back to shared allocation.
 755     // This avoids retiring perfectly good GCLABs, when we encounter a large object.
 756     return NULL;
 757   }
 758 
 759   // Retire current GCLAB, and allocate a new one.
 760   PLAB* gclab = ShenandoahThreadLocalData::gclab(thread);
 761   gclab-&gt;retire();
 762 
 763   size_t actual_size = 0;
 764   HeapWord* gclab_buf = allocate_new_gclab(min_size, new_size, &amp;actual_size);
 765   if (gclab_buf == NULL) {
 766     return NULL;
 767   }
 768 
 769   assert (size &lt;= actual_size, "allocation should fit");
 770 
 771   if (ZeroTLAB) {
 772     // ..and clear it.
 773     Copy::zero_to_words(gclab_buf, actual_size);
 774   } else {
 775     // ...and zap just allocated object.
 776 #ifdef ASSERT
 777     // Skip mangling the space corresponding to the object header to
 778     // ensure that the returned space is not considered parsable by
 779     // any concurrent GC thread.
 780     size_t hdr_size = oopDesc::header_size();
 781     Copy::fill_to_words(gclab_buf + hdr_size, actual_size - hdr_size, badHeapWordVal);
 782 #endif // ASSERT
 783   }
 784   gclab-&gt;set_buf(gclab_buf, actual_size);
 785   return gclab-&gt;allocate(size);
 786 }
 787 
 788 HeapWord* ShenandoahHeap::allocate_new_tlab(size_t min_size,
 789                                             size_t requested_size,
 790                                             size_t* actual_size) {
 791   ShenandoahAllocRequest req = ShenandoahAllocRequest::for_tlab(min_size, requested_size);
 792   HeapWord* res = allocate_memory(req);
 793   if (res != NULL) {
 794     *actual_size = req.actual_size();
 795   } else {
 796     *actual_size = 0;
 797   }
 798   return res;
 799 }
 800 
 801 HeapWord* ShenandoahHeap::allocate_new_gclab(size_t min_size,
 802                                              size_t word_size,
 803                                              size_t* actual_size) {
 804   ShenandoahAllocRequest req = ShenandoahAllocRequest::for_gclab(min_size, word_size);
 805   HeapWord* res = allocate_memory(req);
 806   if (res != NULL) {
 807     *actual_size = req.actual_size();
 808   } else {
 809     *actual_size = 0;
 810   }
 811   return res;
 812 }
 813 
 814 HeapWord* ShenandoahHeap::allocate_memory(ShenandoahAllocRequest&amp; req) {
 815   intptr_t pacer_epoch = 0;
 816   bool in_new_region = false;
 817   HeapWord* result = NULL;
 818 
 819   if (req.is_mutator_alloc()) {
 820     if (ShenandoahPacing) {
 821       pacer()-&gt;pace_for_alloc(req.size());
 822       pacer_epoch = pacer()-&gt;epoch();
 823     }
 824 
 825     if (!ShenandoahAllocFailureALot || !should_inject_alloc_failure()) {
 826       result = allocate_memory_under_lock(req, in_new_region);
 827     }
 828 
 829     // Allocation failed, block until control thread reacted, then retry allocation.
 830     //
 831     // It might happen that one of the threads requesting allocation would unblock
 832     // way later after GC happened, only to fail the second allocation, because
 833     // other threads have already depleted the free storage. In this case, a better
 834     // strategy is to try again, as long as GC makes progress.
 835     //
 836     // Then, we need to make sure the allocation was retried after at least one
 837     // Full GC, which means we want to try more than ShenandoahFullGCThreshold times.
 838 
 839     size_t tries = 0;
 840 
 841     while (result == NULL &amp;&amp; _progress_last_gc.is_set()) {
 842       tries++;
 843       control_thread()-&gt;handle_alloc_failure(req);
 844       result = allocate_memory_under_lock(req, in_new_region);
 845     }
 846 
 847     while (result == NULL &amp;&amp; tries &lt;= ShenandoahFullGCThreshold) {
 848       tries++;
 849       control_thread()-&gt;handle_alloc_failure(req);
 850       result = allocate_memory_under_lock(req, in_new_region);
 851     }
 852 
 853   } else {
 854     assert(req.is_gc_alloc(), "Can only accept GC allocs here");
 855     result = allocate_memory_under_lock(req, in_new_region);
 856     // Do not call handle_alloc_failure() here, because we cannot block.
 857     // The allocation failure would be handled by the LRB slowpath with handle_alloc_failure_evac().
 858   }
 859 
 860   if (in_new_region) {
 861     control_thread()-&gt;notify_heap_changed();
 862   }
 863 
 864   if (result != NULL) {
 865     size_t requested = req.size();
 866     size_t actual = req.actual_size();
 867 
 868     assert (req.is_lab_alloc() || (requested == actual),
 869             "Only LAB allocations are elastic: %s, requested = " SIZE_FORMAT ", actual = " SIZE_FORMAT,
 870             ShenandoahAllocRequest::alloc_type_to_string(req.type()), requested, actual);
 871 
 872     if (req.is_mutator_alloc()) {
 873       notify_mutator_alloc_words(actual, false);
 874 
 875       // If we requested more than we were granted, give the rest back to pacer.
 876       // This only matters if we are in the same pacing epoch: do not try to unpace
 877       // over the budget for the other phase.
 878       if (ShenandoahPacing &amp;&amp; (pacer_epoch &gt; 0) &amp;&amp; (requested &gt; actual)) {
 879         pacer()-&gt;unpace_for_alloc(pacer_epoch, requested - actual);
 880       }
 881     } else {
 882       increase_used(actual*HeapWordSize);
 883     }
 884   }
 885 
 886   return result;
 887 }
 888 
 889 HeapWord* ShenandoahHeap::allocate_memory_under_lock(ShenandoahAllocRequest&amp; req, bool&amp; in_new_region) {
 890   ShenandoahHeapLocker locker(lock());
 891   return _free_set-&gt;allocate(req, in_new_region);
 892 }
 893 
 894 HeapWord* ShenandoahHeap::mem_allocate(size_t size,
 895                                         bool*  gc_overhead_limit_was_exceeded) {
 896   ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared(size);
 897   return allocate_memory(req);
 898 }
 899 
 900 MetaWord* ShenandoahHeap::satisfy_failed_metadata_allocation(ClassLoaderData* loader_data,
 901                                                              size_t size,
 902                                                              Metaspace::MetadataType mdtype) {
 903   MetaWord* result;
 904 
 905   // Inform metaspace OOM to GC heuristics if class unloading is possible.
 906   if (heuristics()-&gt;can_unload_classes()) {
 907     ShenandoahHeuristics* h = heuristics();
 908     h-&gt;record_metaspace_oom();
 909   }
 910 
 911   // Expand and retry allocation
 912   result = loader_data-&gt;metaspace_non_null()-&gt;expand_and_allocate(size, mdtype);
 913   if (result != NULL) {
 914     return result;
 915   }
 916 
 917   // Start full GC
 918   collect(GCCause::_metadata_GC_clear_soft_refs);
 919 
 920   // Retry allocation
 921   result = loader_data-&gt;metaspace_non_null()-&gt;allocate(size, mdtype);
 922   if (result != NULL) {
 923     return result;
 924   }
 925 
 926   // Expand and retry allocation
 927   result = loader_data-&gt;metaspace_non_null()-&gt;expand_and_allocate(size, mdtype);
 928   if (result != NULL) {
 929     return result;
 930   }
 931 
 932   // Out of memory
 933   return NULL;
 934 }
 935 
 936 class ShenandoahConcurrentEvacuateRegionObjectClosure : public ObjectClosure {
 937 private:
 938   ShenandoahHeap* const _heap;
 939   Thread* const _thread;
 940 public:
 941   ShenandoahConcurrentEvacuateRegionObjectClosure(ShenandoahHeap* heap) :
 942     _heap(heap), _thread(Thread::current()) {}
 943 
 944   void do_object(oop p) {
 945     shenandoah_assert_marked(NULL, p);
 946     if (!p-&gt;is_forwarded()) {
 947       _heap-&gt;evacuate_object(p, _thread);
 948     }
 949   }
 950 };
 951 
 952 class ShenandoahEvacuationTask : public AbstractGangTask {
 953 private:
 954   ShenandoahHeap* const _sh;
 955   ShenandoahCollectionSet* const _cs;
 956   bool _concurrent;
 957 public:
 958   ShenandoahEvacuationTask(ShenandoahHeap* sh,
 959                            ShenandoahCollectionSet* cs,
 960                            bool concurrent) :
 961     AbstractGangTask("Parallel Evacuation Task"),
 962     _sh(sh),
 963     _cs(cs),
 964     _concurrent(concurrent)
 965   {}
 966 
 967   void work(uint worker_id) {
 968     if (_concurrent) {
 969       ShenandoahConcurrentWorkerSession worker_session(worker_id);
 970       ShenandoahSuspendibleThreadSetJoiner stsj(ShenandoahSuspendibleWorkers);
 971       ShenandoahEvacOOMScope oom_evac_scope;
 972       do_work();
 973     } else {
 974       ShenandoahParallelWorkerSession worker_session(worker_id);
 975       ShenandoahEvacOOMScope oom_evac_scope;
 976       do_work();
 977     }
 978   }
 979 
 980 private:
 981   void do_work() {
 982     ShenandoahConcurrentEvacuateRegionObjectClosure cl(_sh);
 983     ShenandoahHeapRegion* r;
 984     while ((r =_cs-&gt;claim_next()) != NULL) {
 985       assert(r-&gt;has_live(), "Region " SIZE_FORMAT " should have been reclaimed early", r-&gt;index());
 986       _sh-&gt;marked_object_iterate(r, &amp;cl);
 987 
 988       if (ShenandoahPacing) {
 989         _sh-&gt;pacer()-&gt;report_evac(r-&gt;used() &gt;&gt; LogHeapWordSize);
 990       }
 991 
 992       if (_sh-&gt;check_cancelled_gc_and_yield(_concurrent)) {
 993         break;
 994       }
 995     }
 996   }
 997 };
 998 
 999 void ShenandoahHeap::trash_cset_regions() {
1000   ShenandoahHeapLocker locker(lock());
1001 
1002   ShenandoahCollectionSet* set = collection_set();
1003   ShenandoahHeapRegion* r;
1004   set-&gt;clear_current_index();
1005   while ((r = set-&gt;next()) != NULL) {
1006     r-&gt;make_trash();
1007   }
1008   collection_set()-&gt;clear();
1009 }
1010 
1011 void ShenandoahHeap::print_heap_regions_on(outputStream* st) const {
1012   st-&gt;print_cr("Heap Regions:");
1013   st-&gt;print_cr("EU=empty-uncommitted, EC=empty-committed, R=regular, H=humongous start, HC=humongous continuation, CS=collection set, T=trash, P=pinned");
1014   st-&gt;print_cr("BTE=bottom/top/end, U=used, T=TLAB allocs, G=GCLAB allocs, S=shared allocs, L=live data");
1015   st-&gt;print_cr("R=root, CP=critical pins, TAMS=top-at-mark-start, UWM=update watermark");
1016   st-&gt;print_cr("SN=alloc sequence number");
1017 
1018   for (size_t i = 0; i &lt; num_regions(); i++) {
1019     get_region(i)-&gt;print_on(st);
1020   }
1021 }
1022 
1023 void ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {
1024   assert(start-&gt;is_humongous_start(), "reclaim regions starting with the first one");
1025 
1026   oop humongous_obj = oop(start-&gt;bottom());
1027   size_t size = humongous_obj-&gt;size();
1028   size_t required_regions = ShenandoahHeapRegion::required_regions(size * HeapWordSize);
1029   size_t index = start-&gt;index() + required_regions - 1;
1030 
1031   assert(!start-&gt;has_live(), "liveness must be zero");
1032 
1033   for(size_t i = 0; i &lt; required_regions; i++) {
1034     // Reclaim from tail. Otherwise, assertion fails when printing region to trace log,
1035     // as it expects that every region belongs to a humongous region starting with a humongous start region.
1036     ShenandoahHeapRegion* region = get_region(index --);
1037 
1038     assert(region-&gt;is_humongous(), "expect correct humongous start or continuation");
1039     assert(!region-&gt;is_cset(), "Humongous region should not be in collection set");
1040 
1041     region-&gt;make_trash_immediate();
1042   }
1043 }
1044 
1045 class ShenandoahCheckCleanGCLABClosure : public ThreadClosure {
1046 public:
1047   ShenandoahCheckCleanGCLABClosure() {}
1048   void do_thread(Thread* thread) {
1049     PLAB* gclab = ShenandoahThreadLocalData::gclab(thread);
1050     assert(gclab != NULL, "GCLAB should be initialized for %s", thread-&gt;name());
1051     assert(gclab-&gt;words_remaining() == 0, "GCLAB should not need retirement");
1052   }
1053 };
1054 
1055 class ShenandoahRetireGCLABClosure : public ThreadClosure {
1056 private:
1057   bool const _resize;
1058 public:
1059   ShenandoahRetireGCLABClosure(bool resize) : _resize(resize) {}
1060   void do_thread(Thread* thread) {
1061     PLAB* gclab = ShenandoahThreadLocalData::gclab(thread);
1062     assert(gclab != NULL, "GCLAB should be initialized for %s", thread-&gt;name());
1063     gclab-&gt;retire();
1064     if (_resize &amp;&amp; ShenandoahThreadLocalData::gclab_size(thread) &gt; 0) {
1065       ShenandoahThreadLocalData::set_gclab_size(thread, 0);
1066     }
1067   }
1068 };
1069 
1070 void ShenandoahHeap::labs_make_parsable() {
1071   assert(UseTLAB, "Only call with UseTLAB");
1072 
1073   ShenandoahRetireGCLABClosure cl(false);
1074 
1075   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {
1076     ThreadLocalAllocBuffer&amp; tlab = t-&gt;tlab();
1077     tlab.make_parsable();
1078     cl.do_thread(t);
1079   }
1080 
1081   workers()-&gt;threads_do(&amp;cl);
1082 }
1083 
1084 void ShenandoahHeap::tlabs_retire(bool resize) {
1085   assert(UseTLAB, "Only call with UseTLAB");
1086   assert(!resize || ResizeTLAB, "Only call for resize when ResizeTLAB is enabled");
1087 
1088   ThreadLocalAllocStats stats;
1089 
1090   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {
1091     ThreadLocalAllocBuffer&amp; tlab = t-&gt;tlab();
1092     tlab.retire(&amp;stats);
1093     if (resize) {
1094       tlab.resize();
1095     }
1096   }
1097 
1098   stats.publish();
1099 
1100 #ifdef ASSERT
1101   ShenandoahCheckCleanGCLABClosure cl;
1102   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {
1103     cl.do_thread(t);
1104   }
1105   workers()-&gt;threads_do(&amp;cl);
1106 #endif
1107 }
1108 
1109 void ShenandoahHeap::gclabs_retire(bool resize) {
1110   assert(UseTLAB, "Only call with UseTLAB");
1111   assert(!resize || ResizeTLAB, "Only call for resize when ResizeTLAB is enabled");
1112 
1113   ShenandoahRetireGCLABClosure cl(resize);
1114   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {
1115     cl.do_thread(t);
1116   }
1117   workers()-&gt;threads_do(&amp;cl);
1118 }
1119 
1120 class ShenandoahEvacuateUpdateRootsTask : public AbstractGangTask {
1121 private:
1122   ShenandoahRootEvacuator* _rp;
1123 
1124 public:
1125   ShenandoahEvacuateUpdateRootsTask(ShenandoahRootEvacuator* rp) :
1126     AbstractGangTask("Shenandoah evacuate and update roots"),
1127     _rp(rp) {}
1128 
1129   void work(uint worker_id) {
1130     ShenandoahParallelWorkerSession worker_session(worker_id);
1131     ShenandoahEvacOOMScope oom_evac_scope;
1132     ShenandoahEvacuateUpdateRootsClosure&lt;&gt; cl;
1133     MarkingCodeBlobClosure blobsCl(&amp;cl, CodeBlobToOopClosure::FixRelocations);
1134     _rp-&gt;roots_do(worker_id, &amp;cl);
1135   }
1136 };
1137 
1138 void ShenandoahHeap::evacuate_and_update_roots() {
1139 #if COMPILER2_OR_JVMCI
1140   DerivedPointerTable::clear();
1141 #endif
1142   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), "Only iterate roots while world is stopped");
1143   {
1144     // Include concurrent roots if current cycle can not process those roots concurrently
1145     ShenandoahRootEvacuator rp(workers()-&gt;active_workers(),
1146                                ShenandoahPhaseTimings::init_evac,
1147                                !ShenandoahConcurrentRoots::should_do_concurrent_roots(),
1148                                !ShenandoahConcurrentRoots::should_do_concurrent_class_unloading());
1149     ShenandoahEvacuateUpdateRootsTask roots_task(&amp;rp);
1150     workers()-&gt;run_task(&amp;roots_task);
1151   }
1152 
1153 #if COMPILER2_OR_JVMCI
1154   DerivedPointerTable::update_pointers();
1155 #endif
1156 }
1157 
1158 // Returns size in bytes
1159 size_t ShenandoahHeap::unsafe_max_tlab_alloc(Thread *thread) const {
1160   if (ShenandoahElasticTLAB) {
1161     // With Elastic TLABs, return the max allowed size, and let the allocation path
1162     // figure out the safe size for current allocation.
1163     return ShenandoahHeapRegion::max_tlab_size_bytes();
1164   } else {
1165     return MIN2(_free_set-&gt;unsafe_peek_free(), ShenandoahHeapRegion::max_tlab_size_bytes());
1166   }
1167 }
1168 
1169 size_t ShenandoahHeap::max_tlab_size() const {
1170   // Returns size in words
1171   return ShenandoahHeapRegion::max_tlab_size_words();
1172 }
1173 
1174 void ShenandoahHeap::collect(GCCause::Cause cause) {
1175   control_thread()-&gt;request_gc(cause);
1176 }
1177 
1178 void ShenandoahHeap::do_full_collection(bool clear_all_soft_refs) {
1179   //assert(false, "Shouldn't need to do full collections");
1180 }
1181 
1182 HeapWord* ShenandoahHeap::block_start(const void* addr) const {
1183   ShenandoahHeapRegion* r = heap_region_containing(addr);
1184   if (r != NULL) {
1185     return r-&gt;block_start(addr);
1186   }
1187   return NULL;
1188 }
1189 
1190 bool ShenandoahHeap::block_is_obj(const HeapWord* addr) const {
1191   ShenandoahHeapRegion* r = heap_region_containing(addr);
1192   return r-&gt;block_is_obj(addr);
1193 }
1194 
1195 bool ShenandoahHeap::print_location(outputStream* st, void* addr) const {
1196   return BlockLocationPrinter&lt;ShenandoahHeap&gt;::print_location(st, addr);
1197 }
1198 
1199 jlong ShenandoahHeap::millis_since_last_gc() {
1200   double v = heuristics()-&gt;time_since_last_gc() * 1000;
1201   assert(0 &lt;= v &amp;&amp; v &lt;= max_jlong, "value should fit: %f", v);
1202   return (jlong)v;
1203 }
1204 
1205 void ShenandoahHeap::prepare_for_verify() {
1206   if (SafepointSynchronize::is_at_safepoint() &amp;&amp; UseTLAB) {
1207     labs_make_parsable();
1208   }
1209 }
1210 
1211 void ShenandoahHeap::gc_threads_do(ThreadClosure* tcl) const {
1212   workers()-&gt;threads_do(tcl);
1213   if (_safepoint_workers != NULL) {
1214     _safepoint_workers-&gt;threads_do(tcl);
1215   }
1216   if (ShenandoahStringDedup::is_enabled()) {
1217     ShenandoahStringDedup::threads_do(tcl);
1218   }
1219 }
1220 
1221 void ShenandoahHeap::print_tracing_info() const {
1222   LogTarget(Info, gc, stats) lt;
1223   if (lt.is_enabled()) {
1224     ResourceMark rm;
1225     LogStream ls(lt);
1226 
1227     phase_timings()-&gt;print_global_on(&amp;ls);
1228 
1229     ls.cr();
1230     ls.cr();
1231 
1232     shenandoah_policy()-&gt;print_gc_stats(&amp;ls);
1233 
1234     ls.cr();
1235     ls.cr();
1236 
1237     if (ShenandoahPacing) {
1238       pacer()-&gt;print_on(&amp;ls);
1239     }
1240 
1241     ls.cr();
1242     ls.cr();
1243   }
1244 }
1245 
1246 void ShenandoahHeap::verify(VerifyOption vo) {
1247   if (ShenandoahSafepoint::is_at_shenandoah_safepoint()) {
1248     if (ShenandoahVerify) {
1249       verifier()-&gt;verify_generic(vo);
1250     } else {
1251       // TODO: Consider allocating verification bitmaps on demand,
1252       // and turn this on unconditionally.
1253     }
1254   }
1255 }
1256 size_t ShenandoahHeap::tlab_capacity(Thread *thr) const {
1257   return _free_set-&gt;capacity();
1258 }
1259 
1260 class ObjectIterateScanRootClosure : public BasicOopIterateClosure {
1261 private:
1262   MarkBitMap* _bitmap;
1263   Stack&lt;oop,mtGC&gt;* _oop_stack;
1264   ShenandoahHeap* const _heap;
1265   ShenandoahMarkingContext* const _marking_context;
1266 
1267   template &lt;class T&gt;
1268   void do_oop_work(T* p) {
1269     T o = RawAccess&lt;&gt;::oop_load(p);
1270     if (!CompressedOops::is_null(o)) {
1271       oop obj = CompressedOops::decode_not_null(o);
1272       if (_heap-&gt;is_concurrent_weak_root_in_progress() &amp;&amp; !_marking_context-&gt;is_marked(obj)) {
1273         // There may be dead oops in weak roots in concurrent root phase, do not touch them.
1274         return;
1275       }
1276       obj = ShenandoahBarrierSet::resolve_forwarded_not_null(obj);
1277 
1278       assert(oopDesc::is_oop(obj), "must be a valid oop");
1279       if (!_bitmap-&gt;is_marked(obj)) {
1280         _bitmap-&gt;mark(obj);
1281         _oop_stack-&gt;push(obj);
1282       }
1283     }
1284   }
1285 public:
1286   ObjectIterateScanRootClosure(MarkBitMap* bitmap, Stack&lt;oop,mtGC&gt;* oop_stack) :
1287     _bitmap(bitmap), _oop_stack(oop_stack), _heap(ShenandoahHeap::heap()),
1288     _marking_context(_heap-&gt;marking_context()) {}
1289   void do_oop(oop* p)       { do_oop_work(p); }
1290   void do_oop(narrowOop* p) { do_oop_work(p); }
1291 };
1292 
1293 /*
1294  * This is public API, used in preparation of object_iterate().
1295  * Since we don't do linear scan of heap in object_iterate() (see comment below), we don't
1296  * need to make the heap parsable. For Shenandoah-internal linear heap scans that we can
1297  * control, we call SH::tlabs_retire, SH::gclabs_retire.
1298  */
1299 void ShenandoahHeap::ensure_parsability(bool retire_tlabs) {
1300   // No-op.
1301 }
1302 
1303 /*
1304  * Iterates objects in the heap. This is public API, used for, e.g., heap dumping.
1305  *
1306  * We cannot safely iterate objects by doing a linear scan at random points in time. Linear
1307  * scanning needs to deal with dead objects, which may have dead Klass* pointers (e.g.
1308  * calling oopDesc::size() would crash) or dangling reference fields (crashes) etc. Linear
1309  * scanning therefore depends on having a valid marking bitmap to support it. However, we only
1310  * have a valid marking bitmap after successful marking. In particular, we *don't* have a valid
1311  * marking bitmap during marking, after aborted marking or during/after cleanup (when we just
1312  * wiped the bitmap in preparation for next marking).
1313  *
1314  * For all those reasons, we implement object iteration as a single marking traversal, reporting
1315  * objects as we mark+traverse through the heap, starting from GC roots. JVMTI IterateThroughHeap
1316  * is allowed to report dead objects, but is not required to do so.
1317  */
1318 void ShenandoahHeap::object_iterate(ObjectClosure* cl) {
1319   assert(SafepointSynchronize::is_at_safepoint(), "safe iteration is only available during safepoints");
1320   if (!_aux_bitmap_region_special &amp;&amp; !os::commit_memory((char*)_aux_bitmap_region.start(), _aux_bitmap_region.byte_size(), false)) {
1321     log_warning(gc)("Could not commit native memory for auxiliary marking bitmap for heap iteration");
1322     return;
1323   }
1324 
1325   // Reset bitmap
1326   _aux_bit_map.clear();
1327 
1328   Stack&lt;oop,mtGC&gt; oop_stack;
1329 
1330   ObjectIterateScanRootClosure oops(&amp;_aux_bit_map, &amp;oop_stack);
1331 
1332   {
1333     // First, we process GC roots according to current GC cycle.
1334     // This populates the work stack with initial objects.
1335     // It is important to relinquish the associated locks before diving
1336     // into heap dumper.
1337     ShenandoahHeapIterationRootScanner rp;
1338     rp.roots_do(&amp;oops);
1339   }
1340 
1341   // Work through the oop stack to traverse heap.
1342   while (! oop_stack.is_empty()) {
1343     oop obj = oop_stack.pop();
1344     assert(oopDesc::is_oop(obj), "must be a valid oop");
1345     cl-&gt;do_object(obj);
1346     obj-&gt;oop_iterate(&amp;oops);
1347   }
1348 
1349   assert(oop_stack.is_empty(), "should be empty");
1350 
1351   if (!_aux_bitmap_region_special &amp;&amp; !os::uncommit_memory((char*)_aux_bitmap_region.start(), _aux_bitmap_region.byte_size())) {
1352     log_warning(gc)("Could not uncommit native memory for auxiliary marking bitmap for heap iteration");
1353   }
1354 }
1355 
1356 // Keep alive an object that was loaded with AS_NO_KEEPALIVE.
1357 void ShenandoahHeap::keep_alive(oop obj) {
1358   if (is_concurrent_mark_in_progress()) {
1359     ShenandoahBarrierSet::barrier_set()-&gt;enqueue(obj);
1360   }
1361 }
1362 
1363 void ShenandoahHeap::heap_region_iterate(ShenandoahHeapRegionClosure* blk) const {
1364   for (size_t i = 0; i &lt; num_regions(); i++) {
1365     ShenandoahHeapRegion* current = get_region(i);
1366     blk-&gt;heap_region_do(current);
1367   }
1368 }
1369 
1370 class ShenandoahParallelHeapRegionTask : public AbstractGangTask {
1371 private:
1372   ShenandoahHeap* const _heap;
1373   ShenandoahHeapRegionClosure* const _blk;
1374 
1375   shenandoah_padding(0);
1376   volatile size_t _index;
1377   shenandoah_padding(1);
1378 
1379 public:
1380   ShenandoahParallelHeapRegionTask(ShenandoahHeapRegionClosure* blk) :
1381           AbstractGangTask("Parallel Region Task"),
1382           _heap(ShenandoahHeap::heap()), _blk(blk), _index(0) {}
1383 
1384   void work(uint worker_id) {
1385     ShenandoahParallelWorkerSession worker_session(worker_id);
1386     size_t stride = ShenandoahParallelRegionStride;
1387 
1388     size_t max = _heap-&gt;num_regions();
1389     while (_index &lt; max) {
1390       size_t cur = Atomic::fetch_and_add(&amp;_index, stride);
1391       size_t start = cur;
1392       size_t end = MIN2(cur + stride, max);
1393       if (start &gt;= max) break;
1394 
1395       for (size_t i = cur; i &lt; end; i++) {
1396         ShenandoahHeapRegion* current = _heap-&gt;get_region(i);
1397         _blk-&gt;heap_region_do(current);
1398       }
1399     }
1400   }
1401 };
1402 
1403 void ShenandoahHeap::parallel_heap_region_iterate(ShenandoahHeapRegionClosure* blk) const {
1404   assert(blk-&gt;is_thread_safe(), "Only thread-safe closures here");
1405   if (num_regions() &gt; ShenandoahParallelRegionStride) {
1406     ShenandoahParallelHeapRegionTask task(blk);
1407     workers()-&gt;run_task(&amp;task);
1408   } else {
1409     heap_region_iterate(blk);
1410   }
1411 }
1412 
1413 class ShenandoahInitMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {
1414 private:
1415   ShenandoahMarkingContext* const _ctx;
1416 public:
1417   ShenandoahInitMarkUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()-&gt;marking_context()) {}
1418 
1419   void heap_region_do(ShenandoahHeapRegion* r) {
1420     assert(!r-&gt;has_live(), "Region " SIZE_FORMAT " should have no live data", r-&gt;index());
1421     if (r-&gt;is_active()) {
1422       // Check if region needs updating its TAMS. We have updated it already during concurrent
1423       // reset, so it is very likely we don't need to do another write here.
1424       if (_ctx-&gt;top_at_mark_start(r) != r-&gt;top()) {
1425         _ctx-&gt;capture_top_at_mark_start(r);
1426       }
1427     } else {
1428       assert(_ctx-&gt;top_at_mark_start(r) == r-&gt;top(),
1429              "Region " SIZE_FORMAT " should already have correct TAMS", r-&gt;index());
1430     }
1431   }
1432 
1433   bool is_thread_safe() { return true; }
1434 };
1435 
1436 void ShenandoahHeap::op_init_mark() {
1437   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), "Should be at safepoint");
1438   assert(Thread::current()-&gt;is_VM_thread(), "can only do this in VMThread");
1439 
1440   assert(marking_context()-&gt;is_bitmap_clear(), "need clear marking bitmap");
1441   assert(!marking_context()-&gt;is_complete(), "should not be complete");
1442   assert(!has_forwarded_objects(), "No forwarded objects on this path");
1443 
1444   if (ShenandoahVerify) {
1445     verifier()-&gt;verify_before_concmark();
1446   }
1447 
1448   if (VerifyBeforeGC) {
1449     Universe::verify();
1450   }
1451 
1452   set_concurrent_mark_in_progress(true);
1453 
1454   // We need to reset all TLABs because they might be below the TAMS, and we need to mark
1455   // the objects in them. Do not let mutators allocate any new objects in their current TLABs.
1456   // It is also a good place to resize the TLAB sizes for future allocations.
1457   if (UseTLAB) {
1458     ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_manage_tlabs);
1459     tlabs_retire(ResizeTLAB);
1460   }
1461 
1462   {
1463     ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_region_states);
1464     ShenandoahInitMarkUpdateRegionStateClosure cl;
1465     parallel_heap_region_iterate(&amp;cl);
1466   }
1467 
1468   // Make above changes visible to worker threads
1469   OrderAccess::fence();
1470 
1471   concurrent_mark()-&gt;mark_roots(ShenandoahPhaseTimings::scan_roots);
1472 
1473   if (ShenandoahPacing) {
1474     pacer()-&gt;setup_for_mark();
1475   }
1476 
1477   // Arm nmethods for concurrent marking. When a nmethod is about to be executed,
1478   // we need to make sure that all its metadata are marked. alternative is to remark
1479   // thread roots at final mark pause, but it can be potential latency killer.
1480   if (ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
1481     ShenandoahCodeRoots::arm_nmethods();
1482   }
1483 }
1484 
1485 void ShenandoahHeap::op_mark() {
1486   concurrent_mark()-&gt;mark_from_roots();
1487 }
1488 
1489 class ShenandoahFinalMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {
1490 private:
1491   ShenandoahMarkingContext* const _ctx;
1492   ShenandoahHeapLock* const _lock;
1493 
1494 public:
1495   ShenandoahFinalMarkUpdateRegionStateClosure() :
1496     _ctx(ShenandoahHeap::heap()-&gt;complete_marking_context()), _lock(ShenandoahHeap::heap()-&gt;lock()) {}
1497 
1498   void heap_region_do(ShenandoahHeapRegion* r) {
1499     if (r-&gt;is_active()) {
1500       // All allocations past TAMS are implicitly live, adjust the region data.
1501       // Bitmaps/TAMS are swapped at this point, so we need to poll complete bitmap.
1502       HeapWord *tams = _ctx-&gt;top_at_mark_start(r);
1503       HeapWord *top = r-&gt;top();
1504       if (top &gt; tams) {
1505         r-&gt;increase_live_data_alloc_words(pointer_delta(top, tams));
1506       }
1507 
1508       // We are about to select the collection set, make sure it knows about
1509       // current pinning status. Also, this allows trashing more regions that
1510       // now have their pinning status dropped.
1511       if (r-&gt;is_pinned()) {
1512         if (r-&gt;pin_count() == 0) {
1513           ShenandoahHeapLocker locker(_lock);
1514           r-&gt;make_unpinned();
1515         }
1516       } else {
1517         if (r-&gt;pin_count() &gt; 0) {
1518           ShenandoahHeapLocker locker(_lock);
1519           r-&gt;make_pinned();
1520         }
1521       }
1522 
1523       // Remember limit for updating refs. It's guaranteed that we get no
1524       // from-space-refs written from here on.
1525       r-&gt;set_update_watermark_at_safepoint(r-&gt;top());
1526     } else {
1527       assert(!r-&gt;has_live(), "Region " SIZE_FORMAT " should have no live data", r-&gt;index());
1528       assert(_ctx-&gt;top_at_mark_start(r) == r-&gt;top(),
1529              "Region " SIZE_FORMAT " should have correct TAMS", r-&gt;index());
1530     }
1531   }
1532 
1533   bool is_thread_safe() { return true; }
1534 };
1535 
1536 void ShenandoahHeap::op_final_mark() {
1537   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), "Should be at safepoint");
1538   assert(!has_forwarded_objects(), "No forwarded objects on this path");
1539 
1540   // It is critical that we
1541   // evacuate roots right after finishing marking, so that we don't
1542   // get unmarked objects in the roots.
1543 
1544   if (!cancelled_gc()) {
1545     concurrent_mark()-&gt;finish_mark_from_roots(/* full_gc = */ false);
1546 
1547     // Marking is completed, deactivate SATB barrier
1548     set_concurrent_mark_in_progress(false);
1549     mark_complete_marking_context();
1550 
1551     parallel_cleaning(false /* full gc*/);
1552 
1553     if (ShenandoahVerify) {
1554       verifier()-&gt;verify_roots_no_forwarded();
1555     }
1556 
1557     {
1558       ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_region_states);
1559       ShenandoahFinalMarkUpdateRegionStateClosure cl;
1560       parallel_heap_region_iterate(&amp;cl);
1561 
1562       assert_pinned_region_status();
1563     }
1564 
1565     // Retire the TLABs, which will force threads to reacquire their TLABs after the pause.
1566     // This is needed for two reasons. Strong one: new allocations would be with new freeset,
1567     // which would be outside the collection set, so no cset writes would happen there.
1568     // Weaker one: new allocations would happen past update watermark, and so less work would
1569     // be needed for reference updates (would update the large filler instead).
1570     if (UseTLAB) {
1571       ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_manage_labs);
1572       tlabs_retire(false);
1573     }
1574 
1575     {
1576       ShenandoahGCPhase phase(ShenandoahPhaseTimings::choose_cset);
1577       ShenandoahHeapLocker locker(lock());
1578       _collection_set-&gt;clear();
1579       heuristics()-&gt;choose_collection_set(_collection_set);
1580     }
1581 
1582     {
1583       ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_rebuild_freeset);
1584       ShenandoahHeapLocker locker(lock());
1585       _free_set-&gt;rebuild();
1586     }
1587 
1588     if (!is_degenerated_gc_in_progress()) {
1589       prepare_concurrent_roots();
1590       prepare_concurrent_unloading();
1591     }
1592 
1593     // If collection set has candidates, start evacuation.
1594     // Otherwise, bypass the rest of the cycle.
1595     if (!collection_set()-&gt;is_empty()) {
1596       ShenandoahGCPhase init_evac(ShenandoahPhaseTimings::init_evac);
1597 
1598       if (ShenandoahVerify) {
1599         verifier()-&gt;verify_before_evacuation();
1600       }
1601 
1602       set_evacuation_in_progress(true);
1603       // From here on, we need to update references.
1604       set_has_forwarded_objects(true);
1605 
1606       if (!is_degenerated_gc_in_progress()) {
1607         if (ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
1608           ShenandoahCodeRoots::arm_nmethods();
1609         }
1610         evacuate_and_update_roots();
1611       }
1612 
1613       if (ShenandoahPacing) {
1614         pacer()-&gt;setup_for_evac();
1615       }
1616 
1617       if (ShenandoahVerify) {
1618         // If OOM while evacuating/updating of roots, there is no guarantee of their consistencies
1619         if (!cancelled_gc()) {
1620           ShenandoahRootVerifier::RootTypes types = ShenandoahRootVerifier::None;
1621           if (ShenandoahConcurrentRoots::should_do_concurrent_roots()) {
1622             types = ShenandoahRootVerifier::combine(ShenandoahRootVerifier::JNIHandleRoots, ShenandoahRootVerifier::WeakRoots);
1623             types = ShenandoahRootVerifier::combine(types, ShenandoahRootVerifier::CLDGRoots);
1624             types = ShenandoahRootVerifier::combine(types, ShenandoahRootVerifier::StringDedupRoots);
1625           }
1626 
1627           if (ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
1628             types = ShenandoahRootVerifier::combine(types, ShenandoahRootVerifier::CodeRoots);
1629           }
1630           verifier()-&gt;verify_roots_no_forwarded_except(types);
1631         }
1632         verifier()-&gt;verify_during_evacuation();
1633       }
1634     } else {
1635       if (ShenandoahVerify) {
1636         verifier()-&gt;verify_after_concmark();
1637       }
1638 
1639       if (VerifyAfterGC) {
1640         Universe::verify();
1641       }
1642     }
1643 
1644   } else {
1645     // If this cycle was updating references, we need to keep the has_forwarded_objects
1646     // flag on, for subsequent phases to deal with it.
1647     concurrent_mark()-&gt;cancel();
1648     set_concurrent_mark_in_progress(false);
1649 
1650     if (process_references()) {
1651       // Abandon reference processing right away: pre-cleaning must have failed.
1652       ReferenceProcessor *rp = ref_processor();
1653       rp-&gt;disable_discovery();
1654       rp-&gt;abandon_partial_discovery();
1655       rp-&gt;verify_no_references_recorded();
1656     }
1657   }
1658 }
1659 
1660 void ShenandoahHeap::op_conc_evac() {
1661   ShenandoahEvacuationTask task(this, _collection_set, true);
1662   workers()-&gt;run_task(&amp;task);
1663 }
1664 
1665 void ShenandoahHeap::op_stw_evac() {
1666   ShenandoahEvacuationTask task(this, _collection_set, false);
1667   workers()-&gt;run_task(&amp;task);
1668 }
1669 
1670 void ShenandoahHeap::op_updaterefs() {
1671   update_heap_references(true);
1672 }
1673 
1674 void ShenandoahHeap::op_cleanup_early() {
1675   free_set()-&gt;recycle_trash();
1676 }
1677 
1678 void ShenandoahHeap::op_cleanup_complete() {
1679   free_set()-&gt;recycle_trash();
1680 }
1681 
1682 class ShenandoahConcurrentRootsEvacUpdateTask : public AbstractGangTask {
1683 private:
1684   ShenandoahVMRoots&lt;true /*concurrent*/&gt;        _vm_roots;
1685   ShenandoahClassLoaderDataRoots&lt;true /*concurrent*/, false /*single threaded*/&gt; _cld_roots;
1686 
1687 public:
1688   ShenandoahConcurrentRootsEvacUpdateTask(ShenandoahPhaseTimings::Phase phase) :
1689     AbstractGangTask("Shenandoah Evacuate/Update Concurrent Strong Roots Task"),
1690     _vm_roots(phase),
1691     _cld_roots(phase, ShenandoahHeap::heap()-&gt;workers()-&gt;active_workers()) {}
1692 
1693   void work(uint worker_id) {
1694     ShenandoahConcurrentWorkerSession worker_session(worker_id);
1695     ShenandoahEvacOOMScope oom;
1696     {
1697       // vm_roots and weak_roots are OopStorage backed roots, concurrent iteration
1698       // may race against OopStorage::release() calls.
1699       ShenandoahEvacUpdateOopStorageRootsClosure cl;
1700       _vm_roots.oops_do&lt;ShenandoahEvacUpdateOopStorageRootsClosure&gt;(&amp;cl, worker_id);
1701     }
1702 
1703     {
1704       ShenandoahEvacuateUpdateRootsClosure&lt;&gt; cl;
1705       CLDToOopClosure clds(&amp;cl, ClassLoaderData::_claim_strong);
1706       _cld_roots.cld_do(&amp;clds, worker_id);
1707     }
1708   }
1709 };
1710 
1711 class ShenandoahEvacUpdateCleanupOopStorageRootsClosure : public BasicOopIterateClosure {
1712 private:
1713   ShenandoahHeap* const _heap;
1714   ShenandoahMarkingContext* const _mark_context;
1715   bool  _evac_in_progress;
1716   Thread* const _thread;
1717   size_t  _dead_counter;
1718 
1719 public:
1720   ShenandoahEvacUpdateCleanupOopStorageRootsClosure();
1721   void do_oop(oop* p);
1722   void do_oop(narrowOop* p);
1723 
1724   size_t dead_counter() const;
1725   void reset_dead_counter();
1726 };
1727 
1728 ShenandoahEvacUpdateCleanupOopStorageRootsClosure::ShenandoahEvacUpdateCleanupOopStorageRootsClosure() :
1729   _heap(ShenandoahHeap::heap()),
1730   _mark_context(ShenandoahHeap::heap()-&gt;marking_context()),
1731   _evac_in_progress(ShenandoahHeap::heap()-&gt;is_evacuation_in_progress()),
1732   _thread(Thread::current()),
1733   _dead_counter(0) {
1734 }
1735 
1736 void ShenandoahEvacUpdateCleanupOopStorageRootsClosure::do_oop(oop* p) {
1737   const oop obj = RawAccess&lt;&gt;::oop_load(p);
1738   if (!CompressedOops::is_null(obj)) {
1739     if (!_mark_context-&gt;is_marked(obj)) {
1740       shenandoah_assert_correct(p, obj);
1741       oop old = Atomic::cmpxchg(p, obj, oop(NULL));
1742       if (obj == old) {
1743         _dead_counter ++;
1744       }
1745     } else if (_evac_in_progress &amp;&amp; _heap-&gt;in_collection_set(obj)) {
1746       oop resolved = ShenandoahBarrierSet::resolve_forwarded_not_null(obj);
1747       if (resolved == obj) {
1748         resolved = _heap-&gt;evacuate_object(obj, _thread);
1749       }
1750       Atomic::cmpxchg(p, obj, resolved);
1751       assert(_heap-&gt;cancelled_gc() ||
1752              _mark_context-&gt;is_marked(resolved) &amp;&amp; !_heap-&gt;in_collection_set(resolved),
1753              "Sanity");
1754     }
1755   }
1756 }
1757 
1758 void ShenandoahEvacUpdateCleanupOopStorageRootsClosure::do_oop(narrowOop* p) {
1759   ShouldNotReachHere();
1760 }
1761 
1762 size_t ShenandoahEvacUpdateCleanupOopStorageRootsClosure::dead_counter() const {
1763   return _dead_counter;
1764 }
1765 
1766 void ShenandoahEvacUpdateCleanupOopStorageRootsClosure::reset_dead_counter() {
1767   _dead_counter = 0;
1768 }
1769 
1770 class ShenandoahIsCLDAliveClosure : public CLDClosure {
1771 public:
1772   void do_cld(ClassLoaderData* cld) {
1773     cld-&gt;is_alive();
1774   }
1775 };
1776 
1777 class ShenandoahIsNMethodAliveClosure: public NMethodClosure {
1778 public:
1779   void do_nmethod(nmethod* n) {
1780     n-&gt;is_unloading();
1781   }
1782 };
1783 
1784 // This task not only evacuates/updates marked weak roots, but also "NULL"
1785 // dead weak roots.
1786 class ShenandoahConcurrentWeakRootsEvacUpdateTask : public AbstractGangTask {
1787 private:
1788   ShenandoahWeakRoot&lt;true /*concurrent*/&gt;  _jni_roots;
1789   ShenandoahWeakRoot&lt;true /*concurrent*/&gt;  _string_table_roots;
1790   ShenandoahWeakRoot&lt;true /*concurrent*/&gt;  _resolved_method_table_roots;
1791   ShenandoahWeakRoot&lt;true /*concurrent*/&gt;  _vm_roots;
1792 
1793   // Roots related to concurrent class unloading
1794   ShenandoahClassLoaderDataRoots&lt;true /* concurrent */, false /* single thread*/&gt;
1795                                            _cld_roots;
1796   ShenandoahConcurrentNMethodIterator      _nmethod_itr;
1797   ShenandoahConcurrentStringDedupRoots     _dedup_roots;
1798   bool                                     _concurrent_class_unloading;
1799 
1800 public:
1801   ShenandoahConcurrentWeakRootsEvacUpdateTask(ShenandoahPhaseTimings::Phase phase) :
1802     AbstractGangTask("Shenandoah Concurrent Weak Root Task"),
1803     _jni_roots(OopStorageSet::jni_weak(), phase, ShenandoahPhaseTimings::JNIWeakRoots),
1804     _string_table_roots(OopStorageSet::string_table_weak(), phase, ShenandoahPhaseTimings::StringTableRoots),
1805     _resolved_method_table_roots(OopStorageSet::resolved_method_table_weak(), phase, ShenandoahPhaseTimings::ResolvedMethodTableRoots),
1806     _vm_roots(OopStorageSet::vm_weak(), phase, ShenandoahPhaseTimings::VMWeakRoots),
1807     _cld_roots(phase, ShenandoahHeap::heap()-&gt;workers()-&gt;active_workers()),
1808     _nmethod_itr(ShenandoahCodeRoots::table()),
1809     _dedup_roots(phase),
1810     _concurrent_class_unloading(ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
1811     StringTable::reset_dead_counter();
1812     ResolvedMethodTable::reset_dead_counter();
1813     if (_concurrent_class_unloading) {
1814       MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);
1815       _nmethod_itr.nmethods_do_begin();
1816     }
1817   }
1818 
1819   ~ShenandoahConcurrentWeakRootsEvacUpdateTask() {
1820     StringTable::finish_dead_counter();
1821     ResolvedMethodTable::finish_dead_counter();
1822     if (_concurrent_class_unloading) {
1823       MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);
1824       _nmethod_itr.nmethods_do_end();
1825     }
1826   }
1827 
1828   void work(uint worker_id) {
1829     ShenandoahConcurrentWorkerSession worker_session(worker_id);
1830     {
1831       ShenandoahEvacOOMScope oom;
1832       // jni_roots and weak_roots are OopStorage backed roots, concurrent iteration
1833       // may race against OopStorage::release() calls.
1834       ShenandoahEvacUpdateCleanupOopStorageRootsClosure cl;
1835       _jni_roots.oops_do(&amp;cl, worker_id);
1836       _vm_roots.oops_do(&amp;cl, worker_id);
1837 
1838       cl.reset_dead_counter();
1839       _string_table_roots.oops_do(&amp;cl, worker_id);
1840       StringTable::inc_dead_counter(cl.dead_counter());
1841 
1842       cl.reset_dead_counter();
1843       _resolved_method_table_roots.oops_do(&amp;cl, worker_id);
1844       ResolvedMethodTable::inc_dead_counter(cl.dead_counter());
1845 
1846       // String dedup weak roots
1847       ShenandoahForwardedIsAliveClosure is_alive;
1848       ShenandoahEvacuateUpdateRootsClosure&lt;MO_RELEASE&gt; keep_alive;
1849       _dedup_roots.oops_do(&amp;is_alive, &amp;keep_alive, worker_id);
1850     }
1851 
1852     // If we are going to perform concurrent class unloading later on, we need to
1853     // cleanup the weak oops in CLD and determinate nmethod's unloading state, so that we
1854     // can cleanup immediate garbage sooner.
1855     if (_concurrent_class_unloading) {
1856       // Applies ShenandoahIsCLDAlive closure to CLDs, native barrier will either NULL the
1857       // CLD's holder or evacuate it.
1858       ShenandoahIsCLDAliveClosure is_cld_alive;
1859       _cld_roots.cld_do(&amp;is_cld_alive, worker_id);
1860 
1861       // Applies ShenandoahIsNMethodAliveClosure to registered nmethods.
1862       // The closure calls nmethod-&gt;is_unloading(). The is_unloading
1863       // state is cached, therefore, during concurrent class unloading phase,
1864       // we will not touch the metadata of unloading nmethods
1865       ShenandoahIsNMethodAliveClosure is_nmethod_alive;
1866       _nmethod_itr.nmethods_do(&amp;is_nmethod_alive);
1867     }
1868   }
1869 };
1870 
1871 void ShenandoahHeap::op_weak_roots() {
1872   if (is_concurrent_weak_root_in_progress()) {
1873     // Concurrent weak root processing
1874     {
1875       ShenandoahTimingsTracker t(ShenandoahPhaseTimings::conc_weak_roots_work);
1876       ShenandoahGCWorkerPhase worker_phase(ShenandoahPhaseTimings::conc_weak_roots_work);
1877       ShenandoahConcurrentWeakRootsEvacUpdateTask task(ShenandoahPhaseTimings::conc_weak_roots_work);
1878       workers()-&gt;run_task(&amp;task);
1879       if (!ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
1880         set_concurrent_weak_root_in_progress(false);
1881       }
1882     }
1883 
1884     // Perform handshake to flush out dead oops
1885     {
1886       ShenandoahTimingsTracker t(ShenandoahPhaseTimings::conc_weak_roots_rendezvous);
1887       ShenandoahRendezvousClosure cl;
1888       Handshake::execute(&amp;cl);
1889     }
1890   }
1891 }
1892 
1893 void ShenandoahHeap::op_class_unloading() {
1894   assert (is_concurrent_weak_root_in_progress() &amp;&amp;
1895           ShenandoahConcurrentRoots::should_do_concurrent_class_unloading(),
1896           "Checked by caller");
1897   _unloader.unload();
1898   set_concurrent_weak_root_in_progress(false);
1899 }
1900 
1901 void ShenandoahHeap::op_strong_roots() {
1902   assert(is_concurrent_strong_root_in_progress(), "Checked by caller");
1903   ShenandoahConcurrentRootsEvacUpdateTask task(ShenandoahPhaseTimings::conc_strong_roots);
1904   workers()-&gt;run_task(&amp;task);
1905   set_concurrent_strong_root_in_progress(false);
1906 }
1907 
1908 class ShenandoahResetUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {
1909 private:
1910   ShenandoahMarkingContext* const _ctx;
1911 public:
1912   ShenandoahResetUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()-&gt;marking_context()) {}
1913 
1914   void heap_region_do(ShenandoahHeapRegion* r) {
1915     if (r-&gt;is_active()) {
1916       // Reset live data and set TAMS optimistically. We would recheck these under the pause
1917       // anyway to capture any updates that happened since now.
1918       r-&gt;clear_live_data();
1919       _ctx-&gt;capture_top_at_mark_start(r);
1920     }
1921   }
1922 
1923   bool is_thread_safe() { return true; }
1924 };
1925 
1926 void ShenandoahHeap::op_reset() {
1927   if (ShenandoahPacing) {
1928     pacer()-&gt;setup_for_reset();
1929   }
1930   reset_mark_bitmap();
1931 
1932   ShenandoahResetUpdateRegionStateClosure cl;
1933   parallel_heap_region_iterate(&amp;cl);
1934 }
1935 
1936 void ShenandoahHeap::op_preclean() {
1937   if (ShenandoahPacing) {
1938     pacer()-&gt;setup_for_preclean();
1939   }
1940   concurrent_mark()-&gt;preclean_weak_refs();
1941 }
1942 
1943 void ShenandoahHeap::op_full(GCCause::Cause cause) {
1944   ShenandoahMetricsSnapshot metrics;
1945   metrics.snap_before();
1946 
1947   full_gc()-&gt;do_it(cause);
1948 
1949   metrics.snap_after();
1950 
1951   if (metrics.is_good_progress()) {
1952     _progress_last_gc.set();
1953   } else {
1954     // Nothing to do. Tell the allocation path that we have failed to make
1955     // progress, and it can finally fail.
1956     _progress_last_gc.unset();
1957   }
1958 }
1959 
1960 void ShenandoahHeap::op_degenerated(ShenandoahDegenPoint point) {
1961   // Degenerated GC is STW, but it can also fail. Current mechanics communicates
1962   // GC failure via cancelled_concgc() flag. So, if we detect the failure after
1963   // some phase, we have to upgrade the Degenerate GC to Full GC.
1964 
1965   clear_cancelled_gc();
1966 
1967   ShenandoahMetricsSnapshot metrics;
1968   metrics.snap_before();
1969 
1970   switch (point) {
1971     // The cases below form the Duff's-like device: it describes the actual GC cycle,
1972     // but enters it at different points, depending on which concurrent phase had
1973     // degenerated.
1974 
1975     case _degenerated_outside_cycle:
1976       // We have degenerated from outside the cycle, which means something is bad with
1977       // the heap, most probably heavy humongous fragmentation, or we are very low on free
1978       // space. It makes little sense to wait for Full GC to reclaim as much as it can, when
1979       // we can do the most aggressive degen cycle, which includes processing references and
1980       // class unloading, unless those features are explicitly disabled.
1981       //
1982       // Note that we can only do this for "outside-cycle" degens, otherwise we would risk
1983       // changing the cycle parameters mid-cycle during concurrent -&gt; degenerated handover.
1984       set_process_references(heuristics()-&gt;can_process_references());
1985       set_unload_classes(heuristics()-&gt;can_unload_classes());
1986 
1987       op_reset();
1988 
1989       op_init_mark();
1990       if (cancelled_gc()) {
1991         op_degenerated_fail();
1992         return;
1993       }
1994 
1995     case _degenerated_mark:
1996       op_final_mark();
1997       if (cancelled_gc()) {
1998         op_degenerated_fail();
1999         return;
2000       }
2001 
2002       if (!has_forwarded_objects() &amp;&amp; ShenandoahConcurrentRoots::can_do_concurrent_class_unloading()) {
2003         // Disarm nmethods that armed for concurrent mark. On normal cycle, it would
2004         // be disarmed while conc-roots phase is running.
2005         // TODO: Call op_conc_roots() here instead
2006         ShenandoahCodeRoots::disarm_nmethods();
2007       }
2008 
2009       op_cleanup_early();
2010 
2011     case _degenerated_evac:
2012       // If heuristics thinks we should do the cycle, this flag would be set,
2013       // and we can do evacuation. Otherwise, it would be the shortcut cycle.
2014       if (is_evacuation_in_progress()) {
2015 
2016         // Degeneration under oom-evac protocol might have left some objects in
2017         // collection set un-evacuated. Restart evacuation from the beginning to
2018         // capture all objects. For all the objects that are already evacuated,
2019         // it would be a simple check, which is supposed to be fast. This is also
2020         // safe to do even without degeneration, as CSet iterator is at beginning
2021         // in preparation for evacuation anyway.
2022         //
2023         // Before doing that, we need to make sure we never had any cset-pinned
2024         // regions. This may happen if allocation failure happened when evacuating
2025         // the about-to-be-pinned object, oom-evac protocol left the object in
2026         // the collection set, and then the pin reached the cset region. If we continue
2027         // the cycle here, we would trash the cset and alive objects in it. To avoid
2028         // it, we fail degeneration right away and slide into Full GC to recover.
2029 
2030         {
2031           sync_pinned_region_status();
2032           collection_set()-&gt;clear_current_index();
2033 
2034           ShenandoahHeapRegion* r;
2035           while ((r = collection_set()-&gt;next()) != NULL) {
2036             if (r-&gt;is_pinned()) {
2037               cancel_gc(GCCause::_shenandoah_upgrade_to_full_gc);
2038               op_degenerated_fail();
2039               return;
2040             }
2041           }
2042 
2043           collection_set()-&gt;clear_current_index();
2044         }
2045 
2046         op_stw_evac();
2047         if (cancelled_gc()) {
2048           op_degenerated_fail();
2049           return;
2050         }
2051       }
2052 
2053       // If heuristics thinks we should do the cycle, this flag would be set,
2054       // and we need to do update-refs. Otherwise, it would be the shortcut cycle.
2055       if (has_forwarded_objects()) {
2056         op_init_updaterefs();
2057         if (cancelled_gc()) {
2058           op_degenerated_fail();
2059           return;
2060         }
2061       }
2062 
2063     case _degenerated_updaterefs:
2064       if (has_forwarded_objects()) {
2065         op_final_updaterefs();
2066         if (cancelled_gc()) {
2067           op_degenerated_fail();
2068           return;
2069         }
2070       }
2071 
2072       op_cleanup_complete();
2073       break;
2074 
2075     default:
2076       ShouldNotReachHere();
2077   }
2078 
2079   if (ShenandoahVerify) {
2080     verifier()-&gt;verify_after_degenerated();
2081   }
2082 
2083   if (VerifyAfterGC) {
2084     Universe::verify();
2085   }
2086 
2087   metrics.snap_after();
2088 
2089   // Check for futility and fail. There is no reason to do several back-to-back Degenerated cycles,
2090   // because that probably means the heap is overloaded and/or fragmented.
2091   if (!metrics.is_good_progress()) {
2092     _progress_last_gc.unset();
2093     cancel_gc(GCCause::_shenandoah_upgrade_to_full_gc);
2094     op_degenerated_futile();
2095   } else {
2096     _progress_last_gc.set();
2097   }
2098 }
2099 
2100 void ShenandoahHeap::op_degenerated_fail() {
2101   log_info(gc)("Cannot finish degeneration, upgrading to Full GC");
2102   shenandoah_policy()-&gt;record_degenerated_upgrade_to_full();
2103   op_full(GCCause::_shenandoah_upgrade_to_full_gc);
2104 }
2105 
2106 void ShenandoahHeap::op_degenerated_futile() {
2107   shenandoah_policy()-&gt;record_degenerated_upgrade_to_full();
2108   op_full(GCCause::_shenandoah_upgrade_to_full_gc);
2109 }
2110 
2111 void ShenandoahHeap::force_satb_flush_all_threads() {
2112   if (!is_concurrent_mark_in_progress()) {
2113     // No need to flush SATBs
2114     return;
2115   }
2116 
2117   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {
2118     ShenandoahThreadLocalData::set_force_satb_flush(t, true);
2119   }
2120   // The threads are not "acquiring" their thread-local data, but it does not
2121   // hurt to "release" the updates here anyway.
2122   OrderAccess::fence();
2123 }
2124 
2125 void ShenandoahHeap::set_gc_state_all_threads(char state) {
2126   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {
2127     ShenandoahThreadLocalData::set_gc_state(t, state);
2128   }
2129 }
2130 
2131 void ShenandoahHeap::set_gc_state_mask(uint mask, bool value) {
2132   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), "Should really be Shenandoah safepoint");
2133   _gc_state.set_cond(mask, value);
2134   set_gc_state_all_threads(_gc_state.raw_value());
2135 }
2136 
2137 void ShenandoahHeap::set_concurrent_mark_in_progress(bool in_progress) {
2138   if (has_forwarded_objects()) {
2139     set_gc_state_mask(MARKING | UPDATEREFS, in_progress);
2140   } else {
2141     set_gc_state_mask(MARKING, in_progress);
2142   }
2143   ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(in_progress, !in_progress);
2144 }
2145 
2146 void ShenandoahHeap::set_evacuation_in_progress(bool in_progress) {
2147   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), "Only call this at safepoint");
2148   set_gc_state_mask(EVACUATION, in_progress);
2149 }
2150 
2151 void ShenandoahHeap::set_concurrent_strong_root_in_progress(bool in_progress) {
2152   assert(ShenandoahConcurrentRoots::can_do_concurrent_roots(), "Why set the flag?");
2153   if (in_progress) {
2154     _concurrent_strong_root_in_progress.set();
2155   } else {
2156     _concurrent_strong_root_in_progress.unset();
2157   }
2158 }
2159 
2160 void ShenandoahHeap::set_concurrent_weak_root_in_progress(bool in_progress) {
2161   assert(ShenandoahConcurrentRoots::can_do_concurrent_roots(), "Why set the flag?");
2162   if (in_progress) {
2163     _concurrent_weak_root_in_progress.set();
2164   } else {
2165     _concurrent_weak_root_in_progress.unset();
2166   }
2167 }
2168 
2169 void ShenandoahHeap::ref_processing_init() {
2170   assert(_max_workers &gt; 0, "Sanity");
2171 
2172   _ref_processor =
2173     new ReferenceProcessor(&amp;_subject_to_discovery,  // is_subject_to_discovery
2174                            _ref_proc_mt_processing, // MT processing
2175                            _max_workers,            // Degree of MT processing
2176                            _ref_proc_mt_discovery,  // MT discovery
2177                            _max_workers,            // Degree of MT discovery
2178                            false,                   // Reference discovery is not atomic
2179                            NULL,                    // No closure, should be installed before use
2180                            true);                   // Scale worker threads
2181 
2182   shenandoah_assert_rp_isalive_not_installed();
2183 }
2184 
2185 GCTracer* ShenandoahHeap::tracer() {
2186   return shenandoah_policy()-&gt;tracer();
2187 }
2188 
2189 size_t ShenandoahHeap::tlab_used(Thread* thread) const {
2190   return _free_set-&gt;used();
2191 }
2192 
2193 bool ShenandoahHeap::try_cancel_gc() {
2194   while (true) {
2195     jbyte prev = _cancelled_gc.cmpxchg(CANCELLED, CANCELLABLE);
2196     if (prev == CANCELLABLE) return true;
2197     else if (prev == CANCELLED) return false;
2198     assert(ShenandoahSuspendibleWorkers, "should not get here when not using suspendible workers");
2199     assert(prev == NOT_CANCELLED, "must be NOT_CANCELLED");
2200     if (Thread::current()-&gt;is_Java_thread()) {
2201       // We need to provide a safepoint here, otherwise we might
2202       // spin forever if a SP is pending.
2203       ThreadBlockInVM sp(JavaThread::current());
2204       SpinPause();
2205     }
2206   }
2207 }
2208 
2209 void ShenandoahHeap::cancel_gc(GCCause::Cause cause) {
2210   if (try_cancel_gc()) {
2211     FormatBuffer&lt;&gt; msg("Cancelling GC: %s", GCCause::to_string(cause));
2212     log_info(gc)("%s", msg.buffer());
2213     Events::log(Thread::current(), "%s", msg.buffer());
2214   }
2215 }
2216 
2217 uint ShenandoahHeap::max_workers() {
2218   return _max_workers;
2219 }
2220 
2221 void ShenandoahHeap::stop() {
2222   // The shutdown sequence should be able to terminate when GC is running.
2223 
2224   // Step 0. Notify policy to disable event recording.
2225   _shenandoah_policy-&gt;record_shutdown();
2226 
2227   // Step 1. Notify control thread that we are in shutdown.
2228   // Note that we cannot do that with stop(), because stop() is blocking and waits for the actual shutdown.
2229   // Doing stop() here would wait for the normal GC cycle to complete, never falling through to cancel below.
2230   control_thread()-&gt;prepare_for_graceful_shutdown();
2231 
2232   // Step 2. Notify GC workers that we are cancelling GC.
2233   cancel_gc(GCCause::_shenandoah_stop_vm);
2234 
2235   // Step 3. Wait until GC worker exits normally.
2236   control_thread()-&gt;stop();
2237 
2238   // Step 4. Stop String Dedup thread if it is active
2239   if (ShenandoahStringDedup::is_enabled()) {
2240     ShenandoahStringDedup::stop();
2241   }
2242 }
2243 
2244 void ShenandoahHeap::stw_unload_classes(bool full_gc) {
2245   if (!unload_classes()) return;
2246 
2247   // Unload classes and purge SystemDictionary.
2248   {
2249     ShenandoahGCPhase phase(full_gc ?
2250                             ShenandoahPhaseTimings::full_gc_purge_class_unload :
2251                             ShenandoahPhaseTimings::purge_class_unload);
2252     bool purged_class = SystemDictionary::do_unloading(gc_timer());
2253 
2254     ShenandoahIsAliveSelector is_alive;
2255     uint num_workers = _workers-&gt;active_workers();
2256     ShenandoahClassUnloadingTask unlink_task(is_alive.is_alive_closure(), num_workers, purged_class);
2257     _workers-&gt;run_task(&amp;unlink_task);
2258   }
2259 
2260   {
2261     ShenandoahGCPhase phase(full_gc ?
2262                             ShenandoahPhaseTimings::full_gc_purge_cldg :
2263                             ShenandoahPhaseTimings::purge_cldg);
2264     ClassLoaderDataGraph::purge();
2265   }
2266   // Resize and verify metaspace
2267   MetaspaceGC::compute_new_size();
<a name="2" id="anc2"></a>
2268 }
2269 
2270 // Weak roots are either pre-evacuated (final mark) or updated (final updaterefs),
2271 // so they should not have forwarded oops.
2272 // However, we do need to "null" dead oops in the roots, if can not be done
2273 // in concurrent cycles.
2274 void ShenandoahHeap::stw_process_weak_roots(bool full_gc) {
2275   ShenandoahGCPhase root_phase(full_gc ?
2276                                ShenandoahPhaseTimings::full_gc_purge :
2277                                ShenandoahPhaseTimings::purge);
2278   uint num_workers = _workers-&gt;active_workers();
2279   ShenandoahPhaseTimings::Phase timing_phase = full_gc ?
2280                                                ShenandoahPhaseTimings::full_gc_purge_weak_par :
2281                                                ShenandoahPhaseTimings::purge_weak_par;
2282   ShenandoahGCPhase phase(timing_phase);
2283   ShenandoahGCWorkerPhase worker_phase(timing_phase);
2284 
2285   // Cleanup weak roots
2286   if (has_forwarded_objects()) {
2287     ShenandoahForwardedIsAliveClosure is_alive;
2288     ShenandoahUpdateRefsClosure keep_alive;
2289     ShenandoahParallelWeakRootsCleaningTask&lt;ShenandoahForwardedIsAliveClosure, ShenandoahUpdateRefsClosure&gt;
2290       cleaning_task(timing_phase, &amp;is_alive, &amp;keep_alive, num_workers, !ShenandoahConcurrentRoots::should_do_concurrent_class_unloading());
2291     _workers-&gt;run_task(&amp;cleaning_task);
2292   } else {
2293     ShenandoahIsAliveClosure is_alive;
2294 #ifdef ASSERT
2295     ShenandoahAssertNotForwardedClosure verify_cl;
2296     ShenandoahParallelWeakRootsCleaningTask&lt;ShenandoahIsAliveClosure, ShenandoahAssertNotForwardedClosure&gt;
2297       cleaning_task(timing_phase, &amp;is_alive, &amp;verify_cl, num_workers, !ShenandoahConcurrentRoots::should_do_concurrent_class_unloading());
2298 #else
2299     ShenandoahParallelWeakRootsCleaningTask&lt;ShenandoahIsAliveClosure, DoNothingClosure&gt;
2300       cleaning_task(timing_phase, &amp;is_alive, &amp;do_nothing_cl, num_workers, !ShenandoahConcurrentRoots::should_do_concurrent_class_unloading());
2301 #endif
2302     _workers-&gt;run_task(&amp;cleaning_task);
2303   }
2304 }
2305 
2306 void ShenandoahHeap::parallel_cleaning(bool full_gc) {
2307   assert(SafepointSynchronize::is_at_safepoint(), "Must be at a safepoint");
2308   stw_process_weak_roots(full_gc);
2309   if (!ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
2310     stw_unload_classes(full_gc);
2311   }
2312 }
2313 
2314 void ShenandoahHeap::set_has_forwarded_objects(bool cond) {
2315   set_gc_state_mask(HAS_FORWARDED, cond);
2316 }
2317 
2318 void ShenandoahHeap::set_process_references(bool pr) {
2319   _process_references.set_cond(pr);
2320 }
2321 
2322 void ShenandoahHeap::set_unload_classes(bool uc) {
2323   _unload_classes.set_cond(uc);
2324 }
2325 
2326 bool ShenandoahHeap::process_references() const {
2327   return _process_references.is_set();
2328 }
2329 
2330 bool ShenandoahHeap::unload_classes() const {
2331   return _unload_classes.is_set();
2332 }
2333 
2334 address ShenandoahHeap::in_cset_fast_test_addr() {
2335   ShenandoahHeap* heap = ShenandoahHeap::heap();
2336   assert(heap-&gt;collection_set() != NULL, "Sanity");
2337   return (address) heap-&gt;collection_set()-&gt;biased_map_address();
2338 }
2339 
2340 address ShenandoahHeap::cancelled_gc_addr() {
2341   return (address) ShenandoahHeap::heap()-&gt;_cancelled_gc.addr_of();
2342 }
2343 
2344 address ShenandoahHeap::gc_state_addr() {
2345   return (address) ShenandoahHeap::heap()-&gt;_gc_state.addr_of();
2346 }
2347 
2348 size_t ShenandoahHeap::bytes_allocated_since_gc_start() {
2349   return Atomic::load_acquire(&amp;_bytes_allocated_since_gc_start);
2350 }
2351 
2352 void ShenandoahHeap::reset_bytes_allocated_since_gc_start() {
2353   Atomic::release_store_fence(&amp;_bytes_allocated_since_gc_start, (size_t)0);
2354 }
2355 
2356 void ShenandoahHeap::set_degenerated_gc_in_progress(bool in_progress) {
2357   _degenerated_gc_in_progress.set_cond(in_progress);
2358 }
2359 
2360 void ShenandoahHeap::set_full_gc_in_progress(bool in_progress) {
2361   _full_gc_in_progress.set_cond(in_progress);
2362 }
2363 
2364 void ShenandoahHeap::set_full_gc_move_in_progress(bool in_progress) {
2365   assert (is_full_gc_in_progress(), "should be");
2366   _full_gc_move_in_progress.set_cond(in_progress);
2367 }
2368 
2369 void ShenandoahHeap::set_update_refs_in_progress(bool in_progress) {
2370   set_gc_state_mask(UPDATEREFS, in_progress);
2371 }
2372 
2373 void ShenandoahHeap::register_nmethod(nmethod* nm) {
2374   ShenandoahCodeRoots::register_nmethod(nm);
2375 }
2376 
2377 void ShenandoahHeap::unregister_nmethod(nmethod* nm) {
2378   ShenandoahCodeRoots::unregister_nmethod(nm);
2379 }
2380 
2381 void ShenandoahHeap::flush_nmethod(nmethod* nm) {
2382   ShenandoahCodeRoots::flush_nmethod(nm);
2383 }
2384 
2385 oop ShenandoahHeap::pin_object(JavaThread* thr, oop o) {
2386   heap_region_containing(o)-&gt;record_pin();
2387   return o;
2388 }
2389 
2390 void ShenandoahHeap::unpin_object(JavaThread* thr, oop o) {
2391   heap_region_containing(o)-&gt;record_unpin();
2392 }
2393 
2394 void ShenandoahHeap::sync_pinned_region_status() {
2395   ShenandoahHeapLocker locker(lock());
2396 
2397   for (size_t i = 0; i &lt; num_regions(); i++) {
2398     ShenandoahHeapRegion *r = get_region(i);
2399     if (r-&gt;is_active()) {
2400       if (r-&gt;is_pinned()) {
2401         if (r-&gt;pin_count() == 0) {
2402           r-&gt;make_unpinned();
2403         }
2404       } else {
2405         if (r-&gt;pin_count() &gt; 0) {
2406           r-&gt;make_pinned();
2407         }
2408       }
2409     }
2410   }
2411 
2412   assert_pinned_region_status();
2413 }
2414 
2415 #ifdef ASSERT
2416 void ShenandoahHeap::assert_pinned_region_status() {
2417   for (size_t i = 0; i &lt; num_regions(); i++) {
2418     ShenandoahHeapRegion* r = get_region(i);
2419     assert((r-&gt;is_pinned() &amp;&amp; r-&gt;pin_count() &gt; 0) || (!r-&gt;is_pinned() &amp;&amp; r-&gt;pin_count() == 0),
2420            "Region " SIZE_FORMAT " pinning status is inconsistent", i);
2421   }
2422 }
2423 #endif
2424 
2425 ConcurrentGCTimer* ShenandoahHeap::gc_timer() const {
2426   return _gc_timer;
2427 }
2428 
2429 void ShenandoahHeap::prepare_concurrent_roots() {
2430   assert(SafepointSynchronize::is_at_safepoint(), "Must be at a safepoint");
2431   if (ShenandoahConcurrentRoots::should_do_concurrent_roots()) {
2432     set_concurrent_strong_root_in_progress(!collection_set()-&gt;is_empty());
2433     set_concurrent_weak_root_in_progress(true);
2434   }
2435 }
2436 
2437 void ShenandoahHeap::prepare_concurrent_unloading() {
2438   assert(SafepointSynchronize::is_at_safepoint(), "Must be at a safepoint");
2439   if (ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
2440     _unloader.prepare();
2441   }
2442 }
2443 
2444 void ShenandoahHeap::finish_concurrent_unloading() {
2445   assert(SafepointSynchronize::is_at_safepoint(), "Must be at a safepoint");
2446   if (ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
2447     _unloader.finish();
2448   }
2449 }
2450 
2451 #ifdef ASSERT
2452 void ShenandoahHeap::assert_gc_workers(uint nworkers) {
2453   assert(nworkers &gt; 0 &amp;&amp; nworkers &lt;= max_workers(), "Sanity");
2454 
2455   if (ShenandoahSafepoint::is_at_shenandoah_safepoint()) {
2456     if (UseDynamicNumberOfGCThreads) {
2457       assert(nworkers &lt;= ParallelGCThreads, "Cannot use more than it has");
2458     } else {
2459       // Use ParallelGCThreads inside safepoints
2460       assert(nworkers == ParallelGCThreads, "Use ParallelGCThreads within safepoints");
2461     }
2462   } else {
2463     if (UseDynamicNumberOfGCThreads) {
2464       assert(nworkers &lt;= ConcGCThreads, "Cannot use more than it has");
2465     } else {
2466       // Use ConcGCThreads outside safepoints
2467       assert(nworkers == ConcGCThreads, "Use ConcGCThreads outside safepoints");
2468     }
2469   }
2470 }
2471 #endif
2472 
2473 ShenandoahVerifier* ShenandoahHeap::verifier() {
2474   guarantee(ShenandoahVerify, "Should be enabled");
2475   assert (_verifier != NULL, "sanity");
2476   return _verifier;
2477 }
2478 
2479 template&lt;class T&gt;
2480 class ShenandoahUpdateHeapRefsTask : public AbstractGangTask {
2481 private:
2482   T cl;
2483   ShenandoahHeap* _heap;
2484   ShenandoahRegionIterator* _regions;
2485   bool _concurrent;
2486 public:
2487   ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions, bool concurrent) :
2488     AbstractGangTask("Concurrent Update References Task"),
2489     cl(T()),
2490     _heap(ShenandoahHeap::heap()),
2491     _regions(regions),
2492     _concurrent(concurrent) {
2493   }
2494 
2495   void work(uint worker_id) {
2496     if (_concurrent) {
2497       ShenandoahConcurrentWorkerSession worker_session(worker_id);
2498       ShenandoahSuspendibleThreadSetJoiner stsj(ShenandoahSuspendibleWorkers);
2499       do_work();
2500     } else {
2501       ShenandoahParallelWorkerSession worker_session(worker_id);
2502       do_work();
2503     }
2504   }
2505 
2506 private:
2507   void do_work() {
2508     ShenandoahHeapRegion* r = _regions-&gt;next();
2509     ShenandoahMarkingContext* const ctx = _heap-&gt;complete_marking_context();
2510     while (r != NULL) {
2511       HeapWord* update_watermark = r-&gt;get_update_watermark();
2512       assert (update_watermark &gt;= r-&gt;bottom(), "sanity");
2513       if (r-&gt;is_active() &amp;&amp; !r-&gt;is_cset()) {
2514         _heap-&gt;marked_object_oop_iterate(r, &amp;cl, update_watermark);
2515       }
2516       if (ShenandoahPacing) {
2517         _heap-&gt;pacer()-&gt;report_updaterefs(pointer_delta(update_watermark, r-&gt;bottom()));
2518       }
2519       if (_heap-&gt;check_cancelled_gc_and_yield(_concurrent)) {
2520         return;
2521       }
2522       r = _regions-&gt;next();
2523     }
2524   }
2525 };
2526 
2527 void ShenandoahHeap::update_heap_references(bool concurrent) {
2528   ShenandoahUpdateHeapRefsTask&lt;ShenandoahUpdateHeapRefsClosure&gt; task(&amp;_update_refs_iterator, concurrent);
2529   workers()-&gt;run_task(&amp;task);
2530 }
2531 
2532 void ShenandoahHeap::op_init_updaterefs() {
2533   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), "must be at safepoint");
2534 
2535   set_evacuation_in_progress(false);
2536 
2537   // Evacuation is over, no GCLABs are needed anymore. GCLABs are under URWM, so we need to
2538   // make them parsable for update code to work correctly. Plus, we can compute new sizes
2539   // for future GCLABs here.
2540   if (UseTLAB) {
2541     ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_refs_manage_gclabs);
2542     gclabs_retire(ResizeTLAB);
2543   }
2544 
2545   if (ShenandoahVerify) {
2546     if (!is_degenerated_gc_in_progress()) {
2547       verifier()-&gt;verify_roots_in_to_space_except(ShenandoahRootVerifier::ThreadRoots);
2548     }
2549     verifier()-&gt;verify_before_updaterefs();
2550   }
2551 
2552   set_update_refs_in_progress(true);
2553 
2554   _update_refs_iterator.reset();
2555 
2556   if (ShenandoahPacing) {
2557     pacer()-&gt;setup_for_updaterefs();
2558   }
2559 }
2560 
2561 class ShenandoahFinalUpdateRefsUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {
2562 private:
2563   ShenandoahHeapLock* const _lock;
2564 
2565 public:
2566   ShenandoahFinalUpdateRefsUpdateRegionStateClosure() : _lock(ShenandoahHeap::heap()-&gt;lock()) {}
2567 
2568   void heap_region_do(ShenandoahHeapRegion* r) {
2569     // Drop unnecessary "pinned" state from regions that does not have CP marks
2570     // anymore, as this would allow trashing them.
2571 
2572     if (r-&gt;is_active()) {
2573       if (r-&gt;is_pinned()) {
2574         if (r-&gt;pin_count() == 0) {
2575           ShenandoahHeapLocker locker(_lock);
2576           r-&gt;make_unpinned();
2577         }
2578       } else {
2579         if (r-&gt;pin_count() &gt; 0) {
2580           ShenandoahHeapLocker locker(_lock);
2581           r-&gt;make_pinned();
2582         }
2583       }
2584     }
2585   }
2586 
2587   bool is_thread_safe() { return true; }
2588 };
2589 
2590 void ShenandoahHeap::op_final_updaterefs() {
2591   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), "must be at safepoint");
2592 
2593   finish_concurrent_unloading();
2594 
2595   // Check if there is left-over work, and finish it
2596   if (_update_refs_iterator.has_next()) {
2597     ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_refs_finish_work);
2598 
2599     // Finish updating references where we left off.
2600     clear_cancelled_gc();
2601     update_heap_references(false);
2602   }
2603 
2604   // Clear cancelled GC, if set. On cancellation path, the block before would handle
2605   // everything. On degenerated paths, cancelled gc would not be set anyway.
2606   if (cancelled_gc()) {
2607     clear_cancelled_gc();
2608   }
2609   assert(!cancelled_gc(), "Should have been done right before");
2610 
2611   if (ShenandoahVerify &amp;&amp; !is_degenerated_gc_in_progress()) {
2612     verifier()-&gt;verify_roots_in_to_space_except(ShenandoahRootVerifier::ThreadRoots);
2613   }
2614 
2615   if (is_degenerated_gc_in_progress()) {
2616     concurrent_mark()-&gt;update_roots(ShenandoahPhaseTimings::degen_gc_update_roots);
2617   } else {
2618     concurrent_mark()-&gt;update_thread_roots(ShenandoahPhaseTimings::final_update_refs_roots);
2619   }
2620 
2621   // Has to be done before cset is clear
2622   if (ShenandoahVerify) {
2623     verifier()-&gt;verify_roots_in_to_space();
2624   }
2625 
2626   {
2627     ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_refs_update_region_states);
2628     ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl;
2629     parallel_heap_region_iterate(&amp;cl);
2630 
2631     assert_pinned_region_status();
2632   }
2633 
2634   {
2635     ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_refs_trash_cset);
2636     trash_cset_regions();
2637   }
2638 
2639   set_has_forwarded_objects(false);
2640   set_update_refs_in_progress(false);
2641 
2642   if (ShenandoahVerify) {
2643     verifier()-&gt;verify_after_updaterefs();
2644   }
2645 
2646   if (VerifyAfterGC) {
2647     Universe::verify();
2648   }
2649 
2650   {
2651     ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_refs_rebuild_freeset);
2652     ShenandoahHeapLocker locker(lock());
2653     _free_set-&gt;rebuild();
2654   }
2655 }
2656 
2657 void ShenandoahHeap::print_extended_on(outputStream *st) const {
2658   print_on(st);
2659   print_heap_regions_on(st);
2660 }
2661 
2662 bool ShenandoahHeap::is_bitmap_slice_committed(ShenandoahHeapRegion* r, bool skip_self) {
2663   size_t slice = r-&gt;index() / _bitmap_regions_per_slice;
2664 
2665   size_t regions_from = _bitmap_regions_per_slice * slice;
2666   size_t regions_to   = MIN2(num_regions(), _bitmap_regions_per_slice * (slice + 1));
2667   for (size_t g = regions_from; g &lt; regions_to; g++) {
2668     assert (g / _bitmap_regions_per_slice == slice, "same slice");
2669     if (skip_self &amp;&amp; g == r-&gt;index()) continue;
2670     if (get_region(g)-&gt;is_committed()) {
2671       return true;
2672     }
2673   }
2674   return false;
2675 }
2676 
2677 bool ShenandoahHeap::commit_bitmap_slice(ShenandoahHeapRegion* r) {
2678   shenandoah_assert_heaplocked();
2679 
2680   // Bitmaps in special regions do not need commits
2681   if (_bitmap_region_special) {
2682     return true;
2683   }
2684 
2685   if (is_bitmap_slice_committed(r, true)) {
2686     // Some other region from the group is already committed, meaning the bitmap
2687     // slice is already committed, we exit right away.
2688     return true;
2689   }
2690 
2691   // Commit the bitmap slice:
2692   size_t slice = r-&gt;index() / _bitmap_regions_per_slice;
2693   size_t off = _bitmap_bytes_per_slice * slice;
2694   size_t len = _bitmap_bytes_per_slice;
2695   char* start = (char*) _bitmap_region.start() + off;
2696 
2697   if (!os::commit_memory(start, len, false)) {
2698     return false;
2699   }
2700 
2701   if (AlwaysPreTouch) {
2702     os::pretouch_memory(start, start + len, _pretouch_bitmap_page_size);
2703   }
2704 
2705   return true;
2706 }
2707 
2708 bool ShenandoahHeap::uncommit_bitmap_slice(ShenandoahHeapRegion *r) {
2709   shenandoah_assert_heaplocked();
2710 
2711   // Bitmaps in special regions do not need uncommits
2712   if (_bitmap_region_special) {
2713     return true;
2714   }
2715 
2716   if (is_bitmap_slice_committed(r, true)) {
2717     // Some other region from the group is still committed, meaning the bitmap
2718     // slice is should stay committed, exit right away.
2719     return true;
2720   }
2721 
2722   // Uncommit the bitmap slice:
2723   size_t slice = r-&gt;index() / _bitmap_regions_per_slice;
2724   size_t off = _bitmap_bytes_per_slice * slice;
2725   size_t len = _bitmap_bytes_per_slice;
2726   if (!os::uncommit_memory((char*)_bitmap_region.start() + off, len)) {
2727     return false;
2728   }
2729   return true;
2730 }
2731 
2732 void ShenandoahHeap::safepoint_synchronize_begin() {
2733   if (ShenandoahSuspendibleWorkers || UseStringDeduplication) {
2734     SuspendibleThreadSet::synchronize();
2735   }
2736 }
2737 
2738 void ShenandoahHeap::safepoint_synchronize_end() {
2739   if (ShenandoahSuspendibleWorkers || UseStringDeduplication) {
2740     SuspendibleThreadSet::desynchronize();
2741   }
2742 }
2743 
2744 void ShenandoahHeap::vmop_entry_init_mark() {
2745   TraceCollectorStats tcs(monitoring_support()-&gt;stw_collection_counters());
2746   ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::init_mark_gross);
2747 
2748   try_inject_alloc_failure();
2749   VM_ShenandoahInitMark op;
2750   VMThread::execute(&amp;op); // jump to entry_init_mark() under safepoint
2751 }
2752 
2753 void ShenandoahHeap::vmop_entry_final_mark() {
2754   TraceCollectorStats tcs(monitoring_support()-&gt;stw_collection_counters());
2755   ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::final_mark_gross);
2756 
2757   try_inject_alloc_failure();
2758   VM_ShenandoahFinalMarkStartEvac op;
2759   VMThread::execute(&amp;op); // jump to entry_final_mark under safepoint
2760 }
2761 
2762 void ShenandoahHeap::vmop_entry_init_updaterefs() {
2763   TraceCollectorStats tcs(monitoring_support()-&gt;stw_collection_counters());
2764   ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::init_update_refs_gross);
2765 
2766   try_inject_alloc_failure();
2767   VM_ShenandoahInitUpdateRefs op;
2768   VMThread::execute(&amp;op);
2769 }
2770 
2771 void ShenandoahHeap::vmop_entry_final_updaterefs() {
2772   TraceCollectorStats tcs(monitoring_support()-&gt;stw_collection_counters());
2773   ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::final_update_refs_gross);
2774 
2775   try_inject_alloc_failure();
2776   VM_ShenandoahFinalUpdateRefs op;
2777   VMThread::execute(&amp;op);
2778 }
2779 
2780 void ShenandoahHeap::vmop_entry_full(GCCause::Cause cause) {
2781   TraceCollectorStats tcs(monitoring_support()-&gt;full_stw_collection_counters());
2782   ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::full_gc_gross);
2783 
2784   try_inject_alloc_failure();
2785   VM_ShenandoahFullGC op(cause);
2786   VMThread::execute(&amp;op);
2787 }
2788 
2789 void ShenandoahHeap::vmop_degenerated(ShenandoahDegenPoint point) {
2790   TraceCollectorStats tcs(monitoring_support()-&gt;full_stw_collection_counters());
2791   ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::degen_gc_gross);
2792 
2793   VM_ShenandoahDegeneratedGC degenerated_gc((int)point);
2794   VMThread::execute(&amp;degenerated_gc);
2795 }
2796 
2797 void ShenandoahHeap::entry_init_mark() {
2798   const char* msg = init_mark_event_message();
2799   ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::init_mark);
2800   EventMark em("%s", msg);
2801 
2802   ShenandoahWorkerScope scope(workers(),
2803                               ShenandoahWorkerPolicy::calc_workers_for_init_marking(),
2804                               "init marking");
2805 
2806   op_init_mark();
2807 }
2808 
2809 void ShenandoahHeap::entry_final_mark() {
2810   const char* msg = final_mark_event_message();
2811   ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::final_mark);
2812   EventMark em("%s", msg);
2813 
2814   ShenandoahWorkerScope scope(workers(),
2815                               ShenandoahWorkerPolicy::calc_workers_for_final_marking(),
2816                               "final marking");
2817 
2818   op_final_mark();
2819 }
2820 
2821 void ShenandoahHeap::entry_init_updaterefs() {
2822   static const char* msg = "Pause Init Update Refs";
2823   ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::init_update_refs);
2824   EventMark em("%s", msg);
2825 
2826   // No workers used in this phase, no setup required
2827 
2828   op_init_updaterefs();
2829 }
2830 
2831 void ShenandoahHeap::entry_final_updaterefs() {
2832   static const char* msg = "Pause Final Update Refs";
2833   ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::final_update_refs);
2834   EventMark em("%s", msg);
2835 
2836   ShenandoahWorkerScope scope(workers(),
2837                               ShenandoahWorkerPolicy::calc_workers_for_final_update_ref(),
2838                               "final reference update");
2839 
2840   op_final_updaterefs();
2841 }
2842 
2843 void ShenandoahHeap::entry_full(GCCause::Cause cause) {
2844   static const char* msg = "Pause Full";
2845   ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::full_gc, true /* log_heap_usage */);
2846   EventMark em("%s", msg);
2847 
2848   ShenandoahWorkerScope scope(workers(),
2849                               ShenandoahWorkerPolicy::calc_workers_for_fullgc(),
2850                               "full gc");
2851 
2852   op_full(cause);
2853 }
2854 
2855 void ShenandoahHeap::entry_degenerated(int point) {
2856   ShenandoahDegenPoint dpoint = (ShenandoahDegenPoint)point;
2857   const char* msg = degen_event_message(dpoint);
2858   ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::degen_gc, true /* log_heap_usage */);
2859   EventMark em("%s", msg);
2860 
2861   ShenandoahWorkerScope scope(workers(),
2862                               ShenandoahWorkerPolicy::calc_workers_for_stw_degenerated(),
2863                               "stw degenerated gc");
2864 
2865   set_degenerated_gc_in_progress(true);
2866   op_degenerated(dpoint);
2867   set_degenerated_gc_in_progress(false);
2868 }
2869 
2870 void ShenandoahHeap::entry_mark() {
2871   TraceCollectorStats tcs(monitoring_support()-&gt;concurrent_collection_counters());
2872 
2873   const char* msg = conc_mark_event_message();
2874   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_mark);
2875   EventMark em("%s", msg);
2876 
2877   ShenandoahWorkerScope scope(workers(),
2878                               ShenandoahWorkerPolicy::calc_workers_for_conc_marking(),
2879                               "concurrent marking");
2880 
2881   try_inject_alloc_failure();
2882   op_mark();
2883 }
2884 
2885 void ShenandoahHeap::entry_evac() {
2886   TraceCollectorStats tcs(monitoring_support()-&gt;concurrent_collection_counters());
2887 
2888   static const char* msg = "Concurrent evacuation";
2889   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_evac);
2890   EventMark em("%s", msg);
2891 
2892   ShenandoahWorkerScope scope(workers(),
2893                               ShenandoahWorkerPolicy::calc_workers_for_conc_evac(),
2894                               "concurrent evacuation");
2895 
2896   try_inject_alloc_failure();
2897   op_conc_evac();
2898 }
2899 
2900 void ShenandoahHeap::entry_updaterefs() {
2901   static const char* msg = "Concurrent update references";
2902   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_update_refs);
2903   EventMark em("%s", msg);
2904 
2905   ShenandoahWorkerScope scope(workers(),
2906                               ShenandoahWorkerPolicy::calc_workers_for_conc_update_ref(),
2907                               "concurrent reference update");
2908 
2909   try_inject_alloc_failure();
2910   op_updaterefs();
2911 }
2912 
2913 void ShenandoahHeap::entry_weak_roots() {
2914   static const char* msg = "Concurrent weak roots";
2915   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_weak_roots);
2916   EventMark em("%s", msg);
2917 
2918   ShenandoahWorkerScope scope(workers(),
2919                               ShenandoahWorkerPolicy::calc_workers_for_conc_root_processing(),
2920                               "concurrent weak root");
2921 
2922   try_inject_alloc_failure();
2923   op_weak_roots();
2924 }
2925 
2926 void ShenandoahHeap::entry_class_unloading() {
2927   static const char* msg = "Concurrent class unloading";
2928   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_class_unload);
2929   EventMark em("%s", msg);
2930 
2931   ShenandoahWorkerScope scope(workers(),
2932                               ShenandoahWorkerPolicy::calc_workers_for_conc_root_processing(),
2933                               "concurrent class unloading");
2934 
2935   try_inject_alloc_failure();
2936   op_class_unloading();
2937 }
2938 
2939 void ShenandoahHeap::entry_strong_roots() {
2940   static const char* msg = "Concurrent strong roots";
2941   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_strong_roots);
2942   EventMark em("%s", msg);
2943 
2944   ShenandoahGCWorkerPhase worker_phase(ShenandoahPhaseTimings::conc_strong_roots);
2945 
2946   ShenandoahWorkerScope scope(workers(),
2947                               ShenandoahWorkerPolicy::calc_workers_for_conc_root_processing(),
2948                               "concurrent strong root");
2949 
2950   try_inject_alloc_failure();
2951   op_strong_roots();
2952 }
2953 
2954 void ShenandoahHeap::entry_cleanup_early() {
2955   static const char* msg = "Concurrent cleanup";
2956   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_cleanup_early, true /* log_heap_usage */);
2957   EventMark em("%s", msg);
2958 
2959   // This phase does not use workers, no need for setup
2960 
2961   try_inject_alloc_failure();
2962   op_cleanup_early();
2963 }
2964 
2965 void ShenandoahHeap::entry_cleanup_complete() {
2966   static const char* msg = "Concurrent cleanup";
2967   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_cleanup_complete, true /* log_heap_usage */);
2968   EventMark em("%s", msg);
2969 
2970   // This phase does not use workers, no need for setup
2971 
2972   try_inject_alloc_failure();
2973   op_cleanup_complete();
2974 }
2975 
2976 void ShenandoahHeap::entry_reset() {
2977   static const char* msg = "Concurrent reset";
2978   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset);
2979   EventMark em("%s", msg);
2980 
2981   ShenandoahWorkerScope scope(workers(),
2982                               ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),
2983                               "concurrent reset");
2984 
2985   try_inject_alloc_failure();
2986   op_reset();
2987 }
2988 
2989 void ShenandoahHeap::entry_preclean() {
2990   if (ShenandoahPreclean &amp;&amp; process_references()) {
2991     static const char* msg = "Concurrent precleaning";
2992     ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_preclean);
2993     EventMark em("%s", msg);
2994 
2995     ShenandoahWorkerScope scope(workers(),
2996                                 ShenandoahWorkerPolicy::calc_workers_for_conc_preclean(),
2997                                 "concurrent preclean",
2998                                 /* check_workers = */ false);
2999 
3000     try_inject_alloc_failure();
3001     op_preclean();
3002   }
3003 }
3004 
3005 void ShenandoahHeap::entry_uncommit(double shrink_before) {
3006   static const char *msg = "Concurrent uncommit";
3007   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_uncommit, true /* log_heap_usage */);
3008   EventMark em("%s", msg);
3009 
3010   op_uncommit(shrink_before);
3011 }
3012 
3013 void ShenandoahHeap::try_inject_alloc_failure() {
3014   if (ShenandoahAllocFailureALot &amp;&amp; !cancelled_gc() &amp;&amp; ((os::random() % 1000) &gt; 950)) {
3015     _inject_alloc_failure.set();
3016     os::naked_short_sleep(1);
3017     if (cancelled_gc()) {
3018       log_info(gc)("Allocation failure was successfully injected");
3019     }
3020   }
3021 }
3022 
3023 bool ShenandoahHeap::should_inject_alloc_failure() {
3024   return _inject_alloc_failure.is_set() &amp;&amp; _inject_alloc_failure.try_unset();
3025 }
3026 
3027 void ShenandoahHeap::initialize_serviceability() {
3028   _memory_pool = new ShenandoahMemoryPool(this);
3029   _cycle_memory_manager.add_pool(_memory_pool);
3030   _stw_memory_manager.add_pool(_memory_pool);
3031 }
3032 
3033 GrowableArray&lt;GCMemoryManager*&gt; ShenandoahHeap::memory_managers() {
3034   GrowableArray&lt;GCMemoryManager*&gt; memory_managers(2);
3035   memory_managers.append(&amp;_cycle_memory_manager);
3036   memory_managers.append(&amp;_stw_memory_manager);
3037   return memory_managers;
3038 }
3039 
3040 GrowableArray&lt;MemoryPool*&gt; ShenandoahHeap::memory_pools() {
3041   GrowableArray&lt;MemoryPool*&gt; memory_pools(1);
3042   memory_pools.append(_memory_pool);
3043   return memory_pools;
3044 }
3045 
3046 MemoryUsage ShenandoahHeap::memory_usage() {
3047   return _memory_pool-&gt;get_memory_usage();
3048 }
3049 
3050 ShenandoahRegionIterator::ShenandoahRegionIterator() :
3051   _heap(ShenandoahHeap::heap()),
3052   _index(0) {}
3053 
3054 ShenandoahRegionIterator::ShenandoahRegionIterator(ShenandoahHeap* heap) :
3055   _heap(heap),
3056   _index(0) {}
3057 
3058 void ShenandoahRegionIterator::reset() {
3059   _index = 0;
3060 }
3061 
3062 bool ShenandoahRegionIterator::has_next() const {
3063   return _index &lt; _heap-&gt;num_regions();
3064 }
3065 
3066 char ShenandoahHeap::gc_state() const {
3067   return _gc_state.raw_value();
3068 }
3069 
3070 void ShenandoahHeap::deduplicate_string(oop str) {
3071   assert(java_lang_String::is_instance(str), "invariant");
3072 
3073   if (ShenandoahStringDedup::is_enabled()) {
3074     ShenandoahStringDedup::deduplicate(str);
3075   }
3076 }
3077 
3078 const char* ShenandoahHeap::init_mark_event_message() const {
3079   assert(!has_forwarded_objects(), "Should not have forwarded objects here");
3080 
3081   bool proc_refs = process_references();
3082   bool unload_cls = unload_classes();
3083 
3084   if (proc_refs &amp;&amp; unload_cls) {
3085     return "Pause Init Mark (process weakrefs) (unload classes)";
3086   } else if (proc_refs) {
3087     return "Pause Init Mark (process weakrefs)";
3088   } else if (unload_cls) {
3089     return "Pause Init Mark (unload classes)";
3090   } else {
3091     return "Pause Init Mark";
3092   }
3093 }
3094 
3095 const char* ShenandoahHeap::final_mark_event_message() const {
3096   assert(!has_forwarded_objects(), "Should not have forwarded objects here");
3097 
3098   bool proc_refs = process_references();
3099   bool unload_cls = unload_classes();
3100 
3101   if (proc_refs &amp;&amp; unload_cls) {
3102     return "Pause Final Mark (process weakrefs) (unload classes)";
3103   } else if (proc_refs) {
3104     return "Pause Final Mark (process weakrefs)";
3105   } else if (unload_cls) {
3106     return "Pause Final Mark (unload classes)";
3107   } else {
3108     return "Pause Final Mark";
3109   }
3110 }
3111 
3112 const char* ShenandoahHeap::conc_mark_event_message() const {
3113   assert(!has_forwarded_objects(), "Should not have forwarded objects here");
3114 
3115   bool proc_refs = process_references();
3116   bool unload_cls = unload_classes();
3117 
3118   if (proc_refs &amp;&amp; unload_cls) {
3119     return "Concurrent marking (process weakrefs) (unload classes)";
3120   } else if (proc_refs) {
3121     return "Concurrent marking (process weakrefs)";
3122   } else if (unload_cls) {
3123     return "Concurrent marking (unload classes)";
3124   } else {
3125     return "Concurrent marking";
3126   }
3127 }
3128 
3129 const char* ShenandoahHeap::degen_event_message(ShenandoahDegenPoint point) const {
3130   switch (point) {
3131     case _degenerated_unset:
3132       return "Pause Degenerated GC (&lt;UNSET&gt;)";
3133     case _degenerated_outside_cycle:
3134       return "Pause Degenerated GC (Outside of Cycle)";
3135     case _degenerated_mark:
3136       return "Pause Degenerated GC (Mark)";
3137     case _degenerated_evac:
3138       return "Pause Degenerated GC (Evacuation)";
3139     case _degenerated_updaterefs:
3140       return "Pause Degenerated GC (Update Refs)";
3141     default:
3142       ShouldNotReachHere();
3143       return "ERROR";
3144   }
3145 }
3146 
3147 ShenandoahLiveData* ShenandoahHeap::get_liveness_cache(uint worker_id) {
3148 #ifdef ASSERT
3149   assert(_liveness_cache != NULL, "sanity");
3150   assert(worker_id &lt; _max_workers, "sanity");
3151   for (uint i = 0; i &lt; num_regions(); i++) {
3152     assert(_liveness_cache[worker_id][i] == 0, "liveness cache should be empty");
3153   }
3154 #endif
3155   return _liveness_cache[worker_id];
3156 }
3157 
3158 void ShenandoahHeap::flush_liveness_cache(uint worker_id) {
3159   assert(worker_id &lt; _max_workers, "sanity");
3160   assert(_liveness_cache != NULL, "sanity");
3161   ShenandoahLiveData* ld = _liveness_cache[worker_id];
3162   for (uint i = 0; i &lt; num_regions(); i++) {
3163     ShenandoahLiveData live = ld[i];
3164     if (live &gt; 0) {
3165       ShenandoahHeapRegion* r = get_region(i);
3166       r-&gt;increase_live_data_gc_words(live);
3167       ld[i] = 0;
3168     }
3169   }
3170 }
<a name="3" id="anc3"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="3" type="hidden" /></form></body></html>
