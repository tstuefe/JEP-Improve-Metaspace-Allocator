<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>Old src/hotspot/share/gc/shenandoah/shenandoahHeap.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 2013, 2020, Red Hat, Inc. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "memory/allocation.hpp"
  27 #include "memory/universe.hpp"
  28 
  29 #include "gc/shared/gcArguments.hpp"
  30 #include "gc/shared/gcTimer.hpp"
  31 #include "gc/shared/gcTraceTime.inline.hpp"
  32 #include "gc/shared/locationPrinter.inline.hpp"
  33 #include "gc/shared/memAllocator.hpp"
  34 #include "gc/shared/oopStorageSet.hpp"
  35 #include "gc/shared/plab.hpp"
  36 
  37 #include "gc/shenandoah/shenandoahBarrierSet.hpp"
  38 #include "gc/shenandoah/shenandoahClosures.inline.hpp"
  39 #include "gc/shenandoah/shenandoahCollectionSet.hpp"
  40 #include "gc/shenandoah/shenandoahCollectorPolicy.hpp"
  41 #include "gc/shenandoah/shenandoahConcurrentMark.inline.hpp"
  42 #include "gc/shenandoah/shenandoahConcurrentRoots.hpp"
  43 #include "gc/shenandoah/shenandoahControlThread.hpp"
  44 #include "gc/shenandoah/shenandoahFreeSet.hpp"
  45 #include "gc/shenandoah/shenandoahPhaseTimings.hpp"
  46 #include "gc/shenandoah/shenandoahHeap.inline.hpp"
  47 #include "gc/shenandoah/shenandoahHeapRegion.inline.hpp"
  48 #include "gc/shenandoah/shenandoahHeapRegionSet.hpp"
  49 #include "gc/shenandoah/shenandoahInitLogger.hpp"
  50 #include "gc/shenandoah/shenandoahMarkCompact.hpp"
  51 #include "gc/shenandoah/shenandoahMarkingContext.inline.hpp"
  52 #include "gc/shenandoah/shenandoahMemoryPool.hpp"
  53 #include "gc/shenandoah/shenandoahMetrics.hpp"
  54 #include "gc/shenandoah/shenandoahMonitoringSupport.hpp"
  55 #include "gc/shenandoah/shenandoahOopClosures.inline.hpp"
  56 #include "gc/shenandoah/shenandoahPacer.inline.hpp"
  57 #include "gc/shenandoah/shenandoahPadding.hpp"
  58 #include "gc/shenandoah/shenandoahParallelCleaning.inline.hpp"
  59 #include "gc/shenandoah/shenandoahRootProcessor.inline.hpp"
  60 #include "gc/shenandoah/shenandoahStringDedup.hpp"
  61 #include "gc/shenandoah/shenandoahTaskqueue.hpp"
  62 #include "gc/shenandoah/shenandoahUtils.hpp"
  63 #include "gc/shenandoah/shenandoahVerifier.hpp"
  64 #include "gc/shenandoah/shenandoahCodeRoots.hpp"
  65 #include "gc/shenandoah/shenandoahVMOperations.hpp"
  66 #include "gc/shenandoah/shenandoahWorkGroup.hpp"
  67 #include "gc/shenandoah/shenandoahWorkerPolicy.hpp"
  68 #include "gc/shenandoah/mode/shenandoahIUMode.hpp"
  69 #include "gc/shenandoah/mode/shenandoahPassiveMode.hpp"
  70 #include "gc/shenandoah/mode/shenandoahSATBMode.hpp"
  71 #if INCLUDE_JFR
  72 #include "gc/shenandoah/shenandoahJfrSupport.hpp"
  73 #endif
  74 
  75 #include "memory/metaspace.hpp"
  76 #include "oops/compressedOops.inline.hpp"
  77 #include "runtime/atomic.hpp"
  78 #include "runtime/globals.hpp"
  79 #include "runtime/interfaceSupport.inline.hpp"
  80 #include "runtime/orderAccess.hpp"
  81 #include "runtime/safepointMechanism.hpp"
  82 #include "runtime/vmThread.hpp"
  83 #include "services/mallocTracker.hpp"
  84 #include "utilities/powerOfTwo.hpp"
  85 
  86 #ifdef ASSERT
  87 template &lt;class T&gt;
  88 void ShenandoahAssertToSpaceClosure::do_oop_work(T* p) {
  89   T o = RawAccess&lt;&gt;::oop_load(p);
  90   if (! CompressedOops::is_null(o)) {
  91     oop obj = CompressedOops::decode_not_null(o);
  92     shenandoah_assert_not_forwarded(p, obj);
  93   }
  94 }
  95 
  96 void ShenandoahAssertToSpaceClosure::do_oop(narrowOop* p) { do_oop_work(p); }
  97 void ShenandoahAssertToSpaceClosure::do_oop(oop* p)       { do_oop_work(p); }
  98 #endif
  99 
 100 class ShenandoahPretouchHeapTask : public AbstractGangTask {
 101 private:
 102   ShenandoahRegionIterator _regions;
 103   const size_t _page_size;
 104 public:
 105   ShenandoahPretouchHeapTask(size_t page_size) :
 106     AbstractGangTask("Shenandoah Pretouch Heap"),
 107     _page_size(page_size) {}
 108 
 109   virtual void work(uint worker_id) {
 110     ShenandoahHeapRegion* r = _regions.next();
 111     while (r != NULL) {
 112       if (r-&gt;is_committed()) {
 113         os::pretouch_memory(r-&gt;bottom(), r-&gt;end(), _page_size);
 114       }
 115       r = _regions.next();
 116     }
 117   }
 118 };
 119 
 120 class ShenandoahPretouchBitmapTask : public AbstractGangTask {
 121 private:
 122   ShenandoahRegionIterator _regions;
 123   char* _bitmap_base;
 124   const size_t _bitmap_size;
 125   const size_t _page_size;
 126 public:
 127   ShenandoahPretouchBitmapTask(char* bitmap_base, size_t bitmap_size, size_t page_size) :
 128     AbstractGangTask("Shenandoah Pretouch Bitmap"),
 129     _bitmap_base(bitmap_base),
 130     _bitmap_size(bitmap_size),
 131     _page_size(page_size) {}
 132 
 133   virtual void work(uint worker_id) {
 134     ShenandoahHeapRegion* r = _regions.next();
 135     while (r != NULL) {
 136       size_t start = r-&gt;index()       * ShenandoahHeapRegion::region_size_bytes() / MarkBitMap::heap_map_factor();
 137       size_t end   = (r-&gt;index() + 1) * ShenandoahHeapRegion::region_size_bytes() / MarkBitMap::heap_map_factor();
 138       assert (end &lt;= _bitmap_size, "end is sane: " SIZE_FORMAT " &lt; " SIZE_FORMAT, end, _bitmap_size);
 139 
 140       if (r-&gt;is_committed()) {
 141         os::pretouch_memory(_bitmap_base + start, _bitmap_base + end, _page_size);
 142       }
 143 
 144       r = _regions.next();
 145     }
 146   }
 147 };
 148 
 149 jint ShenandoahHeap::initialize() {
 150   //
 151   // Figure out heap sizing
 152   //
 153 
 154   size_t init_byte_size = InitialHeapSize;
 155   size_t min_byte_size  = MinHeapSize;
 156   size_t max_byte_size  = MaxHeapSize;
 157   size_t heap_alignment = HeapAlignment;
 158 
 159   size_t reg_size_bytes = ShenandoahHeapRegion::region_size_bytes();
 160 
 161   Universe::check_alignment(max_byte_size,  reg_size_bytes, "Shenandoah heap");
 162   Universe::check_alignment(init_byte_size, reg_size_bytes, "Shenandoah heap");
 163 
 164   _num_regions = ShenandoahHeapRegion::region_count();
 165 
 166   // Now we know the number of regions, initialize the heuristics.
 167   initialize_heuristics();
 168 
 169   size_t num_committed_regions = init_byte_size / reg_size_bytes;
 170   num_committed_regions = MIN2(num_committed_regions, _num_regions);
 171   assert(num_committed_regions &lt;= _num_regions, "sanity");
 172   _initial_size = num_committed_regions * reg_size_bytes;
 173 
 174   size_t num_min_regions = min_byte_size / reg_size_bytes;
 175   num_min_regions = MIN2(num_min_regions, _num_regions);
 176   assert(num_min_regions &lt;= _num_regions, "sanity");
 177   _minimum_size = num_min_regions * reg_size_bytes;
 178 
 179   _committed = _initial_size;
 180 
 181   size_t heap_page_size   = UseLargePages ? (size_t)os::large_page_size() : (size_t)os::vm_page_size();
 182   size_t bitmap_page_size = UseLargePages ? (size_t)os::large_page_size() : (size_t)os::vm_page_size();
 183   size_t region_page_size = UseLargePages ? (size_t)os::large_page_size() : (size_t)os::vm_page_size();
 184 
 185   //
 186   // Reserve and commit memory for heap
 187   //
 188 
 189   ReservedHeapSpace heap_rs = Universe::reserve_heap(max_byte_size, heap_alignment);
 190   initialize_reserved_region(heap_rs);
 191   _heap_region = MemRegion((HeapWord*)heap_rs.base(), heap_rs.size() / HeapWordSize);
 192   _heap_region_special = heap_rs.special();
 193 
 194   assert((((size_t) base()) &amp; ShenandoahHeapRegion::region_size_bytes_mask()) == 0,
 195          "Misaligned heap: " PTR_FORMAT, p2i(base()));
 196 
 197 #if SHENANDOAH_OPTIMIZED_OBJTASK
 198   // The optimized ObjArrayChunkedTask takes some bits away from the full object bits.
 199   // Fail if we ever attempt to address more than we can.
 200   if ((uintptr_t)heap_rs.end() &gt;= ObjArrayChunkedTask::max_addressable()) {
 201     FormatBuffer&lt;512&gt; buf("Shenandoah reserved [" PTR_FORMAT ", " PTR_FORMAT") for the heap, \n"
 202                           "but max object address is " PTR_FORMAT ". Try to reduce heap size, or try other \n"
 203                           "VM options that allocate heap at lower addresses (HeapBaseMinAddress, AllocateHeapAt, etc).",
 204                 p2i(heap_rs.base()), p2i(heap_rs.end()), ObjArrayChunkedTask::max_addressable());
 205     vm_exit_during_initialization("Fatal Error", buf);
 206   }
 207 #endif
 208 
 209   ReservedSpace sh_rs = heap_rs.first_part(max_byte_size);
 210   if (!_heap_region_special) {
 211     os::commit_memory_or_exit(sh_rs.base(), _initial_size, heap_alignment, false,
 212                               "Cannot commit heap memory");
 213   }
 214 
 215   //
 216   // Reserve and commit memory for bitmap(s)
 217   //
 218 
 219   _bitmap_size = MarkBitMap::compute_size(heap_rs.size());
 220   _bitmap_size = align_up(_bitmap_size, bitmap_page_size);
 221 
 222   size_t bitmap_bytes_per_region = reg_size_bytes / MarkBitMap::heap_map_factor();
 223 
 224   guarantee(bitmap_bytes_per_region != 0,
 225             "Bitmap bytes per region should not be zero");
 226   guarantee(is_power_of_2(bitmap_bytes_per_region),
 227             "Bitmap bytes per region should be power of two: " SIZE_FORMAT, bitmap_bytes_per_region);
 228 
 229   if (bitmap_page_size &gt; bitmap_bytes_per_region) {
 230     _bitmap_regions_per_slice = bitmap_page_size / bitmap_bytes_per_region;
 231     _bitmap_bytes_per_slice = bitmap_page_size;
 232   } else {
 233     _bitmap_regions_per_slice = 1;
 234     _bitmap_bytes_per_slice = bitmap_bytes_per_region;
 235   }
 236 
 237   guarantee(_bitmap_regions_per_slice &gt;= 1,
 238             "Should have at least one region per slice: " SIZE_FORMAT,
 239             _bitmap_regions_per_slice);
 240 
 241   guarantee(((_bitmap_bytes_per_slice) % bitmap_page_size) == 0,
 242             "Bitmap slices should be page-granular: bps = " SIZE_FORMAT ", page size = " SIZE_FORMAT,
 243             _bitmap_bytes_per_slice, bitmap_page_size);
 244 
 245   ReservedSpace bitmap(_bitmap_size, bitmap_page_size);
 246   MemTracker::record_virtual_memory_type(bitmap.base(), mtGC);
 247   _bitmap_region = MemRegion((HeapWord*) bitmap.base(), bitmap.size() / HeapWordSize);
 248   _bitmap_region_special = bitmap.special();
 249 
 250   size_t bitmap_init_commit = _bitmap_bytes_per_slice *
 251                               align_up(num_committed_regions, _bitmap_regions_per_slice) / _bitmap_regions_per_slice;
 252   bitmap_init_commit = MIN2(_bitmap_size, bitmap_init_commit);
 253   if (!_bitmap_region_special) {
 254     os::commit_memory_or_exit((char *) _bitmap_region.start(), bitmap_init_commit, bitmap_page_size, false,
 255                               "Cannot commit bitmap memory");
 256   }
 257 
 258   _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions);
 259 
 260   if (ShenandoahVerify) {
 261     ReservedSpace verify_bitmap(_bitmap_size, bitmap_page_size);
 262     if (!verify_bitmap.special()) {
 263       os::commit_memory_or_exit(verify_bitmap.base(), verify_bitmap.size(), bitmap_page_size, false,
 264                                 "Cannot commit verification bitmap memory");
 265     }
 266     MemTracker::record_virtual_memory_type(verify_bitmap.base(), mtGC);
 267     MemRegion verify_bitmap_region = MemRegion((HeapWord *) verify_bitmap.base(), verify_bitmap.size() / HeapWordSize);
 268     _verification_bit_map.initialize(_heap_region, verify_bitmap_region);
 269     _verifier = new ShenandoahVerifier(this, &amp;_verification_bit_map);
 270   }
 271 
 272   // Reserve aux bitmap for use in object_iterate(). We don't commit it here.
 273   ReservedSpace aux_bitmap(_bitmap_size, bitmap_page_size);
 274   MemTracker::record_virtual_memory_type(aux_bitmap.base(), mtGC);
 275   _aux_bitmap_region = MemRegion((HeapWord*) aux_bitmap.base(), aux_bitmap.size() / HeapWordSize);
 276   _aux_bitmap_region_special = aux_bitmap.special();
 277   _aux_bit_map.initialize(_heap_region, _aux_bitmap_region);
 278 
 279   //
 280   // Create regions and region sets
 281   //
 282   size_t region_align = align_up(sizeof(ShenandoahHeapRegion), SHENANDOAH_CACHE_LINE_SIZE);
 283   size_t region_storage_size = align_up(region_align * _num_regions, region_page_size);
 284   region_storage_size = align_up(region_storage_size, os::vm_allocation_granularity());
 285 
 286   ReservedSpace region_storage(region_storage_size, region_page_size);
 287   MemTracker::record_virtual_memory_type(region_storage.base(), mtGC);
 288   if (!region_storage.special()) {
 289     os::commit_memory_or_exit(region_storage.base(), region_storage_size, region_page_size, false,
 290                               "Cannot commit region memory");
 291   }
 292 
 293   // Try to fit the collection set bitmap at lower addresses. This optimizes code generation for cset checks.
 294   // Go up until a sensible limit (subject to encoding constraints) and try to reserve the space there.
 295   // If not successful, bite a bullet and allocate at whatever address.
 296   {
 297     size_t cset_align = MAX2&lt;size_t&gt;(os::vm_page_size(), os::vm_allocation_granularity());
 298     size_t cset_size = align_up(((size_t) sh_rs.base() + sh_rs.size()) &gt;&gt; ShenandoahHeapRegion::region_size_bytes_shift(), cset_align);
 299 
 300     uintptr_t min = round_up_power_of_2(cset_align);
 301     uintptr_t max = (1u &lt;&lt; 30u);
 302 
 303     for (uintptr_t addr = min; addr &lt;= max; addr &lt;&lt;= 1u) {
 304       char* req_addr = (char*)addr;
 305       assert(is_aligned(req_addr, cset_align), "Should be aligned");
 306       ReservedSpace cset_rs(cset_size, cset_align, false, req_addr);
 307       if (cset_rs.is_reserved()) {
 308         assert(cset_rs.base() == req_addr, "Allocated where requested: " PTR_FORMAT ", " PTR_FORMAT, p2i(cset_rs.base()), addr);
 309         _collection_set = new ShenandoahCollectionSet(this, cset_rs, sh_rs.base());
 310         break;
 311       }
 312     }
 313 
 314     if (_collection_set == NULL) {
 315       ReservedSpace cset_rs(cset_size, cset_align, false);
 316       _collection_set = new ShenandoahCollectionSet(this, cset_rs, sh_rs.base());
 317     }
 318   }
 319 
 320   _regions = NEW_C_HEAP_ARRAY(ShenandoahHeapRegion*, _num_regions, mtGC);
 321   _free_set = new ShenandoahFreeSet(this, _num_regions);
 322 
 323   {
 324     ShenandoahHeapLocker locker(lock());
 325 
 326     for (size_t i = 0; i &lt; _num_regions; i++) {
 327       HeapWord* start = (HeapWord*)sh_rs.base() + ShenandoahHeapRegion::region_size_words() * i;
 328       bool is_committed = i &lt; num_committed_regions;
 329       void* loc = region_storage.base() + i * region_align;
 330 
 331       ShenandoahHeapRegion* r = new (loc) ShenandoahHeapRegion(start, i, is_committed);
 332       assert(is_aligned(r, SHENANDOAH_CACHE_LINE_SIZE), "Sanity");
 333 
 334       _marking_context-&gt;initialize_top_at_mark_start(r);
 335       _regions[i] = r;
 336       assert(!collection_set()-&gt;is_in(i), "New region should not be in collection set");
 337     }
 338 
 339     // Initialize to complete
 340     _marking_context-&gt;mark_complete();
 341 
 342     _free_set-&gt;rebuild();
 343   }
 344 
 345   if (AlwaysPreTouch) {
 346     // For NUMA, it is important to pre-touch the storage under bitmaps with worker threads,
 347     // before initialize() below zeroes it with initializing thread. For any given region,
 348     // we touch the region and the corresponding bitmaps from the same thread.
 349     ShenandoahPushWorkerScope scope(workers(), _max_workers, false);
 350 
 351     _pretouch_heap_page_size = heap_page_size;
 352     _pretouch_bitmap_page_size = bitmap_page_size;
 353 
 354 #ifdef LINUX
 355     // UseTransparentHugePages would madvise that backing memory can be coalesced into huge
 356     // pages. But, the kernel needs to know that every small page is used, in order to coalesce
 357     // them into huge one. Therefore, we need to pretouch with smaller pages.
 358     if (UseTransparentHugePages) {
 359       _pretouch_heap_page_size = (size_t)os::vm_page_size();
 360       _pretouch_bitmap_page_size = (size_t)os::vm_page_size();
 361     }
 362 #endif
 363 
 364     // OS memory managers may want to coalesce back-to-back pages. Make their jobs
 365     // simpler by pre-touching continuous spaces (heap and bitmap) separately.
 366 
 367     ShenandoahPretouchBitmapTask bcl(bitmap.base(), _bitmap_size, _pretouch_bitmap_page_size);
 368     _workers-&gt;run_task(&amp;bcl);
 369 
 370     ShenandoahPretouchHeapTask hcl(_pretouch_heap_page_size);
 371     _workers-&gt;run_task(&amp;hcl);
 372   }
 373 
 374   //
 375   // Initialize the rest of GC subsystems
 376   //
 377 
 378   _liveness_cache = NEW_C_HEAP_ARRAY(ShenandoahLiveData*, _max_workers, mtGC);
 379   for (uint worker = 0; worker &lt; _max_workers; worker++) {
 380     _liveness_cache[worker] = NEW_C_HEAP_ARRAY(ShenandoahLiveData, _num_regions, mtGC);
 381     Copy::fill_to_bytes(_liveness_cache[worker], _num_regions * sizeof(ShenandoahLiveData));
 382   }
 383 
 384   // There should probably be Shenandoah-specific options for these,
 385   // just as there are G1-specific options.
 386   {
 387     ShenandoahSATBMarkQueueSet&amp; satbqs = ShenandoahBarrierSet::satb_mark_queue_set();
 388     satbqs.set_process_completed_buffers_threshold(20); // G1SATBProcessCompletedThreshold
 389     satbqs.set_buffer_enqueue_threshold_percentage(60); // G1SATBBufferEnqueueingThresholdPercent
 390   }
 391 
 392   _monitoring_support = new ShenandoahMonitoringSupport(this);
 393   _phase_timings = new ShenandoahPhaseTimings(max_workers());
 394   ShenandoahStringDedup::initialize();
 395   ShenandoahCodeRoots::initialize();
 396 
 397   if (ShenandoahPacing) {
 398     _pacer = new ShenandoahPacer(this);
 399     _pacer-&gt;setup_for_idle();
 400   } else {
 401     _pacer = NULL;
 402   }
 403 
 404   _control_thread = new ShenandoahControlThread();
 405 
 406   _ref_proc_mt_processing = ParallelRefProcEnabled &amp;&amp; (ParallelGCThreads &gt; 1);
 407   _ref_proc_mt_discovery = _max_workers &gt; 1;
 408 
 409   ShenandoahInitLogger::print();
 410 
 411   return JNI_OK;
 412 }
 413 
 414 void ShenandoahHeap::initialize_heuristics() {
 415   if (ShenandoahGCMode != NULL) {
 416     if (strcmp(ShenandoahGCMode, "satb") == 0) {
 417       _gc_mode = new ShenandoahSATBMode();
 418     } else if (strcmp(ShenandoahGCMode, "iu") == 0) {
 419       _gc_mode = new ShenandoahIUMode();
 420     } else if (strcmp(ShenandoahGCMode, "passive") == 0) {
 421       _gc_mode = new ShenandoahPassiveMode();
 422     } else {
 423       vm_exit_during_initialization("Unknown -XX:ShenandoahGCMode option");
 424     }
 425   } else {
 426     ShouldNotReachHere();
 427   }
 428   _gc_mode-&gt;initialize_flags();
 429   if (_gc_mode-&gt;is_diagnostic() &amp;&amp; !UnlockDiagnosticVMOptions) {
 430     vm_exit_during_initialization(
 431             err_msg("GC mode \"%s\" is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.",
 432                     _gc_mode-&gt;name()));
 433   }
 434   if (_gc_mode-&gt;is_experimental() &amp;&amp; !UnlockExperimentalVMOptions) {
 435     vm_exit_during_initialization(
 436             err_msg("GC mode \"%s\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.",
 437                     _gc_mode-&gt;name()));
 438   }
 439 
 440   _heuristics = _gc_mode-&gt;initialize_heuristics();
 441 
 442   if (_heuristics-&gt;is_diagnostic() &amp;&amp; !UnlockDiagnosticVMOptions) {
 443     vm_exit_during_initialization(
 444             err_msg("Heuristics \"%s\" is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.",
 445                     _heuristics-&gt;name()));
 446   }
 447   if (_heuristics-&gt;is_experimental() &amp;&amp; !UnlockExperimentalVMOptions) {
 448     vm_exit_during_initialization(
 449             err_msg("Heuristics \"%s\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.",
 450                     _heuristics-&gt;name()));
 451   }
 452 }
 453 
 454 #ifdef _MSC_VER
 455 #pragma warning( push )
 456 #pragma warning( disable:4355 ) // 'this' : used in base member initializer list
 457 #endif
 458 
 459 ShenandoahHeap::ShenandoahHeap(ShenandoahCollectorPolicy* policy) :
 460   CollectedHeap(),
 461   _initial_size(0),
 462   _used(0),
 463   _committed(0),
 464   _bytes_allocated_since_gc_start(0),
 465   _max_workers(MAX2(ConcGCThreads, ParallelGCThreads)),
 466   _workers(NULL),
 467   _safepoint_workers(NULL),
 468   _heap_region_special(false),
 469   _num_regions(0),
 470   _regions(NULL),
 471   _update_refs_iterator(this),
 472   _control_thread(NULL),
 473   _shenandoah_policy(policy),
 474   _heuristics(NULL),
 475   _free_set(NULL),
 476   _scm(new ShenandoahConcurrentMark()),
 477   _full_gc(new ShenandoahMarkCompact()),
 478   _pacer(NULL),
 479   _verifier(NULL),
 480   _phase_timings(NULL),
 481   _monitoring_support(NULL),
 482   _memory_pool(NULL),
 483   _stw_memory_manager("Shenandoah Pauses", "end of GC pause"),
 484   _cycle_memory_manager("Shenandoah Cycles", "end of GC cycle"),
 485   _gc_timer(new (ResourceObj::C_HEAP, mtGC) ConcurrentGCTimer()),
 486   _soft_ref_policy(),
 487   _log_min_obj_alignment_in_bytes(LogMinObjAlignmentInBytes),
 488   _ref_processor(NULL),
 489   _marking_context(NULL),
 490   _bitmap_size(0),
 491   _bitmap_regions_per_slice(0),
 492   _bitmap_bytes_per_slice(0),
 493   _bitmap_region_special(false),
 494   _aux_bitmap_region_special(false),
 495   _liveness_cache(NULL),
 496   _collection_set(NULL)
 497 {
 498   BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this));
 499 
 500   _max_workers = MAX2(_max_workers, 1U);
 501   _workers = new ShenandoahWorkGang("Shenandoah GC Threads", _max_workers,
 502                             /* are_GC_task_threads */ true,
 503                             /* are_ConcurrentGC_threads */ true);
 504   if (_workers == NULL) {
 505     vm_exit_during_initialization("Failed necessary allocation.");
 506   } else {
 507     _workers-&gt;initialize_workers();
 508   }
 509 
 510   if (ParallelGCThreads &gt; 1) {
 511     _safepoint_workers = new ShenandoahWorkGang("Safepoint Cleanup Thread",
 512                                                 ParallelGCThreads,
 513                       /* are_GC_task_threads */ false,
 514                  /* are_ConcurrentGC_threads */ false);
 515     _safepoint_workers-&gt;initialize_workers();
 516   }
 517 }
 518 
 519 #ifdef _MSC_VER
 520 #pragma warning( pop )
 521 #endif
 522 
 523 class ShenandoahResetBitmapTask : public AbstractGangTask {
 524 private:
 525   ShenandoahRegionIterator _regions;
 526 
 527 public:
 528   ShenandoahResetBitmapTask() :
 529     AbstractGangTask("Parallel Reset Bitmap Task") {}
 530 
 531   void work(uint worker_id) {
 532     ShenandoahHeapRegion* region = _regions.next();
 533     ShenandoahHeap* heap = ShenandoahHeap::heap();
 534     ShenandoahMarkingContext* const ctx = heap-&gt;marking_context();
 535     while (region != NULL) {
 536       if (heap-&gt;is_bitmap_slice_committed(region)) {
 537         ctx-&gt;clear_bitmap(region);
 538       }
 539       region = _regions.next();
 540     }
 541   }
 542 };
 543 
 544 void ShenandoahHeap::reset_mark_bitmap() {
 545   assert_gc_workers(_workers-&gt;active_workers());
 546   mark_incomplete_marking_context();
 547 
 548   ShenandoahResetBitmapTask task;
 549   _workers-&gt;run_task(&amp;task);
 550 }
 551 
 552 void ShenandoahHeap::print_on(outputStream* st) const {
 553   st-&gt;print_cr("Shenandoah Heap");
 554   st-&gt;print_cr(" " SIZE_FORMAT "%s total, " SIZE_FORMAT "%s committed, " SIZE_FORMAT "%s used",
 555                byte_size_in_proper_unit(max_capacity()), proper_unit_for_byte_size(max_capacity()),
 556                byte_size_in_proper_unit(committed()),    proper_unit_for_byte_size(committed()),
 557                byte_size_in_proper_unit(used()),         proper_unit_for_byte_size(used()));
 558   st-&gt;print_cr(" " SIZE_FORMAT " x " SIZE_FORMAT"%s regions",
 559                num_regions(),
 560                byte_size_in_proper_unit(ShenandoahHeapRegion::region_size_bytes()),
 561                proper_unit_for_byte_size(ShenandoahHeapRegion::region_size_bytes()));
 562 
 563   st-&gt;print("Status: ");
 564   if (has_forwarded_objects())                 st-&gt;print("has forwarded objects, ");
 565   if (is_concurrent_mark_in_progress())        st-&gt;print("marking, ");
 566   if (is_evacuation_in_progress())             st-&gt;print("evacuating, ");
 567   if (is_update_refs_in_progress())            st-&gt;print("updating refs, ");
 568   if (is_degenerated_gc_in_progress())         st-&gt;print("degenerated gc, ");
 569   if (is_full_gc_in_progress())                st-&gt;print("full gc, ");
 570   if (is_full_gc_move_in_progress())           st-&gt;print("full gc move, ");
 571   if (is_concurrent_weak_root_in_progress())   st-&gt;print("concurrent weak roots, ");
 572   if (is_concurrent_strong_root_in_progress() &amp;&amp;
 573       !is_concurrent_weak_root_in_progress())  st-&gt;print("concurrent strong roots, ");
 574 
 575   if (cancelled_gc()) {
 576     st-&gt;print("cancelled");
 577   } else {
 578     st-&gt;print("not cancelled");
 579   }
 580   st-&gt;cr();
 581 
 582   st-&gt;print_cr("Reserved region:");
 583   st-&gt;print_cr(" - [" PTR_FORMAT ", " PTR_FORMAT ") ",
 584                p2i(reserved_region().start()),
 585                p2i(reserved_region().end()));
 586 
 587   ShenandoahCollectionSet* cset = collection_set();
 588   st-&gt;print_cr("Collection set:");
 589   if (cset != NULL) {
 590     st-&gt;print_cr(" - map (vanilla): " PTR_FORMAT, p2i(cset-&gt;map_address()));
 591     st-&gt;print_cr(" - map (biased):  " PTR_FORMAT, p2i(cset-&gt;biased_map_address()));
 592   } else {
 593     st-&gt;print_cr(" (NULL)");
 594   }
 595 
 596   st-&gt;cr();
 597   MetaspaceUtils::print_on(st);
 598 
 599   if (Verbose) {
 600     print_heap_regions_on(st);
 601   }
 602 }
 603 
 604 class ShenandoahInitWorkerGCLABClosure : public ThreadClosure {
 605 public:
 606   void do_thread(Thread* thread) {
 607     assert(thread != NULL, "Sanity");
 608     assert(thread-&gt;is_Worker_thread(), "Only worker thread expected");
 609     ShenandoahThreadLocalData::initialize_gclab(thread);
 610   }
 611 };
 612 
 613 void ShenandoahHeap::post_initialize() {
 614   CollectedHeap::post_initialize();
 615   MutexLocker ml(Threads_lock);
 616 
 617   ShenandoahInitWorkerGCLABClosure init_gclabs;
 618   _workers-&gt;threads_do(&amp;init_gclabs);
 619 
 620   // gclab can not be initialized early during VM startup, as it can not determinate its max_size.
 621   // Now, we will let WorkGang to initialize gclab when new worker is created.
 622   _workers-&gt;set_initialize_gclab();
 623 
 624   _scm-&gt;initialize(_max_workers);
 625   _full_gc-&gt;initialize(_gc_timer);
 626 
 627   ref_processing_init();
 628 
 629   _heuristics-&gt;initialize();
 630 
 631   JFR_ONLY(ShenandoahJFRSupport::register_jfr_type_serializers());
 632 }
 633 
 634 size_t ShenandoahHeap::used() const {
 635   return Atomic::load_acquire(&amp;_used);
 636 }
 637 
 638 size_t ShenandoahHeap::committed() const {
 639   OrderAccess::acquire();
 640   return _committed;
 641 }
 642 
 643 void ShenandoahHeap::increase_committed(size_t bytes) {
 644   shenandoah_assert_heaplocked_or_safepoint();
 645   _committed += bytes;
 646 }
 647 
 648 void ShenandoahHeap::decrease_committed(size_t bytes) {
 649   shenandoah_assert_heaplocked_or_safepoint();
 650   _committed -= bytes;
 651 }
 652 
 653 void ShenandoahHeap::increase_used(size_t bytes) {
 654   Atomic::add(&amp;_used, bytes);
 655 }
 656 
 657 void ShenandoahHeap::set_used(size_t bytes) {
 658   Atomic::release_store_fence(&amp;_used, bytes);
 659 }
 660 
 661 void ShenandoahHeap::decrease_used(size_t bytes) {
 662   assert(used() &gt;= bytes, "never decrease heap size by more than we've left");
 663   Atomic::sub(&amp;_used, bytes);
 664 }
 665 
 666 void ShenandoahHeap::increase_allocated(size_t bytes) {
 667   Atomic::add(&amp;_bytes_allocated_since_gc_start, bytes);
 668 }
 669 
 670 void ShenandoahHeap::notify_mutator_alloc_words(size_t words, bool waste) {
 671   size_t bytes = words * HeapWordSize;
 672   if (!waste) {
 673     increase_used(bytes);
 674   }
 675   increase_allocated(bytes);
 676   if (ShenandoahPacing) {
 677     control_thread()-&gt;pacing_notify_alloc(words);
 678     if (waste) {
 679       pacer()-&gt;claim_for_alloc(words, true);
 680     }
 681   }
 682 }
 683 
 684 size_t ShenandoahHeap::capacity() const {
 685   return committed();
 686 }
 687 
 688 size_t ShenandoahHeap::max_capacity() const {
 689   return _num_regions * ShenandoahHeapRegion::region_size_bytes();
 690 }
 691 
 692 size_t ShenandoahHeap::min_capacity() const {
 693   return _minimum_size;
 694 }
 695 
 696 size_t ShenandoahHeap::initial_capacity() const {
 697   return _initial_size;
 698 }
 699 
 700 bool ShenandoahHeap::is_in(const void* p) const {
 701   HeapWord* heap_base = (HeapWord*) base();
 702   HeapWord* last_region_end = heap_base + ShenandoahHeapRegion::region_size_words() * num_regions();
 703   return p &gt;= heap_base &amp;&amp; p &lt; last_region_end;
 704 }
 705 
 706 void ShenandoahHeap::op_uncommit(double shrink_before) {
 707   assert (ShenandoahUncommit, "should be enabled");
 708 
 709   // Application allocates from the beginning of the heap, and GC allocates at
 710   // the end of it. It is more efficient to uncommit from the end, so that applications
 711   // could enjoy the near committed regions. GC allocations are much less frequent,
 712   // and therefore can accept the committing costs.
 713 
 714   size_t count = 0;
 715   for (size_t i = num_regions(); i &gt; 0; i--) { // care about size_t underflow
 716     ShenandoahHeapRegion* r = get_region(i - 1);
 717     if (r-&gt;is_empty_committed() &amp;&amp; (r-&gt;empty_time() &lt; shrink_before)) {
 718       ShenandoahHeapLocker locker(lock());
 719       if (r-&gt;is_empty_committed()) {
 720         // Do not uncommit below minimal capacity
 721         if (committed() &lt; min_capacity() + ShenandoahHeapRegion::region_size_bytes()) {
 722           break;
 723         }
 724 
 725         r-&gt;make_uncommitted();
 726         count++;
 727       }
 728     }
 729     SpinPause(); // allow allocators to take the lock
 730   }
 731 
 732   if (count &gt; 0) {
 733     control_thread()-&gt;notify_heap_changed();
 734   }
 735 }
 736 
 737 HeapWord* ShenandoahHeap::allocate_from_gclab_slow(Thread* thread, size_t size) {
 738   // New object should fit the GCLAB size
 739   size_t min_size = MAX2(size, PLAB::min_size());
 740 
 741   // Figure out size of new GCLAB, looking back at heuristics. Expand aggressively.
 742   size_t new_size = ShenandoahThreadLocalData::gclab_size(thread) * 2;
 743   new_size = MIN2(new_size, PLAB::max_size());
 744   new_size = MAX2(new_size, PLAB::min_size());
 745 
 746   // Record new heuristic value even if we take any shortcut. This captures
 747   // the case when moderately-sized objects always take a shortcut. At some point,
 748   // heuristics should catch up with them.
 749   ShenandoahThreadLocalData::set_gclab_size(thread, new_size);
 750 
 751   if (new_size &lt; size) {
 752     // New size still does not fit the object. Fall back to shared allocation.
 753     // This avoids retiring perfectly good GCLABs, when we encounter a large object.
 754     return NULL;
 755   }
 756 
 757   // Retire current GCLAB, and allocate a new one.
 758   PLAB* gclab = ShenandoahThreadLocalData::gclab(thread);
 759   gclab-&gt;retire();
 760 
 761   size_t actual_size = 0;
 762   HeapWord* gclab_buf = allocate_new_gclab(min_size, new_size, &amp;actual_size);
 763   if (gclab_buf == NULL) {
 764     return NULL;
 765   }
 766 
 767   assert (size &lt;= actual_size, "allocation should fit");
 768 
 769   if (ZeroTLAB) {
 770     // ..and clear it.
 771     Copy::zero_to_words(gclab_buf, actual_size);
 772   } else {
 773     // ...and zap just allocated object.
 774 #ifdef ASSERT
 775     // Skip mangling the space corresponding to the object header to
 776     // ensure that the returned space is not considered parsable by
 777     // any concurrent GC thread.
 778     size_t hdr_size = oopDesc::header_size();
 779     Copy::fill_to_words(gclab_buf + hdr_size, actual_size - hdr_size, badHeapWordVal);
 780 #endif // ASSERT
 781   }
 782   gclab-&gt;set_buf(gclab_buf, actual_size);
 783   return gclab-&gt;allocate(size);
 784 }
 785 
 786 HeapWord* ShenandoahHeap::allocate_new_tlab(size_t min_size,
 787                                             size_t requested_size,
 788                                             size_t* actual_size) {
 789   ShenandoahAllocRequest req = ShenandoahAllocRequest::for_tlab(min_size, requested_size);
 790   HeapWord* res = allocate_memory(req);
 791   if (res != NULL) {
 792     *actual_size = req.actual_size();
 793   } else {
 794     *actual_size = 0;
 795   }
 796   return res;
 797 }
 798 
 799 HeapWord* ShenandoahHeap::allocate_new_gclab(size_t min_size,
 800                                              size_t word_size,
 801                                              size_t* actual_size) {
 802   ShenandoahAllocRequest req = ShenandoahAllocRequest::for_gclab(min_size, word_size);
 803   HeapWord* res = allocate_memory(req);
 804   if (res != NULL) {
 805     *actual_size = req.actual_size();
 806   } else {
 807     *actual_size = 0;
 808   }
 809   return res;
 810 }
 811 
 812 HeapWord* ShenandoahHeap::allocate_memory(ShenandoahAllocRequest&amp; req) {
 813   intptr_t pacer_epoch = 0;
 814   bool in_new_region = false;
 815   HeapWord* result = NULL;
 816 
 817   if (req.is_mutator_alloc()) {
 818     if (ShenandoahPacing) {
 819       pacer()-&gt;pace_for_alloc(req.size());
 820       pacer_epoch = pacer()-&gt;epoch();
 821     }
 822 
 823     if (!ShenandoahAllocFailureALot || !should_inject_alloc_failure()) {
 824       result = allocate_memory_under_lock(req, in_new_region);
 825     }
 826 
 827     // Allocation failed, block until control thread reacted, then retry allocation.
 828     //
 829     // It might happen that one of the threads requesting allocation would unblock
 830     // way later after GC happened, only to fail the second allocation, because
 831     // other threads have already depleted the free storage. In this case, a better
 832     // strategy is to try again, as long as GC makes progress.
 833     //
 834     // Then, we need to make sure the allocation was retried after at least one
 835     // Full GC, which means we want to try more than ShenandoahFullGCThreshold times.
 836 
 837     size_t tries = 0;
 838 
 839     while (result == NULL &amp;&amp; _progress_last_gc.is_set()) {
 840       tries++;
 841       control_thread()-&gt;handle_alloc_failure(req);
 842       result = allocate_memory_under_lock(req, in_new_region);
 843     }
 844 
 845     while (result == NULL &amp;&amp; tries &lt;= ShenandoahFullGCThreshold) {
 846       tries++;
 847       control_thread()-&gt;handle_alloc_failure(req);
 848       result = allocate_memory_under_lock(req, in_new_region);
 849     }
 850 
 851   } else {
 852     assert(req.is_gc_alloc(), "Can only accept GC allocs here");
 853     result = allocate_memory_under_lock(req, in_new_region);
 854     // Do not call handle_alloc_failure() here, because we cannot block.
 855     // The allocation failure would be handled by the LRB slowpath with handle_alloc_failure_evac().
 856   }
 857 
 858   if (in_new_region) {
 859     control_thread()-&gt;notify_heap_changed();
 860   }
 861 
 862   if (result != NULL) {
 863     size_t requested = req.size();
 864     size_t actual = req.actual_size();
 865 
 866     assert (req.is_lab_alloc() || (requested == actual),
 867             "Only LAB allocations are elastic: %s, requested = " SIZE_FORMAT ", actual = " SIZE_FORMAT,
 868             ShenandoahAllocRequest::alloc_type_to_string(req.type()), requested, actual);
 869 
 870     if (req.is_mutator_alloc()) {
 871       notify_mutator_alloc_words(actual, false);
 872 
 873       // If we requested more than we were granted, give the rest back to pacer.
 874       // This only matters if we are in the same pacing epoch: do not try to unpace
 875       // over the budget for the other phase.
 876       if (ShenandoahPacing &amp;&amp; (pacer_epoch &gt; 0) &amp;&amp; (requested &gt; actual)) {
 877         pacer()-&gt;unpace_for_alloc(pacer_epoch, requested - actual);
 878       }
 879     } else {
 880       increase_used(actual*HeapWordSize);
 881     }
 882   }
 883 
 884   return result;
 885 }
 886 
 887 HeapWord* ShenandoahHeap::allocate_memory_under_lock(ShenandoahAllocRequest&amp; req, bool&amp; in_new_region) {
 888   ShenandoahHeapLocker locker(lock());
 889   return _free_set-&gt;allocate(req, in_new_region);
 890 }
 891 
 892 HeapWord* ShenandoahHeap::mem_allocate(size_t size,
 893                                         bool*  gc_overhead_limit_was_exceeded) {
 894   ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared(size);
 895   return allocate_memory(req);
 896 }
 897 
 898 MetaWord* ShenandoahHeap::satisfy_failed_metadata_allocation(ClassLoaderData* loader_data,
 899                                                              size_t size,
 900                                                              Metaspace::MetadataType mdtype) {
 901   MetaWord* result;
 902 
 903   // Inform metaspace OOM to GC heuristics if class unloading is possible.
 904   if (heuristics()-&gt;can_unload_classes()) {
 905     ShenandoahHeuristics* h = heuristics();
 906     h-&gt;record_metaspace_oom();
 907   }
 908 
 909   // Expand and retry allocation
 910   result = loader_data-&gt;metaspace_non_null()-&gt;expand_and_allocate(size, mdtype);
 911   if (result != NULL) {
 912     return result;
 913   }
 914 
 915   // Start full GC
 916   collect(GCCause::_metadata_GC_clear_soft_refs);
 917 
 918   // Retry allocation
 919   result = loader_data-&gt;metaspace_non_null()-&gt;allocate(size, mdtype);
 920   if (result != NULL) {
 921     return result;
 922   }
 923 
 924   // Expand and retry allocation
 925   result = loader_data-&gt;metaspace_non_null()-&gt;expand_and_allocate(size, mdtype);
 926   if (result != NULL) {
 927     return result;
 928   }
 929 
 930   // Out of memory
 931   return NULL;
 932 }
 933 
 934 class ShenandoahConcurrentEvacuateRegionObjectClosure : public ObjectClosure {
 935 private:
 936   ShenandoahHeap* const _heap;
 937   Thread* const _thread;
 938 public:
 939   ShenandoahConcurrentEvacuateRegionObjectClosure(ShenandoahHeap* heap) :
 940     _heap(heap), _thread(Thread::current()) {}
 941 
 942   void do_object(oop p) {
 943     shenandoah_assert_marked(NULL, p);
 944     if (!p-&gt;is_forwarded()) {
 945       _heap-&gt;evacuate_object(p, _thread);
 946     }
 947   }
 948 };
 949 
 950 class ShenandoahEvacuationTask : public AbstractGangTask {
 951 private:
 952   ShenandoahHeap* const _sh;
 953   ShenandoahCollectionSet* const _cs;
 954   bool _concurrent;
 955 public:
 956   ShenandoahEvacuationTask(ShenandoahHeap* sh,
 957                            ShenandoahCollectionSet* cs,
 958                            bool concurrent) :
 959     AbstractGangTask("Parallel Evacuation Task"),
 960     _sh(sh),
 961     _cs(cs),
 962     _concurrent(concurrent)
 963   {}
 964 
 965   void work(uint worker_id) {
 966     if (_concurrent) {
 967       ShenandoahConcurrentWorkerSession worker_session(worker_id);
 968       ShenandoahSuspendibleThreadSetJoiner stsj(ShenandoahSuspendibleWorkers);
 969       ShenandoahEvacOOMScope oom_evac_scope;
 970       do_work();
 971     } else {
 972       ShenandoahParallelWorkerSession worker_session(worker_id);
 973       ShenandoahEvacOOMScope oom_evac_scope;
 974       do_work();
 975     }
 976   }
 977 
 978 private:
 979   void do_work() {
 980     ShenandoahConcurrentEvacuateRegionObjectClosure cl(_sh);
 981     ShenandoahHeapRegion* r;
 982     while ((r =_cs-&gt;claim_next()) != NULL) {
 983       assert(r-&gt;has_live(), "Region " SIZE_FORMAT " should have been reclaimed early", r-&gt;index());
 984       _sh-&gt;marked_object_iterate(r, &amp;cl);
 985 
 986       if (ShenandoahPacing) {
 987         _sh-&gt;pacer()-&gt;report_evac(r-&gt;used() &gt;&gt; LogHeapWordSize);
 988       }
 989 
 990       if (_sh-&gt;check_cancelled_gc_and_yield(_concurrent)) {
 991         break;
 992       }
 993     }
 994   }
 995 };
 996 
 997 void ShenandoahHeap::trash_cset_regions() {
 998   ShenandoahHeapLocker locker(lock());
 999 
1000   ShenandoahCollectionSet* set = collection_set();
1001   ShenandoahHeapRegion* r;
1002   set-&gt;clear_current_index();
1003   while ((r = set-&gt;next()) != NULL) {
1004     r-&gt;make_trash();
1005   }
1006   collection_set()-&gt;clear();
1007 }
1008 
1009 void ShenandoahHeap::print_heap_regions_on(outputStream* st) const {
1010   st-&gt;print_cr("Heap Regions:");
1011   st-&gt;print_cr("EU=empty-uncommitted, EC=empty-committed, R=regular, H=humongous start, HC=humongous continuation, CS=collection set, T=trash, P=pinned");
1012   st-&gt;print_cr("BTE=bottom/top/end, U=used, T=TLAB allocs, G=GCLAB allocs, S=shared allocs, L=live data");
1013   st-&gt;print_cr("R=root, CP=critical pins, TAMS=top-at-mark-start, UWM=update watermark");
1014   st-&gt;print_cr("SN=alloc sequence number");
1015 
1016   for (size_t i = 0; i &lt; num_regions(); i++) {
1017     get_region(i)-&gt;print_on(st);
1018   }
1019 }
1020 
1021 void ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {
1022   assert(start-&gt;is_humongous_start(), "reclaim regions starting with the first one");
1023 
1024   oop humongous_obj = oop(start-&gt;bottom());
1025   size_t size = humongous_obj-&gt;size();
1026   size_t required_regions = ShenandoahHeapRegion::required_regions(size * HeapWordSize);
1027   size_t index = start-&gt;index() + required_regions - 1;
1028 
1029   assert(!start-&gt;has_live(), "liveness must be zero");
1030 
1031   for(size_t i = 0; i &lt; required_regions; i++) {
1032     // Reclaim from tail. Otherwise, assertion fails when printing region to trace log,
1033     // as it expects that every region belongs to a humongous region starting with a humongous start region.
1034     ShenandoahHeapRegion* region = get_region(index --);
1035 
1036     assert(region-&gt;is_humongous(), "expect correct humongous start or continuation");
1037     assert(!region-&gt;is_cset(), "Humongous region should not be in collection set");
1038 
1039     region-&gt;make_trash_immediate();
1040   }
1041 }
1042 
1043 class ShenandoahCheckCleanGCLABClosure : public ThreadClosure {
1044 public:
1045   ShenandoahCheckCleanGCLABClosure() {}
1046   void do_thread(Thread* thread) {
1047     PLAB* gclab = ShenandoahThreadLocalData::gclab(thread);
1048     assert(gclab != NULL, "GCLAB should be initialized for %s", thread-&gt;name());
1049     assert(gclab-&gt;words_remaining() == 0, "GCLAB should not need retirement");
1050   }
1051 };
1052 
1053 class ShenandoahRetireGCLABClosure : public ThreadClosure {
1054 private:
1055   bool const _resize;
1056 public:
1057   ShenandoahRetireGCLABClosure(bool resize) : _resize(resize) {}
1058   void do_thread(Thread* thread) {
1059     PLAB* gclab = ShenandoahThreadLocalData::gclab(thread);
1060     assert(gclab != NULL, "GCLAB should be initialized for %s", thread-&gt;name());
1061     gclab-&gt;retire();
1062     if (_resize &amp;&amp; ShenandoahThreadLocalData::gclab_size(thread) &gt; 0) {
1063       ShenandoahThreadLocalData::set_gclab_size(thread, 0);
1064     }
1065   }
1066 };
1067 
1068 void ShenandoahHeap::labs_make_parsable() {
1069   assert(UseTLAB, "Only call with UseTLAB");
1070 
1071   ShenandoahRetireGCLABClosure cl(false);
1072 
1073   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {
1074     ThreadLocalAllocBuffer&amp; tlab = t-&gt;tlab();
1075     tlab.make_parsable();
1076     cl.do_thread(t);
1077   }
1078 
1079   workers()-&gt;threads_do(&amp;cl);
1080 }
1081 
1082 void ShenandoahHeap::tlabs_retire(bool resize) {
1083   assert(UseTLAB, "Only call with UseTLAB");
1084   assert(!resize || ResizeTLAB, "Only call for resize when ResizeTLAB is enabled");
1085 
1086   ThreadLocalAllocStats stats;
1087 
1088   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {
1089     ThreadLocalAllocBuffer&amp; tlab = t-&gt;tlab();
1090     tlab.retire(&amp;stats);
1091     if (resize) {
1092       tlab.resize();
1093     }
1094   }
1095 
1096   stats.publish();
1097 
1098 #ifdef ASSERT
1099   ShenandoahCheckCleanGCLABClosure cl;
1100   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {
1101     cl.do_thread(t);
1102   }
1103   workers()-&gt;threads_do(&amp;cl);
1104 #endif
1105 }
1106 
1107 void ShenandoahHeap::gclabs_retire(bool resize) {
1108   assert(UseTLAB, "Only call with UseTLAB");
1109   assert(!resize || ResizeTLAB, "Only call for resize when ResizeTLAB is enabled");
1110 
1111   ShenandoahRetireGCLABClosure cl(resize);
1112   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {
1113     cl.do_thread(t);
1114   }
1115   workers()-&gt;threads_do(&amp;cl);
1116 }
1117 
1118 class ShenandoahEvacuateUpdateRootsTask : public AbstractGangTask {
1119 private:
1120   ShenandoahRootEvacuator* _rp;
1121 
1122 public:
1123   ShenandoahEvacuateUpdateRootsTask(ShenandoahRootEvacuator* rp) :
1124     AbstractGangTask("Shenandoah evacuate and update roots"),
1125     _rp(rp) {}
1126 
1127   void work(uint worker_id) {
1128     ShenandoahParallelWorkerSession worker_session(worker_id);
1129     ShenandoahEvacOOMScope oom_evac_scope;
1130     ShenandoahEvacuateUpdateRootsClosure&lt;&gt; cl;
1131     MarkingCodeBlobClosure blobsCl(&amp;cl, CodeBlobToOopClosure::FixRelocations);
1132     _rp-&gt;roots_do(worker_id, &amp;cl);
1133   }
1134 };
1135 
1136 void ShenandoahHeap::evacuate_and_update_roots() {
1137 #if COMPILER2_OR_JVMCI
1138   DerivedPointerTable::clear();
1139 #endif
1140   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), "Only iterate roots while world is stopped");
1141   {
1142     // Include concurrent roots if current cycle can not process those roots concurrently
1143     ShenandoahRootEvacuator rp(workers()-&gt;active_workers(),
1144                                ShenandoahPhaseTimings::init_evac,
1145                                !ShenandoahConcurrentRoots::should_do_concurrent_roots(),
1146                                !ShenandoahConcurrentRoots::should_do_concurrent_class_unloading());
1147     ShenandoahEvacuateUpdateRootsTask roots_task(&amp;rp);
1148     workers()-&gt;run_task(&amp;roots_task);
1149   }
1150 
1151 #if COMPILER2_OR_JVMCI
1152   DerivedPointerTable::update_pointers();
1153 #endif
1154 }
1155 
1156 // Returns size in bytes
1157 size_t ShenandoahHeap::unsafe_max_tlab_alloc(Thread *thread) const {
1158   if (ShenandoahElasticTLAB) {
1159     // With Elastic TLABs, return the max allowed size, and let the allocation path
1160     // figure out the safe size for current allocation.
1161     return ShenandoahHeapRegion::max_tlab_size_bytes();
1162   } else {
1163     return MIN2(_free_set-&gt;unsafe_peek_free(), ShenandoahHeapRegion::max_tlab_size_bytes());
1164   }
1165 }
1166 
1167 size_t ShenandoahHeap::max_tlab_size() const {
1168   // Returns size in words
1169   return ShenandoahHeapRegion::max_tlab_size_words();
1170 }
1171 
1172 void ShenandoahHeap::collect(GCCause::Cause cause) {
1173   control_thread()-&gt;request_gc(cause);
1174 }
1175 
1176 void ShenandoahHeap::do_full_collection(bool clear_all_soft_refs) {
1177   //assert(false, "Shouldn't need to do full collections");
1178 }
1179 
1180 HeapWord* ShenandoahHeap::block_start(const void* addr) const {
1181   ShenandoahHeapRegion* r = heap_region_containing(addr);
1182   if (r != NULL) {
1183     return r-&gt;block_start(addr);
1184   }
1185   return NULL;
1186 }
1187 
1188 bool ShenandoahHeap::block_is_obj(const HeapWord* addr) const {
1189   ShenandoahHeapRegion* r = heap_region_containing(addr);
1190   return r-&gt;block_is_obj(addr);
1191 }
1192 
1193 bool ShenandoahHeap::print_location(outputStream* st, void* addr) const {
1194   return BlockLocationPrinter&lt;ShenandoahHeap&gt;::print_location(st, addr);
1195 }
1196 
1197 jlong ShenandoahHeap::millis_since_last_gc() {
1198   double v = heuristics()-&gt;time_since_last_gc() * 1000;
1199   assert(0 &lt;= v &amp;&amp; v &lt;= max_jlong, "value should fit: %f", v);
1200   return (jlong)v;
1201 }
1202 
1203 void ShenandoahHeap::prepare_for_verify() {
1204   if (SafepointSynchronize::is_at_safepoint() &amp;&amp; UseTLAB) {
1205     labs_make_parsable();
1206   }
1207 }
1208 
1209 void ShenandoahHeap::gc_threads_do(ThreadClosure* tcl) const {
1210   workers()-&gt;threads_do(tcl);
1211   if (_safepoint_workers != NULL) {
1212     _safepoint_workers-&gt;threads_do(tcl);
1213   }
1214   if (ShenandoahStringDedup::is_enabled()) {
1215     ShenandoahStringDedup::threads_do(tcl);
1216   }
1217 }
1218 
1219 void ShenandoahHeap::print_tracing_info() const {
1220   LogTarget(Info, gc, stats) lt;
1221   if (lt.is_enabled()) {
1222     ResourceMark rm;
1223     LogStream ls(lt);
1224 
1225     phase_timings()-&gt;print_global_on(&amp;ls);
1226 
1227     ls.cr();
1228     ls.cr();
1229 
1230     shenandoah_policy()-&gt;print_gc_stats(&amp;ls);
1231 
1232     ls.cr();
1233     ls.cr();
1234 
1235     if (ShenandoahPacing) {
1236       pacer()-&gt;print_on(&amp;ls);
1237     }
1238 
1239     ls.cr();
1240     ls.cr();
1241   }
1242 }
1243 
1244 void ShenandoahHeap::verify(VerifyOption vo) {
1245   if (ShenandoahSafepoint::is_at_shenandoah_safepoint()) {
1246     if (ShenandoahVerify) {
1247       verifier()-&gt;verify_generic(vo);
1248     } else {
1249       // TODO: Consider allocating verification bitmaps on demand,
1250       // and turn this on unconditionally.
1251     }
1252   }
1253 }
1254 size_t ShenandoahHeap::tlab_capacity(Thread *thr) const {
1255   return _free_set-&gt;capacity();
1256 }
1257 
1258 class ObjectIterateScanRootClosure : public BasicOopIterateClosure {
1259 private:
1260   MarkBitMap* _bitmap;
1261   Stack&lt;oop,mtGC&gt;* _oop_stack;
1262   ShenandoahHeap* const _heap;
1263   ShenandoahMarkingContext* const _marking_context;
1264 
1265   template &lt;class T&gt;
1266   void do_oop_work(T* p) {
1267     T o = RawAccess&lt;&gt;::oop_load(p);
1268     if (!CompressedOops::is_null(o)) {
1269       oop obj = CompressedOops::decode_not_null(o);
1270       if (_heap-&gt;is_concurrent_weak_root_in_progress() &amp;&amp; !_marking_context-&gt;is_marked(obj)) {
1271         // There may be dead oops in weak roots in concurrent root phase, do not touch them.
1272         return;
1273       }
1274       obj = ShenandoahBarrierSet::resolve_forwarded_not_null(obj);
1275 
1276       assert(oopDesc::is_oop(obj), "must be a valid oop");
1277       if (!_bitmap-&gt;is_marked(obj)) {
1278         _bitmap-&gt;mark(obj);
1279         _oop_stack-&gt;push(obj);
1280       }
1281     }
1282   }
1283 public:
1284   ObjectIterateScanRootClosure(MarkBitMap* bitmap, Stack&lt;oop,mtGC&gt;* oop_stack) :
1285     _bitmap(bitmap), _oop_stack(oop_stack), _heap(ShenandoahHeap::heap()),
1286     _marking_context(_heap-&gt;marking_context()) {}
1287   void do_oop(oop* p)       { do_oop_work(p); }
1288   void do_oop(narrowOop* p) { do_oop_work(p); }
1289 };
1290 
1291 /*
1292  * This is public API, used in preparation of object_iterate().
1293  * Since we don't do linear scan of heap in object_iterate() (see comment below), we don't
1294  * need to make the heap parsable. For Shenandoah-internal linear heap scans that we can
1295  * control, we call SH::tlabs_retire, SH::gclabs_retire.
1296  */
1297 void ShenandoahHeap::ensure_parsability(bool retire_tlabs) {
1298   // No-op.
1299 }
1300 
1301 /*
1302  * Iterates objects in the heap. This is public API, used for, e.g., heap dumping.
1303  *
1304  * We cannot safely iterate objects by doing a linear scan at random points in time. Linear
1305  * scanning needs to deal with dead objects, which may have dead Klass* pointers (e.g.
1306  * calling oopDesc::size() would crash) or dangling reference fields (crashes) etc. Linear
1307  * scanning therefore depends on having a valid marking bitmap to support it. However, we only
1308  * have a valid marking bitmap after successful marking. In particular, we *don't* have a valid
1309  * marking bitmap during marking, after aborted marking or during/after cleanup (when we just
1310  * wiped the bitmap in preparation for next marking).
1311  *
1312  * For all those reasons, we implement object iteration as a single marking traversal, reporting
1313  * objects as we mark+traverse through the heap, starting from GC roots. JVMTI IterateThroughHeap
1314  * is allowed to report dead objects, but is not required to do so.
1315  */
1316 void ShenandoahHeap::object_iterate(ObjectClosure* cl) {
1317   assert(SafepointSynchronize::is_at_safepoint(), "safe iteration is only available during safepoints");
1318   if (!_aux_bitmap_region_special &amp;&amp; !os::commit_memory((char*)_aux_bitmap_region.start(), _aux_bitmap_region.byte_size(), false)) {
1319     log_warning(gc)("Could not commit native memory for auxiliary marking bitmap for heap iteration");
1320     return;
1321   }
1322 
1323   // Reset bitmap
1324   _aux_bit_map.clear();
1325 
1326   Stack&lt;oop,mtGC&gt; oop_stack;
1327 
1328   ObjectIterateScanRootClosure oops(&amp;_aux_bit_map, &amp;oop_stack);
1329 
1330   {
1331     // First, we process GC roots according to current GC cycle.
1332     // This populates the work stack with initial objects.
1333     // It is important to relinquish the associated locks before diving
1334     // into heap dumper.
1335     ShenandoahHeapIterationRootScanner rp;
1336     rp.roots_do(&amp;oops);
1337   }
1338 
1339   // Work through the oop stack to traverse heap.
1340   while (! oop_stack.is_empty()) {
1341     oop obj = oop_stack.pop();
1342     assert(oopDesc::is_oop(obj), "must be a valid oop");
1343     cl-&gt;do_object(obj);
1344     obj-&gt;oop_iterate(&amp;oops);
1345   }
1346 
1347   assert(oop_stack.is_empty(), "should be empty");
1348 
1349   if (!_aux_bitmap_region_special &amp;&amp; !os::uncommit_memory((char*)_aux_bitmap_region.start(), _aux_bitmap_region.byte_size())) {
1350     log_warning(gc)("Could not uncommit native memory for auxiliary marking bitmap for heap iteration");
1351   }
1352 }
1353 
1354 // Keep alive an object that was loaded with AS_NO_KEEPALIVE.
1355 void ShenandoahHeap::keep_alive(oop obj) {
1356   if (is_concurrent_mark_in_progress()) {
1357     ShenandoahBarrierSet::barrier_set()-&gt;enqueue(obj);
1358   }
1359 }
1360 
1361 void ShenandoahHeap::heap_region_iterate(ShenandoahHeapRegionClosure* blk) const {
1362   for (size_t i = 0; i &lt; num_regions(); i++) {
1363     ShenandoahHeapRegion* current = get_region(i);
1364     blk-&gt;heap_region_do(current);
1365   }
1366 }
1367 
1368 class ShenandoahParallelHeapRegionTask : public AbstractGangTask {
1369 private:
1370   ShenandoahHeap* const _heap;
1371   ShenandoahHeapRegionClosure* const _blk;
1372 
1373   shenandoah_padding(0);
1374   volatile size_t _index;
1375   shenandoah_padding(1);
1376 
1377 public:
1378   ShenandoahParallelHeapRegionTask(ShenandoahHeapRegionClosure* blk) :
1379           AbstractGangTask("Parallel Region Task"),
1380           _heap(ShenandoahHeap::heap()), _blk(blk), _index(0) {}
1381 
1382   void work(uint worker_id) {
1383     ShenandoahParallelWorkerSession worker_session(worker_id);
1384     size_t stride = ShenandoahParallelRegionStride;
1385 
1386     size_t max = _heap-&gt;num_regions();
1387     while (_index &lt; max) {
1388       size_t cur = Atomic::fetch_and_add(&amp;_index, stride);
1389       size_t start = cur;
1390       size_t end = MIN2(cur + stride, max);
1391       if (start &gt;= max) break;
1392 
1393       for (size_t i = cur; i &lt; end; i++) {
1394         ShenandoahHeapRegion* current = _heap-&gt;get_region(i);
1395         _blk-&gt;heap_region_do(current);
1396       }
1397     }
1398   }
1399 };
1400 
1401 void ShenandoahHeap::parallel_heap_region_iterate(ShenandoahHeapRegionClosure* blk) const {
1402   assert(blk-&gt;is_thread_safe(), "Only thread-safe closures here");
1403   if (num_regions() &gt; ShenandoahParallelRegionStride) {
1404     ShenandoahParallelHeapRegionTask task(blk);
1405     workers()-&gt;run_task(&amp;task);
1406   } else {
1407     heap_region_iterate(blk);
1408   }
1409 }
1410 
1411 class ShenandoahInitMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {
1412 private:
1413   ShenandoahMarkingContext* const _ctx;
1414 public:
1415   ShenandoahInitMarkUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()-&gt;marking_context()) {}
1416 
1417   void heap_region_do(ShenandoahHeapRegion* r) {
1418     assert(!r-&gt;has_live(), "Region " SIZE_FORMAT " should have no live data", r-&gt;index());
1419     if (r-&gt;is_active()) {
1420       // Check if region needs updating its TAMS. We have updated it already during concurrent
1421       // reset, so it is very likely we don't need to do another write here.
1422       if (_ctx-&gt;top_at_mark_start(r) != r-&gt;top()) {
1423         _ctx-&gt;capture_top_at_mark_start(r);
1424       }
1425     } else {
1426       assert(_ctx-&gt;top_at_mark_start(r) == r-&gt;top(),
1427              "Region " SIZE_FORMAT " should already have correct TAMS", r-&gt;index());
1428     }
1429   }
1430 
1431   bool is_thread_safe() { return true; }
1432 };
1433 
1434 void ShenandoahHeap::op_init_mark() {
1435   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), "Should be at safepoint");
1436   assert(Thread::current()-&gt;is_VM_thread(), "can only do this in VMThread");
1437 
1438   assert(marking_context()-&gt;is_bitmap_clear(), "need clear marking bitmap");
1439   assert(!marking_context()-&gt;is_complete(), "should not be complete");
1440   assert(!has_forwarded_objects(), "No forwarded objects on this path");
1441 
1442   if (ShenandoahVerify) {
1443     verifier()-&gt;verify_before_concmark();
1444   }
1445 
1446   if (VerifyBeforeGC) {
1447     Universe::verify();
1448   }
1449 
1450   set_concurrent_mark_in_progress(true);
1451 
1452   // We need to reset all TLABs because they might be below the TAMS, and we need to mark
1453   // the objects in them. Do not let mutators allocate any new objects in their current TLABs.
1454   // It is also a good place to resize the TLAB sizes for future allocations.
1455   if (UseTLAB) {
1456     ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_manage_tlabs);
1457     tlabs_retire(ResizeTLAB);
1458   }
1459 
1460   {
1461     ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_region_states);
1462     ShenandoahInitMarkUpdateRegionStateClosure cl;
1463     parallel_heap_region_iterate(&amp;cl);
1464   }
1465 
1466   // Make above changes visible to worker threads
1467   OrderAccess::fence();
1468 
1469   concurrent_mark()-&gt;mark_roots(ShenandoahPhaseTimings::scan_roots);
1470 
1471   if (ShenandoahPacing) {
1472     pacer()-&gt;setup_for_mark();
1473   }
1474 
1475   // Arm nmethods for concurrent marking. When a nmethod is about to be executed,
1476   // we need to make sure that all its metadata are marked. alternative is to remark
1477   // thread roots at final mark pause, but it can be potential latency killer.
1478   if (ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
1479     ShenandoahCodeRoots::arm_nmethods();
1480   }
1481 }
1482 
1483 void ShenandoahHeap::op_mark() {
1484   concurrent_mark()-&gt;mark_from_roots();
1485 }
1486 
1487 class ShenandoahFinalMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {
1488 private:
1489   ShenandoahMarkingContext* const _ctx;
1490   ShenandoahHeapLock* const _lock;
1491 
1492 public:
1493   ShenandoahFinalMarkUpdateRegionStateClosure() :
1494     _ctx(ShenandoahHeap::heap()-&gt;complete_marking_context()), _lock(ShenandoahHeap::heap()-&gt;lock()) {}
1495 
1496   void heap_region_do(ShenandoahHeapRegion* r) {
1497     if (r-&gt;is_active()) {
1498       // All allocations past TAMS are implicitly live, adjust the region data.
1499       // Bitmaps/TAMS are swapped at this point, so we need to poll complete bitmap.
1500       HeapWord *tams = _ctx-&gt;top_at_mark_start(r);
1501       HeapWord *top = r-&gt;top();
1502       if (top &gt; tams) {
1503         r-&gt;increase_live_data_alloc_words(pointer_delta(top, tams));
1504       }
1505 
1506       // We are about to select the collection set, make sure it knows about
1507       // current pinning status. Also, this allows trashing more regions that
1508       // now have their pinning status dropped.
1509       if (r-&gt;is_pinned()) {
1510         if (r-&gt;pin_count() == 0) {
1511           ShenandoahHeapLocker locker(_lock);
1512           r-&gt;make_unpinned();
1513         }
1514       } else {
1515         if (r-&gt;pin_count() &gt; 0) {
1516           ShenandoahHeapLocker locker(_lock);
1517           r-&gt;make_pinned();
1518         }
1519       }
1520 
1521       // Remember limit for updating refs. It's guaranteed that we get no
1522       // from-space-refs written from here on.
1523       r-&gt;set_update_watermark_at_safepoint(r-&gt;top());
1524     } else {
1525       assert(!r-&gt;has_live(), "Region " SIZE_FORMAT " should have no live data", r-&gt;index());
1526       assert(_ctx-&gt;top_at_mark_start(r) == r-&gt;top(),
1527              "Region " SIZE_FORMAT " should have correct TAMS", r-&gt;index());
1528     }
1529   }
1530 
1531   bool is_thread_safe() { return true; }
1532 };
1533 
1534 void ShenandoahHeap::op_final_mark() {
1535   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), "Should be at safepoint");
1536   assert(!has_forwarded_objects(), "No forwarded objects on this path");
1537 
1538   // It is critical that we
1539   // evacuate roots right after finishing marking, so that we don't
1540   // get unmarked objects in the roots.
1541 
1542   if (!cancelled_gc()) {
1543     concurrent_mark()-&gt;finish_mark_from_roots(/* full_gc = */ false);
1544 
1545     // Marking is completed, deactivate SATB barrier
1546     set_concurrent_mark_in_progress(false);
1547     mark_complete_marking_context();
1548 
1549     parallel_cleaning(false /* full gc*/);
1550 
1551     if (ShenandoahVerify) {
1552       verifier()-&gt;verify_roots_no_forwarded();
1553     }
1554 
1555     {
1556       ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_region_states);
1557       ShenandoahFinalMarkUpdateRegionStateClosure cl;
1558       parallel_heap_region_iterate(&amp;cl);
1559 
1560       assert_pinned_region_status();
1561     }
1562 
1563     // Retire the TLABs, which will force threads to reacquire their TLABs after the pause.
1564     // This is needed for two reasons. Strong one: new allocations would be with new freeset,
1565     // which would be outside the collection set, so no cset writes would happen there.
1566     // Weaker one: new allocations would happen past update watermark, and so less work would
1567     // be needed for reference updates (would update the large filler instead).
1568     if (UseTLAB) {
1569       ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_manage_labs);
1570       tlabs_retire(false);
1571     }
1572 
1573     {
1574       ShenandoahGCPhase phase(ShenandoahPhaseTimings::choose_cset);
1575       ShenandoahHeapLocker locker(lock());
1576       _collection_set-&gt;clear();
1577       heuristics()-&gt;choose_collection_set(_collection_set);
1578     }
1579 
1580     {
1581       ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_rebuild_freeset);
1582       ShenandoahHeapLocker locker(lock());
1583       _free_set-&gt;rebuild();
1584     }
1585 
1586     if (!is_degenerated_gc_in_progress()) {
1587       prepare_concurrent_roots();
1588       prepare_concurrent_unloading();
1589     }
1590 
1591     // If collection set has candidates, start evacuation.
1592     // Otherwise, bypass the rest of the cycle.
1593     if (!collection_set()-&gt;is_empty()) {
1594       ShenandoahGCPhase init_evac(ShenandoahPhaseTimings::init_evac);
1595 
1596       if (ShenandoahVerify) {
1597         verifier()-&gt;verify_before_evacuation();
1598       }
1599 
1600       set_evacuation_in_progress(true);
1601       // From here on, we need to update references.
1602       set_has_forwarded_objects(true);
1603 
1604       if (!is_degenerated_gc_in_progress()) {
1605         if (ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
1606           ShenandoahCodeRoots::arm_nmethods();
1607         }
1608         evacuate_and_update_roots();
1609       }
1610 
1611       if (ShenandoahPacing) {
1612         pacer()-&gt;setup_for_evac();
1613       }
1614 
1615       if (ShenandoahVerify) {
1616         // If OOM while evacuating/updating of roots, there is no guarantee of their consistencies
1617         if (!cancelled_gc()) {
1618           ShenandoahRootVerifier::RootTypes types = ShenandoahRootVerifier::None;
1619           if (ShenandoahConcurrentRoots::should_do_concurrent_roots()) {
1620             types = ShenandoahRootVerifier::combine(ShenandoahRootVerifier::JNIHandleRoots, ShenandoahRootVerifier::WeakRoots);
1621             types = ShenandoahRootVerifier::combine(types, ShenandoahRootVerifier::CLDGRoots);
1622             types = ShenandoahRootVerifier::combine(types, ShenandoahRootVerifier::StringDedupRoots);
1623           }
1624 
1625           if (ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
1626             types = ShenandoahRootVerifier::combine(types, ShenandoahRootVerifier::CodeRoots);
1627           }
1628           verifier()-&gt;verify_roots_no_forwarded_except(types);
1629         }
1630         verifier()-&gt;verify_during_evacuation();
1631       }
1632     } else {
1633       if (ShenandoahVerify) {
1634         verifier()-&gt;verify_after_concmark();
1635       }
1636 
1637       if (VerifyAfterGC) {
1638         Universe::verify();
1639       }
1640     }
1641 
1642   } else {
1643     // If this cycle was updating references, we need to keep the has_forwarded_objects
1644     // flag on, for subsequent phases to deal with it.
1645     concurrent_mark()-&gt;cancel();
1646     set_concurrent_mark_in_progress(false);
1647 
1648     if (process_references()) {
1649       // Abandon reference processing right away: pre-cleaning must have failed.
1650       ReferenceProcessor *rp = ref_processor();
1651       rp-&gt;disable_discovery();
1652       rp-&gt;abandon_partial_discovery();
1653       rp-&gt;verify_no_references_recorded();
1654     }
1655   }
1656 }
1657 
1658 void ShenandoahHeap::op_conc_evac() {
1659   ShenandoahEvacuationTask task(this, _collection_set, true);
1660   workers()-&gt;run_task(&amp;task);
1661 }
1662 
1663 void ShenandoahHeap::op_stw_evac() {
1664   ShenandoahEvacuationTask task(this, _collection_set, false);
1665   workers()-&gt;run_task(&amp;task);
1666 }
1667 
1668 void ShenandoahHeap::op_updaterefs() {
1669   update_heap_references(true);
1670 }
1671 
1672 void ShenandoahHeap::op_cleanup_early() {
1673   free_set()-&gt;recycle_trash();
1674 }
1675 
1676 void ShenandoahHeap::op_cleanup_complete() {
1677   free_set()-&gt;recycle_trash();
1678 }
1679 
1680 class ShenandoahConcurrentRootsEvacUpdateTask : public AbstractGangTask {
1681 private:
1682   ShenandoahVMRoots&lt;true /*concurrent*/&gt;        _vm_roots;
1683   ShenandoahClassLoaderDataRoots&lt;true /*concurrent*/, false /*single threaded*/&gt; _cld_roots;
1684 
1685 public:
1686   ShenandoahConcurrentRootsEvacUpdateTask(ShenandoahPhaseTimings::Phase phase) :
1687     AbstractGangTask("Shenandoah Evacuate/Update Concurrent Strong Roots Task"),
1688     _vm_roots(phase),
1689     _cld_roots(phase, ShenandoahHeap::heap()-&gt;workers()-&gt;active_workers()) {}
1690 
1691   void work(uint worker_id) {
1692     ShenandoahConcurrentWorkerSession worker_session(worker_id);
1693     ShenandoahEvacOOMScope oom;
1694     {
1695       // vm_roots and weak_roots are OopStorage backed roots, concurrent iteration
1696       // may race against OopStorage::release() calls.
1697       ShenandoahEvacUpdateOopStorageRootsClosure cl;
1698       _vm_roots.oops_do&lt;ShenandoahEvacUpdateOopStorageRootsClosure&gt;(&amp;cl, worker_id);
1699     }
1700 
1701     {
1702       ShenandoahEvacuateUpdateRootsClosure&lt;&gt; cl;
1703       CLDToOopClosure clds(&amp;cl, ClassLoaderData::_claim_strong);
1704       _cld_roots.cld_do(&amp;clds, worker_id);
1705     }
1706   }
1707 };
1708 
1709 class ShenandoahEvacUpdateCleanupOopStorageRootsClosure : public BasicOopIterateClosure {
1710 private:
1711   ShenandoahHeap* const _heap;
1712   ShenandoahMarkingContext* const _mark_context;
1713   bool  _evac_in_progress;
1714   Thread* const _thread;
1715   size_t  _dead_counter;
1716 
1717 public:
1718   ShenandoahEvacUpdateCleanupOopStorageRootsClosure();
1719   void do_oop(oop* p);
1720   void do_oop(narrowOop* p);
1721 
1722   size_t dead_counter() const;
1723   void reset_dead_counter();
1724 };
1725 
1726 ShenandoahEvacUpdateCleanupOopStorageRootsClosure::ShenandoahEvacUpdateCleanupOopStorageRootsClosure() :
1727   _heap(ShenandoahHeap::heap()),
1728   _mark_context(ShenandoahHeap::heap()-&gt;marking_context()),
1729   _evac_in_progress(ShenandoahHeap::heap()-&gt;is_evacuation_in_progress()),
1730   _thread(Thread::current()),
1731   _dead_counter(0) {
1732 }
1733 
1734 void ShenandoahEvacUpdateCleanupOopStorageRootsClosure::do_oop(oop* p) {
1735   const oop obj = RawAccess&lt;&gt;::oop_load(p);
1736   if (!CompressedOops::is_null(obj)) {
1737     if (!_mark_context-&gt;is_marked(obj)) {
1738       shenandoah_assert_correct(p, obj);
1739       oop old = Atomic::cmpxchg(p, obj, oop(NULL));
1740       if (obj == old) {
1741         _dead_counter ++;
1742       }
1743     } else if (_evac_in_progress &amp;&amp; _heap-&gt;in_collection_set(obj)) {
1744       oop resolved = ShenandoahBarrierSet::resolve_forwarded_not_null(obj);
1745       if (resolved == obj) {
1746         resolved = _heap-&gt;evacuate_object(obj, _thread);
1747       }
1748       Atomic::cmpxchg(p, obj, resolved);
1749       assert(_heap-&gt;cancelled_gc() ||
1750              _mark_context-&gt;is_marked(resolved) &amp;&amp; !_heap-&gt;in_collection_set(resolved),
1751              "Sanity");
1752     }
1753   }
1754 }
1755 
1756 void ShenandoahEvacUpdateCleanupOopStorageRootsClosure::do_oop(narrowOop* p) {
1757   ShouldNotReachHere();
1758 }
1759 
1760 size_t ShenandoahEvacUpdateCleanupOopStorageRootsClosure::dead_counter() const {
1761   return _dead_counter;
1762 }
1763 
1764 void ShenandoahEvacUpdateCleanupOopStorageRootsClosure::reset_dead_counter() {
1765   _dead_counter = 0;
1766 }
1767 
1768 class ShenandoahIsCLDAliveClosure : public CLDClosure {
1769 public:
1770   void do_cld(ClassLoaderData* cld) {
1771     cld-&gt;is_alive();
1772   }
1773 };
1774 
1775 class ShenandoahIsNMethodAliveClosure: public NMethodClosure {
1776 public:
1777   void do_nmethod(nmethod* n) {
1778     n-&gt;is_unloading();
1779   }
1780 };
1781 
1782 // This task not only evacuates/updates marked weak roots, but also "NULL"
1783 // dead weak roots.
1784 class ShenandoahConcurrentWeakRootsEvacUpdateTask : public AbstractGangTask {
1785 private:
1786   ShenandoahWeakRoot&lt;true /*concurrent*/&gt;  _jni_roots;
1787   ShenandoahWeakRoot&lt;true /*concurrent*/&gt;  _string_table_roots;
1788   ShenandoahWeakRoot&lt;true /*concurrent*/&gt;  _resolved_method_table_roots;
1789   ShenandoahWeakRoot&lt;true /*concurrent*/&gt;  _vm_roots;
1790 
1791   // Roots related to concurrent class unloading
1792   ShenandoahClassLoaderDataRoots&lt;true /* concurrent */, false /* single thread*/&gt;
1793                                            _cld_roots;
1794   ShenandoahConcurrentNMethodIterator      _nmethod_itr;
1795   ShenandoahConcurrentStringDedupRoots     _dedup_roots;
1796   bool                                     _concurrent_class_unloading;
1797 
1798 public:
1799   ShenandoahConcurrentWeakRootsEvacUpdateTask(ShenandoahPhaseTimings::Phase phase) :
1800     AbstractGangTask("Shenandoah Concurrent Weak Root Task"),
1801     _jni_roots(OopStorageSet::jni_weak(), phase, ShenandoahPhaseTimings::JNIWeakRoots),
1802     _string_table_roots(OopStorageSet::string_table_weak(), phase, ShenandoahPhaseTimings::StringTableRoots),
1803     _resolved_method_table_roots(OopStorageSet::resolved_method_table_weak(), phase, ShenandoahPhaseTimings::ResolvedMethodTableRoots),
1804     _vm_roots(OopStorageSet::vm_weak(), phase, ShenandoahPhaseTimings::VMWeakRoots),
1805     _cld_roots(phase, ShenandoahHeap::heap()-&gt;workers()-&gt;active_workers()),
1806     _nmethod_itr(ShenandoahCodeRoots::table()),
1807     _dedup_roots(phase),
1808     _concurrent_class_unloading(ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
1809     StringTable::reset_dead_counter();
1810     ResolvedMethodTable::reset_dead_counter();
1811     if (_concurrent_class_unloading) {
1812       MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);
1813       _nmethod_itr.nmethods_do_begin();
1814     }
1815   }
1816 
1817   ~ShenandoahConcurrentWeakRootsEvacUpdateTask() {
1818     StringTable::finish_dead_counter();
1819     ResolvedMethodTable::finish_dead_counter();
1820     if (_concurrent_class_unloading) {
1821       MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);
1822       _nmethod_itr.nmethods_do_end();
1823     }
1824   }
1825 
1826   void work(uint worker_id) {
1827     ShenandoahConcurrentWorkerSession worker_session(worker_id);
1828     {
1829       ShenandoahEvacOOMScope oom;
1830       // jni_roots and weak_roots are OopStorage backed roots, concurrent iteration
1831       // may race against OopStorage::release() calls.
1832       ShenandoahEvacUpdateCleanupOopStorageRootsClosure cl;
1833       _jni_roots.oops_do(&amp;cl, worker_id);
1834       _vm_roots.oops_do(&amp;cl, worker_id);
1835 
1836       cl.reset_dead_counter();
1837       _string_table_roots.oops_do(&amp;cl, worker_id);
1838       StringTable::inc_dead_counter(cl.dead_counter());
1839 
1840       cl.reset_dead_counter();
1841       _resolved_method_table_roots.oops_do(&amp;cl, worker_id);
1842       ResolvedMethodTable::inc_dead_counter(cl.dead_counter());
1843 
1844       // String dedup weak roots
1845       ShenandoahForwardedIsAliveClosure is_alive;
1846       ShenandoahEvacuateUpdateRootsClosure&lt;MO_RELEASE&gt; keep_alive;
1847       _dedup_roots.oops_do(&amp;is_alive, &amp;keep_alive, worker_id);
1848     }
1849 
1850     // If we are going to perform concurrent class unloading later on, we need to
1851     // cleanup the weak oops in CLD and determinate nmethod's unloading state, so that we
1852     // can cleanup immediate garbage sooner.
1853     if (_concurrent_class_unloading) {
1854       // Applies ShenandoahIsCLDAlive closure to CLDs, native barrier will either NULL the
1855       // CLD's holder or evacuate it.
1856       ShenandoahIsCLDAliveClosure is_cld_alive;
1857       _cld_roots.cld_do(&amp;is_cld_alive, worker_id);
1858 
1859       // Applies ShenandoahIsNMethodAliveClosure to registered nmethods.
1860       // The closure calls nmethod-&gt;is_unloading(). The is_unloading
1861       // state is cached, therefore, during concurrent class unloading phase,
1862       // we will not touch the metadata of unloading nmethods
1863       ShenandoahIsNMethodAliveClosure is_nmethod_alive;
1864       _nmethod_itr.nmethods_do(&amp;is_nmethod_alive);
1865     }
1866   }
1867 };
1868 
1869 void ShenandoahHeap::op_weak_roots() {
1870   if (is_concurrent_weak_root_in_progress()) {
1871     // Concurrent weak root processing
1872     {
1873       ShenandoahTimingsTracker t(ShenandoahPhaseTimings::conc_weak_roots_work);
1874       ShenandoahGCWorkerPhase worker_phase(ShenandoahPhaseTimings::conc_weak_roots_work);
1875       ShenandoahConcurrentWeakRootsEvacUpdateTask task(ShenandoahPhaseTimings::conc_weak_roots_work);
1876       workers()-&gt;run_task(&amp;task);
1877       if (!ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
1878         set_concurrent_weak_root_in_progress(false);
1879       }
1880     }
1881 
1882     // Perform handshake to flush out dead oops
1883     {
1884       ShenandoahTimingsTracker t(ShenandoahPhaseTimings::conc_weak_roots_rendezvous);
1885       ShenandoahRendezvousClosure cl;
1886       Handshake::execute(&amp;cl);
1887     }
1888   }
1889 }
1890 
1891 void ShenandoahHeap::op_class_unloading() {
1892   assert (is_concurrent_weak_root_in_progress() &amp;&amp;
1893           ShenandoahConcurrentRoots::should_do_concurrent_class_unloading(),
1894           "Checked by caller");
1895   _unloader.unload();
1896   set_concurrent_weak_root_in_progress(false);
1897 }
1898 
1899 void ShenandoahHeap::op_strong_roots() {
1900   assert(is_concurrent_strong_root_in_progress(), "Checked by caller");
1901   ShenandoahConcurrentRootsEvacUpdateTask task(ShenandoahPhaseTimings::conc_strong_roots);
1902   workers()-&gt;run_task(&amp;task);
1903   set_concurrent_strong_root_in_progress(false);
1904 }
1905 
1906 class ShenandoahResetUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {
1907 private:
1908   ShenandoahMarkingContext* const _ctx;
1909 public:
1910   ShenandoahResetUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()-&gt;marking_context()) {}
1911 
1912   void heap_region_do(ShenandoahHeapRegion* r) {
1913     if (r-&gt;is_active()) {
1914       // Reset live data and set TAMS optimistically. We would recheck these under the pause
1915       // anyway to capture any updates that happened since now.
1916       r-&gt;clear_live_data();
1917       _ctx-&gt;capture_top_at_mark_start(r);
1918     }
1919   }
1920 
1921   bool is_thread_safe() { return true; }
1922 };
1923 
1924 void ShenandoahHeap::op_reset() {
1925   if (ShenandoahPacing) {
1926     pacer()-&gt;setup_for_reset();
1927   }
1928   reset_mark_bitmap();
1929 
1930   ShenandoahResetUpdateRegionStateClosure cl;
1931   parallel_heap_region_iterate(&amp;cl);
1932 }
1933 
1934 void ShenandoahHeap::op_preclean() {
1935   if (ShenandoahPacing) {
1936     pacer()-&gt;setup_for_preclean();
1937   }
1938   concurrent_mark()-&gt;preclean_weak_refs();
1939 }
1940 
1941 void ShenandoahHeap::op_full(GCCause::Cause cause) {
1942   ShenandoahMetricsSnapshot metrics;
1943   metrics.snap_before();
1944 
1945   full_gc()-&gt;do_it(cause);
1946 
1947   metrics.snap_after();
1948 
1949   if (metrics.is_good_progress()) {
1950     _progress_last_gc.set();
1951   } else {
1952     // Nothing to do. Tell the allocation path that we have failed to make
1953     // progress, and it can finally fail.
1954     _progress_last_gc.unset();
1955   }
1956 }
1957 
1958 void ShenandoahHeap::op_degenerated(ShenandoahDegenPoint point) {
1959   // Degenerated GC is STW, but it can also fail. Current mechanics communicates
1960   // GC failure via cancelled_concgc() flag. So, if we detect the failure after
1961   // some phase, we have to upgrade the Degenerate GC to Full GC.
1962 
1963   clear_cancelled_gc();
1964 
1965   ShenandoahMetricsSnapshot metrics;
1966   metrics.snap_before();
1967 
1968   switch (point) {
1969     // The cases below form the Duff's-like device: it describes the actual GC cycle,
1970     // but enters it at different points, depending on which concurrent phase had
1971     // degenerated.
1972 
1973     case _degenerated_outside_cycle:
1974       // We have degenerated from outside the cycle, which means something is bad with
1975       // the heap, most probably heavy humongous fragmentation, or we are very low on free
1976       // space. It makes little sense to wait for Full GC to reclaim as much as it can, when
1977       // we can do the most aggressive degen cycle, which includes processing references and
1978       // class unloading, unless those features are explicitly disabled.
1979       //
1980       // Note that we can only do this for "outside-cycle" degens, otherwise we would risk
1981       // changing the cycle parameters mid-cycle during concurrent -&gt; degenerated handover.
1982       set_process_references(heuristics()-&gt;can_process_references());
1983       set_unload_classes(heuristics()-&gt;can_unload_classes());
1984 
1985       op_reset();
1986 
1987       op_init_mark();
1988       if (cancelled_gc()) {
1989         op_degenerated_fail();
1990         return;
1991       }
1992 
1993     case _degenerated_mark:
1994       op_final_mark();
1995       if (cancelled_gc()) {
1996         op_degenerated_fail();
1997         return;
1998       }
1999 
2000       if (!has_forwarded_objects() &amp;&amp; ShenandoahConcurrentRoots::can_do_concurrent_class_unloading()) {
2001         // Disarm nmethods that armed for concurrent mark. On normal cycle, it would
2002         // be disarmed while conc-roots phase is running.
2003         // TODO: Call op_conc_roots() here instead
2004         ShenandoahCodeRoots::disarm_nmethods();
2005       }
2006 
2007       op_cleanup_early();
2008 
2009     case _degenerated_evac:
2010       // If heuristics thinks we should do the cycle, this flag would be set,
2011       // and we can do evacuation. Otherwise, it would be the shortcut cycle.
2012       if (is_evacuation_in_progress()) {
2013 
2014         // Degeneration under oom-evac protocol might have left some objects in
2015         // collection set un-evacuated. Restart evacuation from the beginning to
2016         // capture all objects. For all the objects that are already evacuated,
2017         // it would be a simple check, which is supposed to be fast. This is also
2018         // safe to do even without degeneration, as CSet iterator is at beginning
2019         // in preparation for evacuation anyway.
2020         //
2021         // Before doing that, we need to make sure we never had any cset-pinned
2022         // regions. This may happen if allocation failure happened when evacuating
2023         // the about-to-be-pinned object, oom-evac protocol left the object in
2024         // the collection set, and then the pin reached the cset region. If we continue
2025         // the cycle here, we would trash the cset and alive objects in it. To avoid
2026         // it, we fail degeneration right away and slide into Full GC to recover.
2027 
2028         {
2029           sync_pinned_region_status();
2030           collection_set()-&gt;clear_current_index();
2031 
2032           ShenandoahHeapRegion* r;
2033           while ((r = collection_set()-&gt;next()) != NULL) {
2034             if (r-&gt;is_pinned()) {
2035               cancel_gc(GCCause::_shenandoah_upgrade_to_full_gc);
2036               op_degenerated_fail();
2037               return;
2038             }
2039           }
2040 
2041           collection_set()-&gt;clear_current_index();
2042         }
2043 
2044         op_stw_evac();
2045         if (cancelled_gc()) {
2046           op_degenerated_fail();
2047           return;
2048         }
2049       }
2050 
2051       // If heuristics thinks we should do the cycle, this flag would be set,
2052       // and we need to do update-refs. Otherwise, it would be the shortcut cycle.
2053       if (has_forwarded_objects()) {
2054         op_init_updaterefs();
2055         if (cancelled_gc()) {
2056           op_degenerated_fail();
2057           return;
2058         }
2059       }
2060 
2061     case _degenerated_updaterefs:
2062       if (has_forwarded_objects()) {
2063         op_final_updaterefs();
2064         if (cancelled_gc()) {
2065           op_degenerated_fail();
2066           return;
2067         }
2068       }
2069 
2070       op_cleanup_complete();
2071       break;
2072 
2073     default:
2074       ShouldNotReachHere();
2075   }
2076 
2077   if (ShenandoahVerify) {
2078     verifier()-&gt;verify_after_degenerated();
2079   }
2080 
2081   if (VerifyAfterGC) {
2082     Universe::verify();
2083   }
2084 
2085   metrics.snap_after();
2086 
2087   // Check for futility and fail. There is no reason to do several back-to-back Degenerated cycles,
2088   // because that probably means the heap is overloaded and/or fragmented.
2089   if (!metrics.is_good_progress()) {
2090     _progress_last_gc.unset();
2091     cancel_gc(GCCause::_shenandoah_upgrade_to_full_gc);
2092     op_degenerated_futile();
2093   } else {
2094     _progress_last_gc.set();
2095   }
2096 }
2097 
2098 void ShenandoahHeap::op_degenerated_fail() {
2099   log_info(gc)("Cannot finish degeneration, upgrading to Full GC");
2100   shenandoah_policy()-&gt;record_degenerated_upgrade_to_full();
2101   op_full(GCCause::_shenandoah_upgrade_to_full_gc);
2102 }
2103 
2104 void ShenandoahHeap::op_degenerated_futile() {
2105   shenandoah_policy()-&gt;record_degenerated_upgrade_to_full();
2106   op_full(GCCause::_shenandoah_upgrade_to_full_gc);
2107 }
2108 
2109 void ShenandoahHeap::force_satb_flush_all_threads() {
2110   if (!is_concurrent_mark_in_progress()) {
2111     // No need to flush SATBs
2112     return;
2113   }
2114 
2115   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {
2116     ShenandoahThreadLocalData::set_force_satb_flush(t, true);
2117   }
2118   // The threads are not "acquiring" their thread-local data, but it does not
2119   // hurt to "release" the updates here anyway.
2120   OrderAccess::fence();
2121 }
2122 
2123 void ShenandoahHeap::set_gc_state_all_threads(char state) {
2124   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {
2125     ShenandoahThreadLocalData::set_gc_state(t, state);
2126   }
2127 }
2128 
2129 void ShenandoahHeap::set_gc_state_mask(uint mask, bool value) {
2130   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), "Should really be Shenandoah safepoint");
2131   _gc_state.set_cond(mask, value);
2132   set_gc_state_all_threads(_gc_state.raw_value());
2133 }
2134 
2135 void ShenandoahHeap::set_concurrent_mark_in_progress(bool in_progress) {
2136   if (has_forwarded_objects()) {
2137     set_gc_state_mask(MARKING | UPDATEREFS, in_progress);
2138   } else {
2139     set_gc_state_mask(MARKING, in_progress);
2140   }
2141   ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(in_progress, !in_progress);
2142 }
2143 
2144 void ShenandoahHeap::set_evacuation_in_progress(bool in_progress) {
2145   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), "Only call this at safepoint");
2146   set_gc_state_mask(EVACUATION, in_progress);
2147 }
2148 
2149 void ShenandoahHeap::set_concurrent_strong_root_in_progress(bool in_progress) {
2150   assert(ShenandoahConcurrentRoots::can_do_concurrent_roots(), "Why set the flag?");
2151   if (in_progress) {
2152     _concurrent_strong_root_in_progress.set();
2153   } else {
2154     _concurrent_strong_root_in_progress.unset();
2155   }
2156 }
2157 
2158 void ShenandoahHeap::set_concurrent_weak_root_in_progress(bool in_progress) {
2159   assert(ShenandoahConcurrentRoots::can_do_concurrent_roots(), "Why set the flag?");
2160   if (in_progress) {
2161     _concurrent_weak_root_in_progress.set();
2162   } else {
2163     _concurrent_weak_root_in_progress.unset();
2164   }
2165 }
2166 
2167 void ShenandoahHeap::ref_processing_init() {
2168   assert(_max_workers &gt; 0, "Sanity");
2169 
2170   _ref_processor =
2171     new ReferenceProcessor(&amp;_subject_to_discovery,  // is_subject_to_discovery
2172                            _ref_proc_mt_processing, // MT processing
2173                            _max_workers,            // Degree of MT processing
2174                            _ref_proc_mt_discovery,  // MT discovery
2175                            _max_workers,            // Degree of MT discovery
2176                            false,                   // Reference discovery is not atomic
2177                            NULL,                    // No closure, should be installed before use
2178                            true);                   // Scale worker threads
2179 
2180   shenandoah_assert_rp_isalive_not_installed();
2181 }
2182 
2183 GCTracer* ShenandoahHeap::tracer() {
2184   return shenandoah_policy()-&gt;tracer();
2185 }
2186 
2187 size_t ShenandoahHeap::tlab_used(Thread* thread) const {
2188   return _free_set-&gt;used();
2189 }
2190 
2191 bool ShenandoahHeap::try_cancel_gc() {
2192   while (true) {
2193     jbyte prev = _cancelled_gc.cmpxchg(CANCELLED, CANCELLABLE);
2194     if (prev == CANCELLABLE) return true;
2195     else if (prev == CANCELLED) return false;
2196     assert(ShenandoahSuspendibleWorkers, "should not get here when not using suspendible workers");
2197     assert(prev == NOT_CANCELLED, "must be NOT_CANCELLED");
2198     if (Thread::current()-&gt;is_Java_thread()) {
2199       // We need to provide a safepoint here, otherwise we might
2200       // spin forever if a SP is pending.
2201       ThreadBlockInVM sp(JavaThread::current());
2202       SpinPause();
2203     }
2204   }
2205 }
2206 
2207 void ShenandoahHeap::cancel_gc(GCCause::Cause cause) {
2208   if (try_cancel_gc()) {
2209     FormatBuffer&lt;&gt; msg("Cancelling GC: %s", GCCause::to_string(cause));
2210     log_info(gc)("%s", msg.buffer());
2211     Events::log(Thread::current(), "%s", msg.buffer());
2212   }
2213 }
2214 
2215 uint ShenandoahHeap::max_workers() {
2216   return _max_workers;
2217 }
2218 
2219 void ShenandoahHeap::stop() {
2220   // The shutdown sequence should be able to terminate when GC is running.
2221 
2222   // Step 0. Notify policy to disable event recording.
2223   _shenandoah_policy-&gt;record_shutdown();
2224 
2225   // Step 1. Notify control thread that we are in shutdown.
2226   // Note that we cannot do that with stop(), because stop() is blocking and waits for the actual shutdown.
2227   // Doing stop() here would wait for the normal GC cycle to complete, never falling through to cancel below.
2228   control_thread()-&gt;prepare_for_graceful_shutdown();
2229 
2230   // Step 2. Notify GC workers that we are cancelling GC.
2231   cancel_gc(GCCause::_shenandoah_stop_vm);
2232 
2233   // Step 3. Wait until GC worker exits normally.
2234   control_thread()-&gt;stop();
2235 
2236   // Step 4. Stop String Dedup thread if it is active
2237   if (ShenandoahStringDedup::is_enabled()) {
2238     ShenandoahStringDedup::stop();
2239   }
2240 }
2241 
2242 void ShenandoahHeap::stw_unload_classes(bool full_gc) {
2243   if (!unload_classes()) return;
2244 
2245   // Unload classes and purge SystemDictionary.
2246   {
2247     ShenandoahGCPhase phase(full_gc ?
2248                             ShenandoahPhaseTimings::full_gc_purge_class_unload :
2249                             ShenandoahPhaseTimings::purge_class_unload);
2250     bool purged_class = SystemDictionary::do_unloading(gc_timer());
2251 
2252     ShenandoahIsAliveSelector is_alive;
2253     uint num_workers = _workers-&gt;active_workers();
2254     ShenandoahClassUnloadingTask unlink_task(is_alive.is_alive_closure(), num_workers, purged_class);
2255     _workers-&gt;run_task(&amp;unlink_task);
2256   }
2257 
2258   {
2259     ShenandoahGCPhase phase(full_gc ?
2260                             ShenandoahPhaseTimings::full_gc_purge_cldg :
2261                             ShenandoahPhaseTimings::purge_cldg);
2262     ClassLoaderDataGraph::purge();
2263   }
2264   // Resize and verify metaspace
2265   MetaspaceGC::compute_new_size();
2266   MetaspaceUtils::verify_metrics();
2267 }
2268 
2269 // Weak roots are either pre-evacuated (final mark) or updated (final updaterefs),
2270 // so they should not have forwarded oops.
2271 // However, we do need to "null" dead oops in the roots, if can not be done
2272 // in concurrent cycles.
2273 void ShenandoahHeap::stw_process_weak_roots(bool full_gc) {
2274   ShenandoahGCPhase root_phase(full_gc ?
2275                                ShenandoahPhaseTimings::full_gc_purge :
2276                                ShenandoahPhaseTimings::purge);
2277   uint num_workers = _workers-&gt;active_workers();
2278   ShenandoahPhaseTimings::Phase timing_phase = full_gc ?
2279                                                ShenandoahPhaseTimings::full_gc_purge_weak_par :
2280                                                ShenandoahPhaseTimings::purge_weak_par;
2281   ShenandoahGCPhase phase(timing_phase);
2282   ShenandoahGCWorkerPhase worker_phase(timing_phase);
2283 
2284   // Cleanup weak roots
2285   if (has_forwarded_objects()) {
2286     ShenandoahForwardedIsAliveClosure is_alive;
2287     ShenandoahUpdateRefsClosure keep_alive;
2288     ShenandoahParallelWeakRootsCleaningTask&lt;ShenandoahForwardedIsAliveClosure, ShenandoahUpdateRefsClosure&gt;
2289       cleaning_task(timing_phase, &amp;is_alive, &amp;keep_alive, num_workers, !ShenandoahConcurrentRoots::should_do_concurrent_class_unloading());
2290     _workers-&gt;run_task(&amp;cleaning_task);
2291   } else {
2292     ShenandoahIsAliveClosure is_alive;
2293 #ifdef ASSERT
2294     ShenandoahAssertNotForwardedClosure verify_cl;
2295     ShenandoahParallelWeakRootsCleaningTask&lt;ShenandoahIsAliveClosure, ShenandoahAssertNotForwardedClosure&gt;
2296       cleaning_task(timing_phase, &amp;is_alive, &amp;verify_cl, num_workers, !ShenandoahConcurrentRoots::should_do_concurrent_class_unloading());
2297 #else
2298     ShenandoahParallelWeakRootsCleaningTask&lt;ShenandoahIsAliveClosure, DoNothingClosure&gt;
2299       cleaning_task(timing_phase, &amp;is_alive, &amp;do_nothing_cl, num_workers, !ShenandoahConcurrentRoots::should_do_concurrent_class_unloading());
2300 #endif
2301     _workers-&gt;run_task(&amp;cleaning_task);
2302   }
2303 }
2304 
2305 void ShenandoahHeap::parallel_cleaning(bool full_gc) {
2306   assert(SafepointSynchronize::is_at_safepoint(), "Must be at a safepoint");
2307   stw_process_weak_roots(full_gc);
2308   if (!ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
2309     stw_unload_classes(full_gc);
2310   }
2311 }
2312 
2313 void ShenandoahHeap::set_has_forwarded_objects(bool cond) {
2314   set_gc_state_mask(HAS_FORWARDED, cond);
2315 }
2316 
2317 void ShenandoahHeap::set_process_references(bool pr) {
2318   _process_references.set_cond(pr);
2319 }
2320 
2321 void ShenandoahHeap::set_unload_classes(bool uc) {
2322   _unload_classes.set_cond(uc);
2323 }
2324 
2325 bool ShenandoahHeap::process_references() const {
2326   return _process_references.is_set();
2327 }
2328 
2329 bool ShenandoahHeap::unload_classes() const {
2330   return _unload_classes.is_set();
2331 }
2332 
2333 address ShenandoahHeap::in_cset_fast_test_addr() {
2334   ShenandoahHeap* heap = ShenandoahHeap::heap();
2335   assert(heap-&gt;collection_set() != NULL, "Sanity");
2336   return (address) heap-&gt;collection_set()-&gt;biased_map_address();
2337 }
2338 
2339 address ShenandoahHeap::cancelled_gc_addr() {
2340   return (address) ShenandoahHeap::heap()-&gt;_cancelled_gc.addr_of();
2341 }
2342 
2343 address ShenandoahHeap::gc_state_addr() {
2344   return (address) ShenandoahHeap::heap()-&gt;_gc_state.addr_of();
2345 }
2346 
2347 size_t ShenandoahHeap::bytes_allocated_since_gc_start() {
2348   return Atomic::load_acquire(&amp;_bytes_allocated_since_gc_start);
2349 }
2350 
2351 void ShenandoahHeap::reset_bytes_allocated_since_gc_start() {
2352   Atomic::release_store_fence(&amp;_bytes_allocated_since_gc_start, (size_t)0);
2353 }
2354 
2355 void ShenandoahHeap::set_degenerated_gc_in_progress(bool in_progress) {
2356   _degenerated_gc_in_progress.set_cond(in_progress);
2357 }
2358 
2359 void ShenandoahHeap::set_full_gc_in_progress(bool in_progress) {
2360   _full_gc_in_progress.set_cond(in_progress);
2361 }
2362 
2363 void ShenandoahHeap::set_full_gc_move_in_progress(bool in_progress) {
2364   assert (is_full_gc_in_progress(), "should be");
2365   _full_gc_move_in_progress.set_cond(in_progress);
2366 }
2367 
2368 void ShenandoahHeap::set_update_refs_in_progress(bool in_progress) {
2369   set_gc_state_mask(UPDATEREFS, in_progress);
2370 }
2371 
2372 void ShenandoahHeap::register_nmethod(nmethod* nm) {
2373   ShenandoahCodeRoots::register_nmethod(nm);
2374 }
2375 
2376 void ShenandoahHeap::unregister_nmethod(nmethod* nm) {
2377   ShenandoahCodeRoots::unregister_nmethod(nm);
2378 }
2379 
2380 void ShenandoahHeap::flush_nmethod(nmethod* nm) {
2381   ShenandoahCodeRoots::flush_nmethod(nm);
2382 }
2383 
2384 oop ShenandoahHeap::pin_object(JavaThread* thr, oop o) {
2385   heap_region_containing(o)-&gt;record_pin();
2386   return o;
2387 }
2388 
2389 void ShenandoahHeap::unpin_object(JavaThread* thr, oop o) {
2390   heap_region_containing(o)-&gt;record_unpin();
2391 }
2392 
2393 void ShenandoahHeap::sync_pinned_region_status() {
2394   ShenandoahHeapLocker locker(lock());
2395 
2396   for (size_t i = 0; i &lt; num_regions(); i++) {
2397     ShenandoahHeapRegion *r = get_region(i);
2398     if (r-&gt;is_active()) {
2399       if (r-&gt;is_pinned()) {
2400         if (r-&gt;pin_count() == 0) {
2401           r-&gt;make_unpinned();
2402         }
2403       } else {
2404         if (r-&gt;pin_count() &gt; 0) {
2405           r-&gt;make_pinned();
2406         }
2407       }
2408     }
2409   }
2410 
2411   assert_pinned_region_status();
2412 }
2413 
2414 #ifdef ASSERT
2415 void ShenandoahHeap::assert_pinned_region_status() {
2416   for (size_t i = 0; i &lt; num_regions(); i++) {
2417     ShenandoahHeapRegion* r = get_region(i);
2418     assert((r-&gt;is_pinned() &amp;&amp; r-&gt;pin_count() &gt; 0) || (!r-&gt;is_pinned() &amp;&amp; r-&gt;pin_count() == 0),
2419            "Region " SIZE_FORMAT " pinning status is inconsistent", i);
2420   }
2421 }
2422 #endif
2423 
2424 ConcurrentGCTimer* ShenandoahHeap::gc_timer() const {
2425   return _gc_timer;
2426 }
2427 
2428 void ShenandoahHeap::prepare_concurrent_roots() {
2429   assert(SafepointSynchronize::is_at_safepoint(), "Must be at a safepoint");
2430   if (ShenandoahConcurrentRoots::should_do_concurrent_roots()) {
2431     set_concurrent_strong_root_in_progress(!collection_set()-&gt;is_empty());
2432     set_concurrent_weak_root_in_progress(true);
2433   }
2434 }
2435 
2436 void ShenandoahHeap::prepare_concurrent_unloading() {
2437   assert(SafepointSynchronize::is_at_safepoint(), "Must be at a safepoint");
2438   if (ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
2439     _unloader.prepare();
2440   }
2441 }
2442 
2443 void ShenandoahHeap::finish_concurrent_unloading() {
2444   assert(SafepointSynchronize::is_at_safepoint(), "Must be at a safepoint");
2445   if (ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
2446     _unloader.finish();
2447   }
2448 }
2449 
2450 #ifdef ASSERT
2451 void ShenandoahHeap::assert_gc_workers(uint nworkers) {
2452   assert(nworkers &gt; 0 &amp;&amp; nworkers &lt;= max_workers(), "Sanity");
2453 
2454   if (ShenandoahSafepoint::is_at_shenandoah_safepoint()) {
2455     if (UseDynamicNumberOfGCThreads) {
2456       assert(nworkers &lt;= ParallelGCThreads, "Cannot use more than it has");
2457     } else {
2458       // Use ParallelGCThreads inside safepoints
2459       assert(nworkers == ParallelGCThreads, "Use ParallelGCThreads within safepoints");
2460     }
2461   } else {
2462     if (UseDynamicNumberOfGCThreads) {
2463       assert(nworkers &lt;= ConcGCThreads, "Cannot use more than it has");
2464     } else {
2465       // Use ConcGCThreads outside safepoints
2466       assert(nworkers == ConcGCThreads, "Use ConcGCThreads outside safepoints");
2467     }
2468   }
2469 }
2470 #endif
2471 
2472 ShenandoahVerifier* ShenandoahHeap::verifier() {
2473   guarantee(ShenandoahVerify, "Should be enabled");
2474   assert (_verifier != NULL, "sanity");
2475   return _verifier;
2476 }
2477 
2478 template&lt;class T&gt;
2479 class ShenandoahUpdateHeapRefsTask : public AbstractGangTask {
2480 private:
2481   T cl;
2482   ShenandoahHeap* _heap;
2483   ShenandoahRegionIterator* _regions;
2484   bool _concurrent;
2485 public:
2486   ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions, bool concurrent) :
2487     AbstractGangTask("Concurrent Update References Task"),
2488     cl(T()),
2489     _heap(ShenandoahHeap::heap()),
2490     _regions(regions),
2491     _concurrent(concurrent) {
2492   }
2493 
2494   void work(uint worker_id) {
2495     if (_concurrent) {
2496       ShenandoahConcurrentWorkerSession worker_session(worker_id);
2497       ShenandoahSuspendibleThreadSetJoiner stsj(ShenandoahSuspendibleWorkers);
2498       do_work();
2499     } else {
2500       ShenandoahParallelWorkerSession worker_session(worker_id);
2501       do_work();
2502     }
2503   }
2504 
2505 private:
2506   void do_work() {
2507     ShenandoahHeapRegion* r = _regions-&gt;next();
2508     ShenandoahMarkingContext* const ctx = _heap-&gt;complete_marking_context();
2509     while (r != NULL) {
2510       HeapWord* update_watermark = r-&gt;get_update_watermark();
2511       assert (update_watermark &gt;= r-&gt;bottom(), "sanity");
2512       if (r-&gt;is_active() &amp;&amp; !r-&gt;is_cset()) {
2513         _heap-&gt;marked_object_oop_iterate(r, &amp;cl, update_watermark);
2514       }
2515       if (ShenandoahPacing) {
2516         _heap-&gt;pacer()-&gt;report_updaterefs(pointer_delta(update_watermark, r-&gt;bottom()));
2517       }
2518       if (_heap-&gt;check_cancelled_gc_and_yield(_concurrent)) {
2519         return;
2520       }
2521       r = _regions-&gt;next();
2522     }
2523   }
2524 };
2525 
2526 void ShenandoahHeap::update_heap_references(bool concurrent) {
2527   ShenandoahUpdateHeapRefsTask&lt;ShenandoahUpdateHeapRefsClosure&gt; task(&amp;_update_refs_iterator, concurrent);
2528   workers()-&gt;run_task(&amp;task);
2529 }
2530 
2531 void ShenandoahHeap::op_init_updaterefs() {
2532   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), "must be at safepoint");
2533 
2534   set_evacuation_in_progress(false);
2535 
2536   // Evacuation is over, no GCLABs are needed anymore. GCLABs are under URWM, so we need to
2537   // make them parsable for update code to work correctly. Plus, we can compute new sizes
2538   // for future GCLABs here.
2539   if (UseTLAB) {
2540     ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_refs_manage_gclabs);
2541     gclabs_retire(ResizeTLAB);
2542   }
2543 
2544   if (ShenandoahVerify) {
2545     if (!is_degenerated_gc_in_progress()) {
2546       verifier()-&gt;verify_roots_in_to_space_except(ShenandoahRootVerifier::ThreadRoots);
2547     }
2548     verifier()-&gt;verify_before_updaterefs();
2549   }
2550 
2551   set_update_refs_in_progress(true);
2552 
2553   _update_refs_iterator.reset();
2554 
2555   if (ShenandoahPacing) {
2556     pacer()-&gt;setup_for_updaterefs();
2557   }
2558 }
2559 
2560 class ShenandoahFinalUpdateRefsUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {
2561 private:
2562   ShenandoahHeapLock* const _lock;
2563 
2564 public:
2565   ShenandoahFinalUpdateRefsUpdateRegionStateClosure() : _lock(ShenandoahHeap::heap()-&gt;lock()) {}
2566 
2567   void heap_region_do(ShenandoahHeapRegion* r) {
2568     // Drop unnecessary "pinned" state from regions that does not have CP marks
2569     // anymore, as this would allow trashing them.
2570 
2571     if (r-&gt;is_active()) {
2572       if (r-&gt;is_pinned()) {
2573         if (r-&gt;pin_count() == 0) {
2574           ShenandoahHeapLocker locker(_lock);
2575           r-&gt;make_unpinned();
2576         }
2577       } else {
2578         if (r-&gt;pin_count() &gt; 0) {
2579           ShenandoahHeapLocker locker(_lock);
2580           r-&gt;make_pinned();
2581         }
2582       }
2583     }
2584   }
2585 
2586   bool is_thread_safe() { return true; }
2587 };
2588 
2589 void ShenandoahHeap::op_final_updaterefs() {
2590   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), "must be at safepoint");
2591 
2592   finish_concurrent_unloading();
2593 
2594   // Check if there is left-over work, and finish it
2595   if (_update_refs_iterator.has_next()) {
2596     ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_refs_finish_work);
2597 
2598     // Finish updating references where we left off.
2599     clear_cancelled_gc();
2600     update_heap_references(false);
2601   }
2602 
2603   // Clear cancelled GC, if set. On cancellation path, the block before would handle
2604   // everything. On degenerated paths, cancelled gc would not be set anyway.
2605   if (cancelled_gc()) {
2606     clear_cancelled_gc();
2607   }
2608   assert(!cancelled_gc(), "Should have been done right before");
2609 
2610   if (ShenandoahVerify &amp;&amp; !is_degenerated_gc_in_progress()) {
2611     verifier()-&gt;verify_roots_in_to_space_except(ShenandoahRootVerifier::ThreadRoots);
2612   }
2613 
2614   if (is_degenerated_gc_in_progress()) {
2615     concurrent_mark()-&gt;update_roots(ShenandoahPhaseTimings::degen_gc_update_roots);
2616   } else {
2617     concurrent_mark()-&gt;update_thread_roots(ShenandoahPhaseTimings::final_update_refs_roots);
2618   }
2619 
2620   // Has to be done before cset is clear
2621   if (ShenandoahVerify) {
2622     verifier()-&gt;verify_roots_in_to_space();
2623   }
2624 
2625   {
2626     ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_refs_update_region_states);
2627     ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl;
2628     parallel_heap_region_iterate(&amp;cl);
2629 
2630     assert_pinned_region_status();
2631   }
2632 
2633   {
2634     ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_refs_trash_cset);
2635     trash_cset_regions();
2636   }
2637 
2638   set_has_forwarded_objects(false);
2639   set_update_refs_in_progress(false);
2640 
2641   if (ShenandoahVerify) {
2642     verifier()-&gt;verify_after_updaterefs();
2643   }
2644 
2645   if (VerifyAfterGC) {
2646     Universe::verify();
2647   }
2648 
2649   {
2650     ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_refs_rebuild_freeset);
2651     ShenandoahHeapLocker locker(lock());
2652     _free_set-&gt;rebuild();
2653   }
2654 }
2655 
2656 void ShenandoahHeap::print_extended_on(outputStream *st) const {
2657   print_on(st);
2658   print_heap_regions_on(st);
2659 }
2660 
2661 bool ShenandoahHeap::is_bitmap_slice_committed(ShenandoahHeapRegion* r, bool skip_self) {
2662   size_t slice = r-&gt;index() / _bitmap_regions_per_slice;
2663 
2664   size_t regions_from = _bitmap_regions_per_slice * slice;
2665   size_t regions_to   = MIN2(num_regions(), _bitmap_regions_per_slice * (slice + 1));
2666   for (size_t g = regions_from; g &lt; regions_to; g++) {
2667     assert (g / _bitmap_regions_per_slice == slice, "same slice");
2668     if (skip_self &amp;&amp; g == r-&gt;index()) continue;
2669     if (get_region(g)-&gt;is_committed()) {
2670       return true;
2671     }
2672   }
2673   return false;
2674 }
2675 
2676 bool ShenandoahHeap::commit_bitmap_slice(ShenandoahHeapRegion* r) {
2677   shenandoah_assert_heaplocked();
2678 
2679   // Bitmaps in special regions do not need commits
2680   if (_bitmap_region_special) {
2681     return true;
2682   }
2683 
2684   if (is_bitmap_slice_committed(r, true)) {
2685     // Some other region from the group is already committed, meaning the bitmap
2686     // slice is already committed, we exit right away.
2687     return true;
2688   }
2689 
2690   // Commit the bitmap slice:
2691   size_t slice = r-&gt;index() / _bitmap_regions_per_slice;
2692   size_t off = _bitmap_bytes_per_slice * slice;
2693   size_t len = _bitmap_bytes_per_slice;
2694   char* start = (char*) _bitmap_region.start() + off;
2695 
2696   if (!os::commit_memory(start, len, false)) {
2697     return false;
2698   }
2699 
2700   if (AlwaysPreTouch) {
2701     os::pretouch_memory(start, start + len, _pretouch_bitmap_page_size);
2702   }
2703 
2704   return true;
2705 }
2706 
2707 bool ShenandoahHeap::uncommit_bitmap_slice(ShenandoahHeapRegion *r) {
2708   shenandoah_assert_heaplocked();
2709 
2710   // Bitmaps in special regions do not need uncommits
2711   if (_bitmap_region_special) {
2712     return true;
2713   }
2714 
2715   if (is_bitmap_slice_committed(r, true)) {
2716     // Some other region from the group is still committed, meaning the bitmap
2717     // slice is should stay committed, exit right away.
2718     return true;
2719   }
2720 
2721   // Uncommit the bitmap slice:
2722   size_t slice = r-&gt;index() / _bitmap_regions_per_slice;
2723   size_t off = _bitmap_bytes_per_slice * slice;
2724   size_t len = _bitmap_bytes_per_slice;
2725   if (!os::uncommit_memory((char*)_bitmap_region.start() + off, len)) {
2726     return false;
2727   }
2728   return true;
2729 }
2730 
2731 void ShenandoahHeap::safepoint_synchronize_begin() {
2732   if (ShenandoahSuspendibleWorkers || UseStringDeduplication) {
2733     SuspendibleThreadSet::synchronize();
2734   }
2735 }
2736 
2737 void ShenandoahHeap::safepoint_synchronize_end() {
2738   if (ShenandoahSuspendibleWorkers || UseStringDeduplication) {
2739     SuspendibleThreadSet::desynchronize();
2740   }
2741 }
2742 
2743 void ShenandoahHeap::vmop_entry_init_mark() {
2744   TraceCollectorStats tcs(monitoring_support()-&gt;stw_collection_counters());
2745   ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::init_mark_gross);
2746 
2747   try_inject_alloc_failure();
2748   VM_ShenandoahInitMark op;
2749   VMThread::execute(&amp;op); // jump to entry_init_mark() under safepoint
2750 }
2751 
2752 void ShenandoahHeap::vmop_entry_final_mark() {
2753   TraceCollectorStats tcs(monitoring_support()-&gt;stw_collection_counters());
2754   ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::final_mark_gross);
2755 
2756   try_inject_alloc_failure();
2757   VM_ShenandoahFinalMarkStartEvac op;
2758   VMThread::execute(&amp;op); // jump to entry_final_mark under safepoint
2759 }
2760 
2761 void ShenandoahHeap::vmop_entry_init_updaterefs() {
2762   TraceCollectorStats tcs(monitoring_support()-&gt;stw_collection_counters());
2763   ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::init_update_refs_gross);
2764 
2765   try_inject_alloc_failure();
2766   VM_ShenandoahInitUpdateRefs op;
2767   VMThread::execute(&amp;op);
2768 }
2769 
2770 void ShenandoahHeap::vmop_entry_final_updaterefs() {
2771   TraceCollectorStats tcs(monitoring_support()-&gt;stw_collection_counters());
2772   ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::final_update_refs_gross);
2773 
2774   try_inject_alloc_failure();
2775   VM_ShenandoahFinalUpdateRefs op;
2776   VMThread::execute(&amp;op);
2777 }
2778 
2779 void ShenandoahHeap::vmop_entry_full(GCCause::Cause cause) {
2780   TraceCollectorStats tcs(monitoring_support()-&gt;full_stw_collection_counters());
2781   ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::full_gc_gross);
2782 
2783   try_inject_alloc_failure();
2784   VM_ShenandoahFullGC op(cause);
2785   VMThread::execute(&amp;op);
2786 }
2787 
2788 void ShenandoahHeap::vmop_degenerated(ShenandoahDegenPoint point) {
2789   TraceCollectorStats tcs(monitoring_support()-&gt;full_stw_collection_counters());
2790   ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::degen_gc_gross);
2791 
2792   VM_ShenandoahDegeneratedGC degenerated_gc((int)point);
2793   VMThread::execute(&amp;degenerated_gc);
2794 }
2795 
2796 void ShenandoahHeap::entry_init_mark() {
2797   const char* msg = init_mark_event_message();
2798   ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::init_mark);
2799   EventMark em("%s", msg);
2800 
2801   ShenandoahWorkerScope scope(workers(),
2802                               ShenandoahWorkerPolicy::calc_workers_for_init_marking(),
2803                               "init marking");
2804 
2805   op_init_mark();
2806 }
2807 
2808 void ShenandoahHeap::entry_final_mark() {
2809   const char* msg = final_mark_event_message();
2810   ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::final_mark);
2811   EventMark em("%s", msg);
2812 
2813   ShenandoahWorkerScope scope(workers(),
2814                               ShenandoahWorkerPolicy::calc_workers_for_final_marking(),
2815                               "final marking");
2816 
2817   op_final_mark();
2818 }
2819 
2820 void ShenandoahHeap::entry_init_updaterefs() {
2821   static const char* msg = "Pause Init Update Refs";
2822   ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::init_update_refs);
2823   EventMark em("%s", msg);
2824 
2825   // No workers used in this phase, no setup required
2826 
2827   op_init_updaterefs();
2828 }
2829 
2830 void ShenandoahHeap::entry_final_updaterefs() {
2831   static const char* msg = "Pause Final Update Refs";
2832   ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::final_update_refs);
2833   EventMark em("%s", msg);
2834 
2835   ShenandoahWorkerScope scope(workers(),
2836                               ShenandoahWorkerPolicy::calc_workers_for_final_update_ref(),
2837                               "final reference update");
2838 
2839   op_final_updaterefs();
2840 }
2841 
2842 void ShenandoahHeap::entry_full(GCCause::Cause cause) {
2843   static const char* msg = "Pause Full";
2844   ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::full_gc, true /* log_heap_usage */);
2845   EventMark em("%s", msg);
2846 
2847   ShenandoahWorkerScope scope(workers(),
2848                               ShenandoahWorkerPolicy::calc_workers_for_fullgc(),
2849                               "full gc");
2850 
2851   op_full(cause);
2852 }
2853 
2854 void ShenandoahHeap::entry_degenerated(int point) {
2855   ShenandoahDegenPoint dpoint = (ShenandoahDegenPoint)point;
2856   const char* msg = degen_event_message(dpoint);
2857   ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::degen_gc, true /* log_heap_usage */);
2858   EventMark em("%s", msg);
2859 
2860   ShenandoahWorkerScope scope(workers(),
2861                               ShenandoahWorkerPolicy::calc_workers_for_stw_degenerated(),
2862                               "stw degenerated gc");
2863 
2864   set_degenerated_gc_in_progress(true);
2865   op_degenerated(dpoint);
2866   set_degenerated_gc_in_progress(false);
2867 }
2868 
2869 void ShenandoahHeap::entry_mark() {
2870   TraceCollectorStats tcs(monitoring_support()-&gt;concurrent_collection_counters());
2871 
2872   const char* msg = conc_mark_event_message();
2873   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_mark);
2874   EventMark em("%s", msg);
2875 
2876   ShenandoahWorkerScope scope(workers(),
2877                               ShenandoahWorkerPolicy::calc_workers_for_conc_marking(),
2878                               "concurrent marking");
2879 
2880   try_inject_alloc_failure();
2881   op_mark();
2882 }
2883 
2884 void ShenandoahHeap::entry_evac() {
2885   TraceCollectorStats tcs(monitoring_support()-&gt;concurrent_collection_counters());
2886 
2887   static const char* msg = "Concurrent evacuation";
2888   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_evac);
2889   EventMark em("%s", msg);
2890 
2891   ShenandoahWorkerScope scope(workers(),
2892                               ShenandoahWorkerPolicy::calc_workers_for_conc_evac(),
2893                               "concurrent evacuation");
2894 
2895   try_inject_alloc_failure();
2896   op_conc_evac();
2897 }
2898 
2899 void ShenandoahHeap::entry_updaterefs() {
2900   static const char* msg = "Concurrent update references";
2901   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_update_refs);
2902   EventMark em("%s", msg);
2903 
2904   ShenandoahWorkerScope scope(workers(),
2905                               ShenandoahWorkerPolicy::calc_workers_for_conc_update_ref(),
2906                               "concurrent reference update");
2907 
2908   try_inject_alloc_failure();
2909   op_updaterefs();
2910 }
2911 
2912 void ShenandoahHeap::entry_weak_roots() {
2913   static const char* msg = "Concurrent weak roots";
2914   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_weak_roots);
2915   EventMark em("%s", msg);
2916 
2917   ShenandoahWorkerScope scope(workers(),
2918                               ShenandoahWorkerPolicy::calc_workers_for_conc_root_processing(),
2919                               "concurrent weak root");
2920 
2921   try_inject_alloc_failure();
2922   op_weak_roots();
2923 }
2924 
2925 void ShenandoahHeap::entry_class_unloading() {
2926   static const char* msg = "Concurrent class unloading";
2927   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_class_unload);
2928   EventMark em("%s", msg);
2929 
2930   ShenandoahWorkerScope scope(workers(),
2931                               ShenandoahWorkerPolicy::calc_workers_for_conc_root_processing(),
2932                               "concurrent class unloading");
2933 
2934   try_inject_alloc_failure();
2935   op_class_unloading();
2936 }
2937 
2938 void ShenandoahHeap::entry_strong_roots() {
2939   static const char* msg = "Concurrent strong roots";
2940   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_strong_roots);
2941   EventMark em("%s", msg);
2942 
2943   ShenandoahGCWorkerPhase worker_phase(ShenandoahPhaseTimings::conc_strong_roots);
2944 
2945   ShenandoahWorkerScope scope(workers(),
2946                               ShenandoahWorkerPolicy::calc_workers_for_conc_root_processing(),
2947                               "concurrent strong root");
2948 
2949   try_inject_alloc_failure();
2950   op_strong_roots();
2951 }
2952 
2953 void ShenandoahHeap::entry_cleanup_early() {
2954   static const char* msg = "Concurrent cleanup";
2955   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_cleanup_early, true /* log_heap_usage */);
2956   EventMark em("%s", msg);
2957 
2958   // This phase does not use workers, no need for setup
2959 
2960   try_inject_alloc_failure();
2961   op_cleanup_early();
2962 }
2963 
2964 void ShenandoahHeap::entry_cleanup_complete() {
2965   static const char* msg = "Concurrent cleanup";
2966   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_cleanup_complete, true /* log_heap_usage */);
2967   EventMark em("%s", msg);
2968 
2969   // This phase does not use workers, no need for setup
2970 
2971   try_inject_alloc_failure();
2972   op_cleanup_complete();
2973 }
2974 
2975 void ShenandoahHeap::entry_reset() {
2976   static const char* msg = "Concurrent reset";
2977   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset);
2978   EventMark em("%s", msg);
2979 
2980   ShenandoahWorkerScope scope(workers(),
2981                               ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),
2982                               "concurrent reset");
2983 
2984   try_inject_alloc_failure();
2985   op_reset();
2986 }
2987 
2988 void ShenandoahHeap::entry_preclean() {
2989   if (ShenandoahPreclean &amp;&amp; process_references()) {
2990     static const char* msg = "Concurrent precleaning";
2991     ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_preclean);
2992     EventMark em("%s", msg);
2993 
2994     ShenandoahWorkerScope scope(workers(),
2995                                 ShenandoahWorkerPolicy::calc_workers_for_conc_preclean(),
2996                                 "concurrent preclean",
2997                                 /* check_workers = */ false);
2998 
2999     try_inject_alloc_failure();
3000     op_preclean();
3001   }
3002 }
3003 
3004 void ShenandoahHeap::entry_uncommit(double shrink_before) {
3005   static const char *msg = "Concurrent uncommit";
3006   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_uncommit, true /* log_heap_usage */);
3007   EventMark em("%s", msg);
3008 
3009   op_uncommit(shrink_before);
3010 }
3011 
3012 void ShenandoahHeap::try_inject_alloc_failure() {
3013   if (ShenandoahAllocFailureALot &amp;&amp; !cancelled_gc() &amp;&amp; ((os::random() % 1000) &gt; 950)) {
3014     _inject_alloc_failure.set();
3015     os::naked_short_sleep(1);
3016     if (cancelled_gc()) {
3017       log_info(gc)("Allocation failure was successfully injected");
3018     }
3019   }
3020 }
3021 
3022 bool ShenandoahHeap::should_inject_alloc_failure() {
3023   return _inject_alloc_failure.is_set() &amp;&amp; _inject_alloc_failure.try_unset();
3024 }
3025 
3026 void ShenandoahHeap::initialize_serviceability() {
3027   _memory_pool = new ShenandoahMemoryPool(this);
3028   _cycle_memory_manager.add_pool(_memory_pool);
3029   _stw_memory_manager.add_pool(_memory_pool);
3030 }
3031 
3032 GrowableArray&lt;GCMemoryManager*&gt; ShenandoahHeap::memory_managers() {
3033   GrowableArray&lt;GCMemoryManager*&gt; memory_managers(2);
3034   memory_managers.append(&amp;_cycle_memory_manager);
3035   memory_managers.append(&amp;_stw_memory_manager);
3036   return memory_managers;
3037 }
3038 
3039 GrowableArray&lt;MemoryPool*&gt; ShenandoahHeap::memory_pools() {
3040   GrowableArray&lt;MemoryPool*&gt; memory_pools(1);
3041   memory_pools.append(_memory_pool);
3042   return memory_pools;
3043 }
3044 
3045 MemoryUsage ShenandoahHeap::memory_usage() {
3046   return _memory_pool-&gt;get_memory_usage();
3047 }
3048 
3049 ShenandoahRegionIterator::ShenandoahRegionIterator() :
3050   _heap(ShenandoahHeap::heap()),
3051   _index(0) {}
3052 
3053 ShenandoahRegionIterator::ShenandoahRegionIterator(ShenandoahHeap* heap) :
3054   _heap(heap),
3055   _index(0) {}
3056 
3057 void ShenandoahRegionIterator::reset() {
3058   _index = 0;
3059 }
3060 
3061 bool ShenandoahRegionIterator::has_next() const {
3062   return _index &lt; _heap-&gt;num_regions();
3063 }
3064 
3065 char ShenandoahHeap::gc_state() const {
3066   return _gc_state.raw_value();
3067 }
3068 
3069 void ShenandoahHeap::deduplicate_string(oop str) {
3070   assert(java_lang_String::is_instance(str), "invariant");
3071 
3072   if (ShenandoahStringDedup::is_enabled()) {
3073     ShenandoahStringDedup::deduplicate(str);
3074   }
3075 }
3076 
3077 const char* ShenandoahHeap::init_mark_event_message() const {
3078   assert(!has_forwarded_objects(), "Should not have forwarded objects here");
3079 
3080   bool proc_refs = process_references();
3081   bool unload_cls = unload_classes();
3082 
3083   if (proc_refs &amp;&amp; unload_cls) {
3084     return "Pause Init Mark (process weakrefs) (unload classes)";
3085   } else if (proc_refs) {
3086     return "Pause Init Mark (process weakrefs)";
3087   } else if (unload_cls) {
3088     return "Pause Init Mark (unload classes)";
3089   } else {
3090     return "Pause Init Mark";
3091   }
3092 }
3093 
3094 const char* ShenandoahHeap::final_mark_event_message() const {
3095   assert(!has_forwarded_objects(), "Should not have forwarded objects here");
3096 
3097   bool proc_refs = process_references();
3098   bool unload_cls = unload_classes();
3099 
3100   if (proc_refs &amp;&amp; unload_cls) {
3101     return "Pause Final Mark (process weakrefs) (unload classes)";
3102   } else if (proc_refs) {
3103     return "Pause Final Mark (process weakrefs)";
3104   } else if (unload_cls) {
3105     return "Pause Final Mark (unload classes)";
3106   } else {
3107     return "Pause Final Mark";
3108   }
3109 }
3110 
3111 const char* ShenandoahHeap::conc_mark_event_message() const {
3112   assert(!has_forwarded_objects(), "Should not have forwarded objects here");
3113 
3114   bool proc_refs = process_references();
3115   bool unload_cls = unload_classes();
3116 
3117   if (proc_refs &amp;&amp; unload_cls) {
3118     return "Concurrent marking (process weakrefs) (unload classes)";
3119   } else if (proc_refs) {
3120     return "Concurrent marking (process weakrefs)";
3121   } else if (unload_cls) {
3122     return "Concurrent marking (unload classes)";
3123   } else {
3124     return "Concurrent marking";
3125   }
3126 }
3127 
3128 const char* ShenandoahHeap::degen_event_message(ShenandoahDegenPoint point) const {
3129   switch (point) {
3130     case _degenerated_unset:
3131       return "Pause Degenerated GC (&lt;UNSET&gt;)";
3132     case _degenerated_outside_cycle:
3133       return "Pause Degenerated GC (Outside of Cycle)";
3134     case _degenerated_mark:
3135       return "Pause Degenerated GC (Mark)";
3136     case _degenerated_evac:
3137       return "Pause Degenerated GC (Evacuation)";
3138     case _degenerated_updaterefs:
3139       return "Pause Degenerated GC (Update Refs)";
3140     default:
3141       ShouldNotReachHere();
3142       return "ERROR";
3143   }
3144 }
3145 
3146 ShenandoahLiveData* ShenandoahHeap::get_liveness_cache(uint worker_id) {
3147 #ifdef ASSERT
3148   assert(_liveness_cache != NULL, "sanity");
3149   assert(worker_id &lt; _max_workers, "sanity");
3150   for (uint i = 0; i &lt; num_regions(); i++) {
3151     assert(_liveness_cache[worker_id][i] == 0, "liveness cache should be empty");
3152   }
3153 #endif
3154   return _liveness_cache[worker_id];
3155 }
3156 
3157 void ShenandoahHeap::flush_liveness_cache(uint worker_id) {
3158   assert(worker_id &lt; _max_workers, "sanity");
3159   assert(_liveness_cache != NULL, "sanity");
3160   ShenandoahLiveData* ld = _liveness_cache[worker_id];
3161   for (uint i = 0; i &lt; num_regions(); i++) {
3162     ShenandoahLiveData live = ld[i];
3163     if (live &gt; 0) {
3164       ShenandoahHeapRegion* r = get_region(i);
3165       r-&gt;increase_live_data_gc_words(live);
3166       ld[i] = 0;
3167     }
3168   }
3169 }
</pre></body></html>
