<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>New src/hotspot/share/memory/metaspace.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 2011, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 
  27 #include "aot/aotLoader.hpp"
  28 #include "gc/shared/collectedHeap.hpp"
  29 #include "logging/log.hpp"
  30 #include "logging/logStream.hpp"
  31 #include "memory/filemap.hpp"
  32 #include "memory/metaspace.hpp"
  33 #include "memory/metaspaceShared.hpp"
  34 #include "memory/metaspaceTracer.hpp"
  35 #include "memory/metaspace/chunkManager.hpp"
  36 #include "memory/metaspace/classLoaderMetaspace.hpp"
  37 #include "memory/metaspace/commitLimiter.hpp"
  38 #include "memory/metaspace/metaspaceCommon.hpp"
  39 #include "memory/metaspace/metaspaceEnums.hpp"
  40 #include "memory/metaspace/metaspaceReport.hpp"
  41 #include "memory/metaspace/metaspaceSizesSnapshot.hpp"
  42 #include "memory/metaspace/runningCounters.hpp"
  43 #include "memory/metaspace/settings.hpp"
  44 #include "memory/metaspace/virtualSpaceList.hpp"
  45 #include "memory/universe.hpp"
  46 #include "oops/compressedOops.hpp"
  47 #include "runtime/atomic.hpp"
  48 #include "runtime/init.hpp"
  49 #include "runtime/java.hpp"
  50 #include "services/memTracker.hpp"
  51 #include "utilities/copy.hpp"
  52 #include "utilities/debug.hpp"
  53 #include "utilities/formatBuffer.hpp"
  54 #include "utilities/globalDefinitions.hpp"
  55 
  56 
  57 using metaspace::ChunkManager;
  58 using metaspace::ClassLoaderMetaspace;
  59 using metaspace::CommitLimiter;
  60 using metaspace::MetaspaceType;
  61 using metaspace::MetaspaceReporter;
  62 using metaspace::RunningCounters;
  63 using metaspace::VirtualSpaceList;
  64 
  65 
  66 // Used by MetaspaceCounters
  67 size_t MetaspaceUtils::free_chunks_total_words(Metaspace::MetadataType mdtype) {
  68   return metaspace::is_class(mdtype) ? RunningCounters::free_chunks_words_class() : RunningCounters::free_chunks_words_nonclass();
  69 }
  70 
  71 size_t MetaspaceUtils::used_words() {
  72   return RunningCounters::used_words();
  73 }
  74 
  75 size_t MetaspaceUtils::used_words(Metaspace::MetadataType mdtype) {
  76   return metaspace::is_class(mdtype) ? RunningCounters::used_words_class() : RunningCounters::used_words_nonclass();
  77 }
  78 
  79 size_t MetaspaceUtils::reserved_words() {
  80   return RunningCounters::reserved_words();
  81 }
  82 
  83 size_t MetaspaceUtils::reserved_words(Metaspace::MetadataType mdtype) {
  84   return metaspace::is_class(mdtype) ? RunningCounters::reserved_words_class() : RunningCounters::reserved_words_nonclass();
  85 }
  86 
  87 size_t MetaspaceUtils::committed_words() {
  88   return RunningCounters::committed_words();
  89 }
  90 
  91 size_t MetaspaceUtils::committed_words(Metaspace::MetadataType mdtype) {
  92   return metaspace::is_class(mdtype) ? RunningCounters::committed_words_class() : RunningCounters::committed_words_nonclass();
  93 }
  94 
  95 
  96 
  97 void MetaspaceUtils::print_metaspace_change(const metaspace::MetaspaceSizesSnapshot&amp; pre_meta_values) {
  98   const metaspace::MetaspaceSizesSnapshot meta_values;
  99 
 100   // We print used and committed since these are the most useful at-a-glance vitals for Metaspace:
 101   // - used tells you how much memory is actually used for metadata
 102   // - committed tells you how much memory is committed for the purpose of metadata
 103   // The difference between those two would be waste, which can have various forms (freelists,
 104   //   unused parts of committed chunks etc)
 105   //
 106   // Left out is reserved, since this is not as exciting as the first two values: for class space,
 107   // it is a constant (to uninformed users, often confusingly large). For non-class space, it would
 108   // be interesting since free chunks can be uncommitted, but for now it is left out.
 109 
 110   if (Metaspace::using_class_space()) {
 111     log_info(gc, metaspace)(HEAP_CHANGE_FORMAT" "
 112                             HEAP_CHANGE_FORMAT" "
 113                             HEAP_CHANGE_FORMAT,
 114                             HEAP_CHANGE_FORMAT_ARGS("Metaspace",
 115                                                     pre_meta_values.used(),
 116                                                     pre_meta_values.committed(),
 117                                                     meta_values.used(),
 118                                                     meta_values.committed()),
 119                             HEAP_CHANGE_FORMAT_ARGS("NonClass",
 120                                                     pre_meta_values.non_class_used(),
 121                                                     pre_meta_values.non_class_committed(),
 122                                                     meta_values.non_class_used(),
 123                                                     meta_values.non_class_committed()),
 124                             HEAP_CHANGE_FORMAT_ARGS("Class",
 125                                                     pre_meta_values.class_used(),
 126                                                     pre_meta_values.class_committed(),
 127                                                     meta_values.class_used(),
 128                                                     meta_values.class_committed()));
 129   } else {
 130     log_info(gc, metaspace)(HEAP_CHANGE_FORMAT,
 131                             HEAP_CHANGE_FORMAT_ARGS("Metaspace",
 132                                                     pre_meta_values.used(),
 133                                                     pre_meta_values.committed(),
 134                                                     meta_values.used(),
 135                                                     meta_values.committed()));
 136   }
 137 }
 138 
 139 // This will print out a basic metaspace usage report but
 140 // unlike print_report() is guaranteed not to lock or to walk the CLDG.
 141 void MetaspaceUtils::print_basic_report(outputStream* out, size_t scale) {
 142   MetaspaceReporter::print_basic_report(out, scale);
 143 }
 144 
 145 // Prints a report about the current metaspace state.
 146 // Optional parts can be enabled via flags.
 147 // Function will walk the CLDG and will lock the expand lock; if that is not
 148 // convenient, use print_basic_report() instead.
 149 void MetaspaceUtils::print_full_report(outputStream* out, size_t scale) {
 150   const int flags =
 151       MetaspaceReporter::rf_show_loaders |
 152       MetaspaceReporter::rf_break_down_by_chunktype |
 153       MetaspaceReporter::rf_show_classes;
 154   MetaspaceReporter::print_report(out, scale, flags);
 155 }
 156 
 157 void MetaspaceUtils::print_on(outputStream* out) {
 158 
 159   // Used from all GCs. It first prints out totals, then, separately, the class space portion.
 160 
 161   out-&gt;print_cr(" Metaspace       "
 162                 "used "      SIZE_FORMAT "K, "
 163                 "committed " SIZE_FORMAT "K, "
 164                 "reserved "  SIZE_FORMAT "K",
 165                 used_bytes()/K,
 166                 committed_bytes()/K,
 167                 reserved_bytes()/K);
 168 
 169   if (Metaspace::using_class_space()) {
 170     const Metaspace::MetadataType ct = Metaspace::ClassType;
 171     out-&gt;print_cr("  class space    "
 172                   "used "      SIZE_FORMAT "K, "
 173                   "committed " SIZE_FORMAT "K, "
 174                   "reserved "  SIZE_FORMAT "K",
 175                   used_bytes(ct)/K,
 176                   committed_bytes(ct)/K,
 177                   reserved_bytes(ct)/K);
 178   }
 179 }
 180 
 181 #ifdef ASSERT
 182 void MetaspaceUtils::verify(bool slow) {
 183   if (Metaspace::initialized()) {
 184 
 185     // Verify non-class chunkmanager...
 186     ChunkManager* cm = ChunkManager::chunkmanager_nonclass();
 187     cm-&gt;verify(slow);
 188 
 189     // ... and space list.
 190     VirtualSpaceList* vsl = VirtualSpaceList::vslist_nonclass();
 191     vsl-&gt;verify(slow);
 192 
 193     if (Metaspace::using_class_space()) {
 194       // If we use compressed class pointers, verify class chunkmanager...
 195       cm = ChunkManager::chunkmanager_class();
 196       assert(cm != NULL, "Sanity");
 197       cm-&gt;verify(slow);
 198 
 199       // ... and class spacelist.
 200       VirtualSpaceList* vsl = VirtualSpaceList::vslist_nonclass();
 201       assert(vsl != NULL, "Sanity");
 202       vsl-&gt;verify(slow);
 203     }
 204 
 205   }
 206 }
 207 #endif
 208 
 209 ////////////////////////////////7
 210 // MetaspaceGC methods
 211 
 212 volatile size_t MetaspaceGC::_capacity_until_GC = 0;
 213 uint MetaspaceGC::_shrink_factor = 0;
 214 
 215 // VM_CollectForMetadataAllocation is the vm operation used to GC.
 216 // Within the VM operation after the GC the attempt to allocate the metadata
 217 // should succeed.  If the GC did not free enough space for the metaspace
 218 // allocation, the HWM is increased so that another virtualspace will be
 219 // allocated for the metadata.  With perm gen the increase in the perm
 220 // gen had bounds, MinMetaspaceExpansion and MaxMetaspaceExpansion.  The
 221 // metaspace policy uses those as the small and large steps for the HWM.
 222 //
 223 // After the GC the compute_new_size() for MetaspaceGC is called to
 224 // resize the capacity of the metaspaces.  The current implementation
 225 // is based on the flags MinMetaspaceFreeRatio and MaxMetaspaceFreeRatio used
 226 // to resize the Java heap by some GC's.  New flags can be implemented
 227 // if really needed.  MinMetaspaceFreeRatio is used to calculate how much
 228 // free space is desirable in the metaspace capacity to decide how much
 229 // to increase the HWM.  MaxMetaspaceFreeRatio is used to decide how much
 230 // free space is desirable in the metaspace capacity before decreasing
 231 // the HWM.
 232 
 233 // Calculate the amount to increase the high water mark (HWM).
 234 // Increase by a minimum amount (MinMetaspaceExpansion) so that
 235 // another expansion is not requested too soon.  If that is not
 236 // enough to satisfy the allocation, increase by MaxMetaspaceExpansion.
 237 // If that is still not enough, expand by the size of the allocation
 238 // plus some.
 239 size_t MetaspaceGC::delta_capacity_until_GC(size_t bytes) {
 240   size_t min_delta = MinMetaspaceExpansion;
 241   size_t max_delta = MaxMetaspaceExpansion;
 242   size_t delta = align_up(bytes, Metaspace::commit_alignment());
 243 
 244   if (delta &lt;= min_delta) {
 245     delta = min_delta;
 246   } else if (delta &lt;= max_delta) {
 247     // Don't want to hit the high water mark on the next
 248     // allocation so make the delta greater than just enough
 249     // for this allocation.
 250     delta = max_delta;
 251   } else {
 252     // This allocation is large but the next ones are probably not
 253     // so increase by the minimum.
 254     delta = delta + min_delta;
 255   }
 256 
 257   assert_is_aligned(delta, Metaspace::commit_alignment());
 258 
 259   return delta;
 260 }
 261 
 262 size_t MetaspaceGC::capacity_until_GC() {
 263   size_t value = Atomic::load_acquire(&amp;_capacity_until_GC);
 264   assert(value &gt;= MetaspaceSize, "Not initialized properly?");
 265   return value;
 266 }
 267 
 268 // Try to increase the _capacity_until_GC limit counter by v bytes.
 269 // Returns true if it succeeded. It may fail if either another thread
 270 // concurrently increased the limit or the new limit would be larger
 271 // than MaxMetaspaceSize.
 272 // On success, optionally returns new and old metaspace capacity in
 273 // new_cap_until_GC and old_cap_until_GC respectively.
 274 // On error, optionally sets can_retry to indicate whether if there is
 275 // actually enough space remaining to satisfy the request.
 276 bool MetaspaceGC::inc_capacity_until_GC(size_t v, size_t* new_cap_until_GC, size_t* old_cap_until_GC, bool* can_retry) {
 277   assert_is_aligned(v, Metaspace::commit_alignment());
 278 
 279   size_t old_capacity_until_GC = _capacity_until_GC;
 280   size_t new_value = old_capacity_until_GC + v;
 281 
 282   if (new_value &lt; old_capacity_until_GC) {
 283     // The addition wrapped around, set new_value to aligned max value.
 284     new_value = align_down(max_uintx, Metaspace::commit_alignment());
 285   }
 286 
 287   if (new_value &gt; MaxMetaspaceSize) {
 288     if (can_retry != NULL) {
 289       *can_retry = false;
 290     }
 291     return false;
 292   }
 293 
 294   if (can_retry != NULL) {
 295     *can_retry = true;
 296   }
 297   size_t prev_value = Atomic::cmpxchg(&amp;_capacity_until_GC, old_capacity_until_GC, new_value);
 298 
 299   if (old_capacity_until_GC != prev_value) {
 300     return false;
 301   }
 302 
 303   if (new_cap_until_GC != NULL) {
 304     *new_cap_until_GC = new_value;
 305   }
 306   if (old_cap_until_GC != NULL) {
 307     *old_cap_until_GC = old_capacity_until_GC;
 308   }
 309   return true;
 310 }
 311 
 312 size_t MetaspaceGC::dec_capacity_until_GC(size_t v) {
 313   assert_is_aligned(v, Metaspace::commit_alignment());
 314 
 315   return Atomic::sub(&amp;_capacity_until_GC, v);
 316 }
 317 
 318 void MetaspaceGC::initialize() {
 319   // Set the high-water mark to MaxMetapaceSize during VM initializaton since
 320   // we can't do a GC during initialization.
 321   _capacity_until_GC = MaxMetaspaceSize;
 322 }
 323 
 324 void MetaspaceGC::post_initialize() {
 325   // Reset the high-water mark once the VM initialization is done.
 326   _capacity_until_GC = MAX2(MetaspaceUtils::committed_bytes(), MetaspaceSize);
 327 }
 328 
 329 bool MetaspaceGC::can_expand(size_t word_size, bool is_class) {
 330   // Check if the compressed class space is full.
 331   if (is_class &amp;&amp; Metaspace::using_class_space()) {
 332     size_t class_committed = MetaspaceUtils::committed_bytes(Metaspace::ClassType);
 333     if (class_committed + word_size * BytesPerWord &gt; CompressedClassSpaceSize) {
 334       log_trace(gc, metaspace, freelist)("Cannot expand %s metaspace by " SIZE_FORMAT " words (CompressedClassSpaceSize = " SIZE_FORMAT " words)",
 335                 (is_class ? "class" : "non-class"), word_size, CompressedClassSpaceSize / sizeof(MetaWord));
 336       return false;
 337     }
 338   }
 339 
 340   // Check if the user has imposed a limit on the metaspace memory.
 341   size_t committed_bytes = MetaspaceUtils::committed_bytes();
 342   if (committed_bytes + word_size * BytesPerWord &gt; MaxMetaspaceSize) {
 343     log_trace(gc, metaspace, freelist)("Cannot expand %s metaspace by " SIZE_FORMAT " words (MaxMetaspaceSize = " SIZE_FORMAT " words)",
 344               (is_class ? "class" : "non-class"), word_size, MaxMetaspaceSize / sizeof(MetaWord));
 345     return false;
 346   }
 347 
 348   return true;
 349 }
 350 
 351 size_t MetaspaceGC::allowed_expansion() {
 352   size_t committed_bytes = MetaspaceUtils::committed_bytes();
 353   size_t capacity_until_gc = capacity_until_GC();
 354 
 355   assert(capacity_until_gc &gt;= committed_bytes,
 356          "capacity_until_gc: " SIZE_FORMAT " &lt; committed_bytes: " SIZE_FORMAT,
 357          capacity_until_gc, committed_bytes);
 358 
 359   size_t left_until_max  = MaxMetaspaceSize - committed_bytes;
 360   size_t left_until_GC = capacity_until_gc - committed_bytes;
 361   size_t left_to_commit = MIN2(left_until_GC, left_until_max);
 362   log_trace(gc, metaspace, freelist)("allowed expansion words: " SIZE_FORMAT
 363             " (left_until_max: " SIZE_FORMAT ", left_until_GC: " SIZE_FORMAT ".",
 364             left_to_commit / BytesPerWord, left_until_max / BytesPerWord, left_until_GC / BytesPerWord);
 365 
 366   return left_to_commit / BytesPerWord;
 367 }
 368 
 369 void MetaspaceGC::compute_new_size() {
 370   assert(_shrink_factor &lt;= 100, "invalid shrink factor");
 371   uint current_shrink_factor = _shrink_factor;
 372   _shrink_factor = 0;
 373 
 374   // Using committed_bytes() for used_after_gc is an overestimation, since the
 375   // chunk free lists are included in committed_bytes() and the memory in an
 376   // un-fragmented chunk free list is available for future allocations.
 377   // However, if the chunk free lists becomes fragmented, then the memory may
 378   // not be available for future allocations and the memory is therefore "in use".
 379   // Including the chunk free lists in the definition of "in use" is therefore
 380   // necessary. Not including the chunk free lists can cause capacity_until_GC to
 381   // shrink below committed_bytes() and this has caused serious bugs in the past.
 382   const size_t used_after_gc = MetaspaceUtils::committed_bytes();
 383   const size_t capacity_until_GC = MetaspaceGC::capacity_until_GC();
 384 
 385   const double minimum_free_percentage = MinMetaspaceFreeRatio / 100.0;
 386   const double maximum_used_percentage = 1.0 - minimum_free_percentage;
 387 
 388   const double min_tmp = used_after_gc / maximum_used_percentage;
 389   size_t minimum_desired_capacity =
 390     (size_t)MIN2(min_tmp, double(MaxMetaspaceSize));
 391   // Don't shrink less than the initial generation size
 392   minimum_desired_capacity = MAX2(minimum_desired_capacity,
 393                                   MetaspaceSize);
 394 
 395   log_trace(gc, metaspace)("MetaspaceGC::compute_new_size: ");
 396   log_trace(gc, metaspace)("    minimum_free_percentage: %6.2f  maximum_used_percentage: %6.2f",
 397                            minimum_free_percentage, maximum_used_percentage);
 398   log_trace(gc, metaspace)("     used_after_gc       : %6.1fKB", used_after_gc / (double) K);
 399 
 400 
 401   size_t shrink_bytes = 0;
 402   if (capacity_until_GC &lt; minimum_desired_capacity) {
 403     // If we have less capacity below the metaspace HWM, then
 404     // increment the HWM.
 405     size_t expand_bytes = minimum_desired_capacity - capacity_until_GC;
 406     expand_bytes = align_up(expand_bytes, Metaspace::commit_alignment());
 407     // Don't expand unless it's significant
 408     if (expand_bytes &gt;= MinMetaspaceExpansion) {
 409       size_t new_capacity_until_GC = 0;
 410       bool succeeded = MetaspaceGC::inc_capacity_until_GC(expand_bytes, &amp;new_capacity_until_GC);
 411       assert(succeeded, "Should always succesfully increment HWM when at safepoint");
 412 
 413       Metaspace::tracer()-&gt;report_gc_threshold(capacity_until_GC,
 414                                                new_capacity_until_GC,
 415                                                MetaspaceGCThresholdUpdater::ComputeNewSize);
 416       log_trace(gc, metaspace)("    expanding:  minimum_desired_capacity: %6.1fKB  expand_bytes: %6.1fKB  MinMetaspaceExpansion: %6.1fKB  new metaspace HWM:  %6.1fKB",
 417                                minimum_desired_capacity / (double) K,
 418                                expand_bytes / (double) K,
 419                                MinMetaspaceExpansion / (double) K,
 420                                new_capacity_until_GC / (double) K);
 421     }
 422     return;
 423   }
 424 
 425   // No expansion, now see if we want to shrink
 426   // We would never want to shrink more than this
 427   assert(capacity_until_GC &gt;= minimum_desired_capacity,
 428          SIZE_FORMAT " &gt;= " SIZE_FORMAT,
 429          capacity_until_GC, minimum_desired_capacity);
 430   size_t max_shrink_bytes = capacity_until_GC - minimum_desired_capacity;
 431 
 432   // Should shrinking be considered?
 433   if (MaxMetaspaceFreeRatio &lt; 100) {
 434     const double maximum_free_percentage = MaxMetaspaceFreeRatio / 100.0;
 435     const double minimum_used_percentage = 1.0 - maximum_free_percentage;
 436     const double max_tmp = used_after_gc / minimum_used_percentage;
 437     size_t maximum_desired_capacity = (size_t)MIN2(max_tmp, double(MaxMetaspaceSize));
 438     maximum_desired_capacity = MAX2(maximum_desired_capacity,
 439                                     MetaspaceSize);
 440     log_trace(gc, metaspace)("    maximum_free_percentage: %6.2f  minimum_used_percentage: %6.2f",
 441                              maximum_free_percentage, minimum_used_percentage);
 442     log_trace(gc, metaspace)("    minimum_desired_capacity: %6.1fKB  maximum_desired_capacity: %6.1fKB",
 443                              minimum_desired_capacity / (double) K, maximum_desired_capacity / (double) K);
 444 
 445     assert(minimum_desired_capacity &lt;= maximum_desired_capacity,
 446            "sanity check");
 447 
 448     if (capacity_until_GC &gt; maximum_desired_capacity) {
 449       // Capacity too large, compute shrinking size
 450       shrink_bytes = capacity_until_GC - maximum_desired_capacity;
 451       // We don't want shrink all the way back to initSize if people call
 452       // System.gc(), because some programs do that between "phases" and then
 453       // we'd just have to grow the heap up again for the next phase.  So we
 454       // damp the shrinking: 0% on the first call, 10% on the second call, 40%
 455       // on the third call, and 100% by the fourth call.  But if we recompute
 456       // size without shrinking, it goes back to 0%.
 457       shrink_bytes = shrink_bytes / 100 * current_shrink_factor;
 458 
 459       shrink_bytes = align_down(shrink_bytes, Metaspace::commit_alignment());
 460 
 461       assert(shrink_bytes &lt;= max_shrink_bytes,
 462              "invalid shrink size " SIZE_FORMAT " not &lt;= " SIZE_FORMAT,
 463              shrink_bytes, max_shrink_bytes);
 464       if (current_shrink_factor == 0) {
 465         _shrink_factor = 10;
 466       } else {
 467         _shrink_factor = MIN2(current_shrink_factor * 4, (uint) 100);
 468       }
 469       log_trace(gc, metaspace)("    shrinking:  initThreshold: %.1fK  maximum_desired_capacity: %.1fK",
 470                                MetaspaceSize / (double) K, maximum_desired_capacity / (double) K);
 471       log_trace(gc, metaspace)("    shrink_bytes: %.1fK  current_shrink_factor: %d  new shrink factor: %d  MinMetaspaceExpansion: %.1fK",
 472                                shrink_bytes / (double) K, current_shrink_factor, _shrink_factor, MinMetaspaceExpansion / (double) K);
 473     }
 474   }
 475 
 476   // Don't shrink unless it's significant
 477   if (shrink_bytes &gt;= MinMetaspaceExpansion &amp;&amp;
 478       ((capacity_until_GC - shrink_bytes) &gt;= MetaspaceSize)) {
 479     size_t new_capacity_until_GC = MetaspaceGC::dec_capacity_until_GC(shrink_bytes);
 480     Metaspace::tracer()-&gt;report_gc_threshold(capacity_until_GC,
 481                                              new_capacity_until_GC,
 482                                              MetaspaceGCThresholdUpdater::ComputeNewSize);
 483   }
 484 }
 485 
 486 
 487 
 488 //////  Metaspace methods /////
 489 
 490 
 491 
 492 MetaWord* Metaspace::_compressed_class_space_base = NULL;
 493 size_t Metaspace::_compressed_class_space_size = 0;
 494 const MetaspaceTracer* Metaspace::_tracer = NULL;
 495 bool Metaspace::_initialized = false;
 496 size_t Metaspace::_commit_alignment = 0;
 497 size_t Metaspace::_reserve_alignment = 0;
 498 
 499 DEBUG_ONLY(bool Metaspace::_frozen = false;)
 500 
 501 
 502 #ifdef _LP64
 503 
 504 void Metaspace::print_compressed_class_space(outputStream* st) {
 505   if (VirtualSpaceList::vslist_class() != NULL) {
 506     MetaWord* base = VirtualSpaceList::vslist_class()-&gt;base_of_first_node();
 507     size_t size = VirtualSpaceList::vslist_class()-&gt;word_size_of_first_node();
 508     MetaWord* top = base + size;
 509     st-&gt;print("Compressed class space mapped at: " PTR_FORMAT "-" PTR_FORMAT ", reserved size: " SIZE_FORMAT,
 510                p2i(base), p2i(top), (top - base) * BytesPerWord);
 511     st-&gt;cr();
 512   }
 513 }
 514 
 515 // Given a prereserved space, use that to set up the compressed class space list.
 516 void Metaspace::initialize_class_space(ReservedSpace rs) {
 517   assert(rs.size() &gt;= CompressedClassSpaceSize,
 518          SIZE_FORMAT " != " SIZE_FORMAT, rs.size(), CompressedClassSpaceSize);
 519   assert(using_class_space(), "Must be using class space");
 520 
 521   assert(rs.size() == CompressedClassSpaceSize, SIZE_FORMAT " != " SIZE_FORMAT,
 522          rs.size(), CompressedClassSpaceSize);
 523   assert(is_aligned(rs.base(), Metaspace::reserve_alignment()) &amp;&amp;
 524          is_aligned(rs.size(), Metaspace::reserve_alignment()),
 525          "wrong alignment");
 526 
 527   VirtualSpaceList* vsl = new VirtualSpaceList("class space list", rs, CommitLimiter::globalLimiter());
 528   VirtualSpaceList::set_vslist_class(vsl);
 529   ChunkManager* cm = new ChunkManager("class space chunk manager", vsl);
 530   ChunkManager::set_chunkmanager_class(cm);
 531 
 532   // This does currently not work because rs may be the result of a split
 533   // operation and NMT seems not to be able to handle splits.
 534   // Will be fixed with JDK-8243535.
 535   // MemTracker::record_virtual_memory_type((address)rs.base(), mtClass);
 536 
 537 }
 538 
 539 // Returns true if class space has been setup (initialize_class_space).
 540 bool Metaspace::class_space_is_initialized() {
 541   return VirtualSpaceList::vslist_class() != NULL;
 542 }
 543 
 544 // Reserve a range of memory at an address suitable for en/decoding narrow
 545 // Klass pointers (see: CompressedClassPointers::is_valid_base()).
 546 // The returned address shall both be suitable as a compressed class pointers
 547 //  base, and aligned to Metaspace::reserve_alignment (which is equal to or a
 548 //  multiple of allocation granularity).
 549 // On error, returns an unreserved space.
 550 ReservedSpace Metaspace::reserve_address_space_for_compressed_classes(size_t size) {
 551 
 552 #ifdef AARCH64
 553   const size_t alignment = Metaspace::reserve_alignment();
 554 
 555   // AArch64: Try to align metaspace so that we can decode a compressed
 556   // klass with a single MOVK instruction. We can do this iff the
 557   // compressed class base is a multiple of 4G.
 558   // Additionally, above 32G, ensure the lower LogKlassAlignmentInBytes bits
 559   // of the upper 32-bits of the address are zero so we can handle a shift
 560   // when decoding.
 561 
 562   static const struct {
 563     address from;
 564     address to;
 565     size_t increment;
 566   } search_ranges[] = {
 567     {  (address)(4*G),   (address)(32*G),   4*G, },
 568     {  (address)(32*G),  (address)(1024*G), (4 &lt;&lt; LogKlassAlignmentInBytes) * G },
 569     {  NULL, NULL, 0 }
 570   };
 571 
 572   for (int i = 0; search_ranges[i].from != NULL; i ++) {
 573     address a = search_ranges[i].from;
 574     assert(CompressedKlassPointers::is_valid_base(a), "Sanity");
 575     while (a &lt; search_ranges[i].to) {
 576       ReservedSpace rs(size, Metaspace::reserve_alignment(),
 577                        false /*large_pages*/, (char*)a);
 578       if (rs.is_reserved()) {
 579         assert(a == (address)rs.base(), "Sanity");
 580         return rs;
 581       }
 582       a +=  search_ranges[i].increment;
 583     }
 584   }
 585 
 586   // Note: on AARCH64, if the code above does not find any good placement, we
 587   // have no recourse. We return an empty space and the VM will exit.
 588   return ReservedSpace();
 589 #else
 590   // Default implementation: Just reserve anywhere.
 591   return ReservedSpace(size, Metaspace::reserve_alignment(), false, (char*)NULL);
 592 #endif // AARCH64
 593 }
 594 
 595 #endif // _LP64
 596 
 597 
 598 void Metaspace::ergo_initialize() {
 599 
 600   // Must happen before using any setting from Settings::---
 601   metaspace::Settings::ergo_initialize();
 602 
 603   // Commit alignment: (I would rather hide this since this is an implementation detail but we need it
 604   // when calculating the gc threshold).
 605   _commit_alignment  = metaspace::Settings::commit_granule_bytes();
 606 
 607   // Reserve alignment: all Metaspace memory mappings are to be aligned to the size of a root chunk.
 608   _reserve_alignment = MAX2((size_t)os::vm_allocation_granularity(), metaspace::chunklevel::MAX_CHUNK_BYTE_SIZE);
 609 
 610 
 611   // MaxMetaspaceSize and CompressedClassSpaceSize:
 612   //
 613   // MaxMetaspaceSize is the maximum size, in bytes, of memory we are allowed
 614   //  to commit for the Metaspace.
 615   //  It is just a number; a limit we compare against before committing. It
 616   //  does not have to be aligned to anything.
 617   //  It gets used as compare value in class CommitLimiter.
 618   //  It is set to max_uintx in globals.hpp by default, so by default it does
 619   //  not limit anything.
 620   //
 621   // CompressedClassSpaceSize is the size, in bytes, of the address range we
 622   //  pre-reserve for the compressed class space (if we use class space).
 623   //  This size has to be aligned to the metaspace reserve alignment (to the
 624   //  size of a root chunk). It gets aligned up from whatever value the caller
 625   //  gave us to the next multiple of root chunk size.
 626   //
 627   // Note: Strictly speaking MaxMetaspaceSize and CompressedClassSpaceSize have
 628   //  very little to do with each other. The notion often encountered:
 629   //  MaxMetaspaceSize = CompressedClassSpaceSize + &lt;non-class metadata size&gt;
 630   //  is subtly wrong: MaxMetaspaceSize can besmaller than CompressedClassSpaceSize,
 631   //  in which case we just would not be able to fully commit the class space range.
 632   //
 633   // We still adjust CompressedClassSpaceSize to reasonable limits, mainly to
 634   //  save on reserved space, and to make ergnonomics less confusing.
 635 
 636   // (aligned just for cleanliness:)
 637   MaxMetaspaceSize = MAX2(align_down(MaxMetaspaceSize, _commit_alignment), _commit_alignment);
 638 
 639   if (UseCompressedClassPointers) {
 640     // Let CCS size not be larger than 80% of MaxMetaspaceSize. Note that is
 641     // grossly over-dimensioned for most usage scenarios; typical ratio of
 642     // class space : non class space usage is about 1:6. With many small classes,
 643     // it can get as low as 1:2. It is not a big deal though since ccs is only
 644     // reserved and will be committed on demand only.
 645     size_t max_ccs_size = MaxMetaspaceSize * 0.8;
 646     size_t adjusted_ccs_size = MIN2(CompressedClassSpaceSize, max_ccs_size);
 647 
 648     // CCS must be aligned to root chunk size, and be at least the size of one
 649     //  root chunk.
 650     adjusted_ccs_size = align_up(adjusted_ccs_size, _reserve_alignment);
 651     adjusted_ccs_size = MAX2(adjusted_ccs_size, _reserve_alignment);
 652 
 653     // Note: re-adjusting may have us left with a CompressedClassSpaceSize
 654     //  larger than MaxMetaspaceSize for very small values of MaxMetaspaceSize.
 655     //  Lets just live with that, its not a big deal.
 656 
 657     if (adjusted_ccs_size != CompressedClassSpaceSize) {
 658       FLAG_SET_ERGO(CompressedClassSpaceSize, adjusted_ccs_size);
 659       log_info(metaspace)("Setting CompressedClassSpaceSize to " SIZE_FORMAT ".",
 660                           CompressedClassSpaceSize);
 661     }
 662   }
 663 
 664   // Set MetaspaceSize, MinMetaspaceExpansion and MaxMetaspaceExpansion
 665   if (MetaspaceSize &gt; MaxMetaspaceSize) {
 666     MetaspaceSize = MaxMetaspaceSize;
 667   }
 668 
 669   MetaspaceSize = align_down_bounded(MetaspaceSize, _commit_alignment);
 670 
 671   assert(MetaspaceSize &lt;= MaxMetaspaceSize, "MetaspaceSize should be limited by MaxMetaspaceSize");
 672 
 673   MinMetaspaceExpansion = align_down_bounded(MinMetaspaceExpansion, _commit_alignment);
 674   MaxMetaspaceExpansion = align_down_bounded(MaxMetaspaceExpansion, _commit_alignment);
 675 
 676   _compressed_class_space_size = CompressedClassSpaceSize;
 677 
 678 }
 679 
 680 void Metaspace::global_initialize() {
 681   MetaspaceGC::initialize(); // &lt;- since we do not prealloc init chunks anymore is this still needed?
 682 
 683   // If UseCompressedClassPointers=1, we have two cases:
 684   // a) if CDS is active (either dump time or runtime), it will create the ccs
 685   //    for us, initialize it and set up CompressedKlassPointers encoding.
 686   //    Class space will be reserved above the mapped archives.
 687   // b) if CDS is not active, we will create the ccs on our own. It will be
 688   //    placed above the java heap, since we assume it has been placed in low
 689   //    address regions. We may rethink this (see JDK-8244943). Failing that,
 690   //    it will be placed anywhere.
 691 
 692 #if INCLUDE_CDS
 693   // case (a)
 694   if (DumpSharedSpaces) {
 695     MetaspaceShared::initialize_dumptime_shared_and_meta_spaces();
 696   } else if (UseSharedSpaces) {
 697     // If any of the archived space fails to map, UseSharedSpaces
 698     // is reset to false.
 699     MetaspaceShared::initialize_runtime_shared_and_meta_spaces();
 700   }
 701 
 702   if (DynamicDumpSharedSpaces &amp;&amp; !UseSharedSpaces) {
 703     vm_exit_during_initialization("DynamicDumpSharedSpaces is unsupported when base CDS archive is not loaded", NULL);
 704   }
 705 #endif // INCLUDE_CDS
 706 
 707 #ifdef _LP64
 708 
 709   if (using_class_space() &amp;&amp; !class_space_is_initialized()) {
 710     assert(!UseSharedSpaces &amp;&amp; !DumpSharedSpaces, "CDS should be off at this point");
 711 
 712     // case (b)
 713     ReservedSpace rs;
 714 
 715     // If UseCompressedOops=1, java heap may have been placed in coops-friendly
 716     //  territory already (lower address regions), so we attempt to place ccs
 717     //  right above the java heap.
 718     // If UseCompressedOops=0, the heap has been placed anywhere - probably in
 719     //  high memory regions. In that case, try to place ccs at the lowest allowed
 720     //  mapping address.
 721     address base = UseCompressedOops ? CompressedOops::end() : (address)HeapBaseMinAddress;
 722     base = align_up(base, Metaspace::reserve_alignment());
 723 
 724     const size_t size = align_up(CompressedClassSpaceSize, Metaspace::reserve_alignment());
 725     if (base != NULL) {
 726       if (CompressedKlassPointers::is_valid_base(base)) {
 727         rs = ReservedSpace(size, Metaspace::reserve_alignment(),
 728                            false /* large */, (char*)base);
 729       }
 730     }
 731 
 732     // ...failing that, reserve anywhere, but let platform do optimized placement:
 733     if (!rs.is_reserved()) {
 734       rs = Metaspace::reserve_address_space_for_compressed_classes(size);
 735     }
 736 
 737     // ...failing that, give up.
 738     if (!rs.is_reserved()) {
 739       vm_exit_during_initialization(
 740           err_msg("Could not allocate compressed class space: " SIZE_FORMAT " bytes",
 741                    compressed_class_space_size()));
 742     }
 743 
 744     // Initialize space
 745     Metaspace::initialize_class_space(rs);
 746 
 747     // Set up compressed class pointer encoding.
 748     CompressedKlassPointers::initialize((address)rs.base(), rs.size());
 749   }
 750 
 751 #endif
 752 
 753   // Initialize non-class virtual space list, and its chunk manager:
 754   VirtualSpaceList* vsl = new VirtualSpaceList("non-class virtualspacelist", CommitLimiter::globalLimiter());
 755   VirtualSpaceList::set_vslist_nonclass(vsl);
 756   ChunkManager* cm = new ChunkManager("non-class chunkmanager", vsl);
 757   ChunkManager::set_chunkmanager_nonclass(cm);
 758 
 759   _tracer = new MetaspaceTracer();
 760 
 761   _initialized = true;
 762 
 763   // We must prevent the very first address of the ccs from being used to store
 764   // metadata, since that address would translate to a narrow pointer of 0, and the
 765   // VM does not distinguish between "narrow 0 as in NULL" and "narrow 0 as in start
 766   //  of ccs".
 767   // Before Elastic Metaspace that did not happen due to the fact that every Metachunk
 768   // had a header and therefore could not allocate anything at offset 0.
 769 #ifdef _LP64
 770   if (using_class_space()) {
 771     // The simplest way to fix this is to allocate a tiny chunk right at the start of ccs
 772     // and do not use it for anything.
 773     ChunkManager::chunkmanager_class()-&gt;get_chunk(metaspace::chunklevel::HIGHEST_CHUNK_LEVEL);
 774   }
 775 #endif
 776 
 777 #ifdef _LP64
 778   if (UseCompressedClassPointers) {
 779     // Note: "cds" would be a better fit but keep this for backward compatibility.
 780     LogTarget(Info, gc, metaspace) lt;
 781     if (lt.is_enabled()) {
 782       ResourceMark rm;
 783       LogStream ls(lt);
 784       CDS_ONLY(MetaspaceShared::print_on(&amp;ls);)
 785       Metaspace::print_compressed_class_space(&amp;ls);
 786       CompressedKlassPointers::print_mode(&amp;ls);
 787     }
 788   }
 789 #endif
 790 
 791 }
 792 
 793 void Metaspace::post_initialize() {
 794   MetaspaceGC::post_initialize();
 795 }
 796 
 797 size_t Metaspace::max_allocation_word_size() {
 798   return metaspace::chunklevel::MAX_CHUNK_WORD_SIZE;
 799 }
 800 
 801 MetaWord* Metaspace::allocate(ClassLoaderData* loader_data, size_t word_size,
 802                               MetaspaceObj::Type type, TRAPS) {
 803   assert(word_size &lt;= Metaspace::max_allocation_word_size(),
 804          "allocation size too large (" SIZE_FORMAT ")", word_size);
 805   assert(!_frozen, "sanity");
 806   assert(!(DumpSharedSpaces &amp;&amp; THREAD-&gt;is_VM_thread()), "sanity");
 807 
 808   if (HAS_PENDING_EXCEPTION) {
 809     assert(false, "Should not allocate with exception pending");
 810     return NULL;  // caller does a CHECK_NULL too
 811   }
 812 
 813   assert(loader_data != NULL, "Should never pass around a NULL loader_data. "
 814         "ClassLoaderData::the_null_class_loader_data() should have been used.");
 815 
 816   Metaspace::MetadataType mdtype = (type == MetaspaceObj::ClassType) ? Metaspace::ClassType : Metaspace::NonClassType;
 817 
 818   // Try to allocate metadata.
 819   MetaWord* result = loader_data-&gt;metaspace_non_null()-&gt;allocate(word_size, mdtype);
 820 
 821   if (result == NULL) {
 822     tracer()-&gt;report_metaspace_allocation_failure(loader_data, word_size, type, mdtype);
 823 
 824     // Allocation failed.
 825     if (is_init_completed()) {
 826       // Only start a GC if the bootstrapping has completed.
 827       // Try to clean out some heap memory and retry. This can prevent premature
 828       // expansion of the metaspace.
 829       result = Universe::heap()-&gt;satisfy_failed_metadata_allocation(loader_data, word_size, mdtype);
 830     }
 831   }
 832 
 833   if (result == NULL) {
 834     if (DumpSharedSpaces) {
 835       // CDS dumping keeps loading classes, so if we hit an OOM we probably will keep hitting OOM.
 836       // We should abort to avoid generating a potentially bad archive.
 837       vm_exit_during_cds_dumping(err_msg("Failed allocating metaspace object type %s of size " SIZE_FORMAT ". CDS dump aborted.",
 838           MetaspaceObj::type_name(type), word_size * BytesPerWord),
 839         err_msg("Please increase MaxMetaspaceSize (currently " SIZE_FORMAT " bytes).", MaxMetaspaceSize));
 840     }
 841     report_metadata_oome(loader_data, word_size, type, mdtype, THREAD);
 842     assert(HAS_PENDING_EXCEPTION, "sanity");
 843     return NULL;
 844   }
 845 
 846   // Zero initialize.
 847   Copy::fill_to_words((HeapWord*)result, word_size, 0);
 848 
 849   log_trace(metaspace)("Metaspace::allocate: type %d return " PTR_FORMAT ".", (int)type, p2i(result));
 850 
 851   return result;
 852 }
 853 
 854 void Metaspace::report_metadata_oome(ClassLoaderData* loader_data, size_t word_size, MetaspaceObj::Type type, MetadataType mdtype, TRAPS) {
 855   tracer()-&gt;report_metadata_oom(loader_data, word_size, type, mdtype);
 856 
 857   // If result is still null, we are out of memory.
 858   Log(gc, metaspace, freelist, oom) log;
 859   if (log.is_info()) {
 860     log.info("Metaspace (%s) allocation failed for size " SIZE_FORMAT,
 861              metaspace::is_class(mdtype) ? "class" : "data", word_size);
 862     ResourceMark rm;
 863     if (log.is_debug()) {
 864       if (loader_data-&gt;metaspace_or_null() != NULL) {
 865         LogStream ls(log.debug());
 866         loader_data-&gt;print_value_on(&amp;ls);
 867       }
 868     }
 869     LogStream ls(log.info());
 870     // In case of an OOM, log out a short but still useful report.
 871     MetaspaceUtils::print_basic_report(&amp;ls, 0);
 872   }
 873 
 874   // Which limit did we hit? CompressedClassSpaceSize or MaxMetaspaceSize?
 875   bool out_of_compressed_class_space = false;
 876   if (metaspace::is_class(mdtype)) {
 877     ClassLoaderMetaspace* metaspace = loader_data-&gt;metaspace_non_null();
 878     out_of_compressed_class_space =
 879       MetaspaceUtils::committed_bytes(Metaspace::ClassType) +
 880       // TODO: Okay this is just cheesy.
 881       // Of course this may fail and return incorrect results.
 882       // Think this over - we need some clean way to remember which limit
 883       // exactly we hit during an allocation. Some sort of allocation context structure?
 884       align_up(word_size * BytesPerWord, 4 * M) &gt;
 885       CompressedClassSpaceSize;
 886   }
 887 
 888   // -XX:+HeapDumpOnOutOfMemoryError and -XX:OnOutOfMemoryError support
 889   const char* space_string = out_of_compressed_class_space ?
 890     "Compressed class space" : "Metaspace";
 891 
 892   report_java_out_of_memory(space_string);
 893 
 894   if (JvmtiExport::should_post_resource_exhausted()) {
 895     JvmtiExport::post_resource_exhausted(
 896         JVMTI_RESOURCE_EXHAUSTED_OOM_ERROR,
 897         space_string);
 898   }
 899 
 900   if (!is_init_completed()) {
 901     vm_exit_during_initialization("OutOfMemoryError", space_string);
 902   }
 903 
 904   if (out_of_compressed_class_space) {
 905     THROW_OOP(Universe::out_of_memory_error_class_metaspace());
 906   } else {
 907     THROW_OOP(Universe::out_of_memory_error_metaspace());
 908   }
 909 }
 910 
 911 void Metaspace::purge() {
 912   ChunkManager* cm = ChunkManager::chunkmanager_nonclass();
 913   if (cm != NULL) {
 914     cm-&gt;wholesale_reclaim();
 915   }
 916   if (using_class_space()) {
 917     cm = ChunkManager::chunkmanager_class();
 918     if (cm != NULL) {
 919       cm-&gt;wholesale_reclaim();
 920     }
 921   }
 922 }
 923 
 924 bool Metaspace::contains(const void* ptr) {
 925   if (MetaspaceShared::is_in_shared_metaspace(ptr)) {
 926     return true;
 927   }
 928   return contains_non_shared(ptr);
 929 }
 930 
 931 bool Metaspace::contains_non_shared(const void* ptr) {
 932   if (using_class_space() &amp;&amp; VirtualSpaceList::vslist_class()-&gt;contains((MetaWord*)ptr)) {
 933      return true;
 934   }
 935 
 936   return VirtualSpaceList::vslist_nonclass()-&gt;contains((MetaWord*)ptr);
 937 }
</pre></body></html>
