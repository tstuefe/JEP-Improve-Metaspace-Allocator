<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>Old src/hotspot/share/memory/metaspace.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 2011, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "aot/aotLoader.hpp"
  27 #include "classfile/classLoaderDataGraph.hpp"
  28 #include "gc/shared/collectedHeap.hpp"
  29 #include "logging/log.hpp"
  30 #include "logging/logStream.hpp"
  31 #include "memory/filemap.hpp"
  32 #include "memory/metaspace.hpp"
  33 #include "memory/metaspace/chunkManager.hpp"
  34 #include "memory/metaspace/metachunk.hpp"
  35 #include "memory/metaspace/metaspaceCommon.hpp"
  36 #include "memory/metaspace/printCLDMetaspaceInfoClosure.hpp"
  37 #include "memory/metaspace/spaceManager.hpp"
  38 #include "memory/metaspace/virtualSpaceList.hpp"
  39 #include "memory/metaspaceShared.hpp"
  40 #include "memory/metaspaceTracer.hpp"
  41 #include "memory/universe.hpp"
  42 #include "oops/compressedOops.hpp"
  43 #include "runtime/atomic.hpp"
  44 #include "runtime/init.hpp"
  45 #include "services/memTracker.hpp"
  46 #include "utilities/copy.hpp"
  47 #include "utilities/debug.hpp"
  48 #include "utilities/formatBuffer.hpp"
  49 #include "utilities/globalDefinitions.hpp"
  50 #include "utilities/vmError.hpp"
  51 
  52 
  53 using namespace metaspace;
  54 
  55 MetaWord* last_allocated = 0;
  56 
  57 size_t Metaspace::_compressed_class_space_size;
  58 const MetaspaceTracer* Metaspace::_tracer = NULL;
  59 
  60 DEBUG_ONLY(bool Metaspace::_frozen = false;)
  61 
  62 static const char* space_type_name(Metaspace::MetaspaceType t) {
  63   const char* s = NULL;
  64   switch (t) {
  65     case Metaspace::StandardMetaspaceType: s = "Standard"; break;
  66     case Metaspace::BootMetaspaceType: s = "Boot"; break;
  67     case Metaspace::ClassMirrorHolderMetaspaceType: s = "ClassMirrorHolder"; break;
  68     case Metaspace::ReflectionMetaspaceType: s = "Reflection"; break;
  69     default: ShouldNotReachHere();
  70   }
  71   return s;
  72 }
  73 
  74 volatile size_t MetaspaceGC::_capacity_until_GC = 0;
  75 uint MetaspaceGC::_shrink_factor = 0;
  76 
  77 // BlockFreelist methods
  78 
  79 // VirtualSpaceNode methods
  80 
  81 // MetaspaceGC methods
  82 
  83 // VM_CollectForMetadataAllocation is the vm operation used to GC.
  84 // Within the VM operation after the GC the attempt to allocate the metadata
  85 // should succeed.  If the GC did not free enough space for the metaspace
  86 // allocation, the HWM is increased so that another virtualspace will be
  87 // allocated for the metadata.  With perm gen the increase in the perm
  88 // gen had bounds, MinMetaspaceExpansion and MaxMetaspaceExpansion.  The
  89 // metaspace policy uses those as the small and large steps for the HWM.
  90 //
  91 // After the GC the compute_new_size() for MetaspaceGC is called to
  92 // resize the capacity of the metaspaces.  The current implementation
  93 // is based on the flags MinMetaspaceFreeRatio and MaxMetaspaceFreeRatio used
  94 // to resize the Java heap by some GC's.  New flags can be implemented
  95 // if really needed.  MinMetaspaceFreeRatio is used to calculate how much
  96 // free space is desirable in the metaspace capacity to decide how much
  97 // to increase the HWM.  MaxMetaspaceFreeRatio is used to decide how much
  98 // free space is desirable in the metaspace capacity before decreasing
  99 // the HWM.
 100 
 101 // Calculate the amount to increase the high water mark (HWM).
 102 // Increase by a minimum amount (MinMetaspaceExpansion) so that
 103 // another expansion is not requested too soon.  If that is not
 104 // enough to satisfy the allocation, increase by MaxMetaspaceExpansion.
 105 // If that is still not enough, expand by the size of the allocation
 106 // plus some.
 107 size_t MetaspaceGC::delta_capacity_until_GC(size_t bytes) {
 108   size_t min_delta = MinMetaspaceExpansion;
 109   size_t max_delta = MaxMetaspaceExpansion;
 110   size_t delta = align_up(bytes, Metaspace::commit_alignment());
 111 
 112   if (delta &lt;= min_delta) {
 113     delta = min_delta;
 114   } else if (delta &lt;= max_delta) {
 115     // Don't want to hit the high water mark on the next
 116     // allocation so make the delta greater than just enough
 117     // for this allocation.
 118     delta = max_delta;
 119   } else {
 120     // This allocation is large but the next ones are probably not
 121     // so increase by the minimum.
 122     delta = delta + min_delta;
 123   }
 124 
 125   assert_is_aligned(delta, Metaspace::commit_alignment());
 126 
 127   return delta;
 128 }
 129 
 130 size_t MetaspaceGC::capacity_until_GC() {
 131   size_t value = Atomic::load_acquire(&amp;_capacity_until_GC);
 132   assert(value &gt;= MetaspaceSize, "Not initialized properly?");
 133   return value;
 134 }
 135 
 136 // Try to increase the _capacity_until_GC limit counter by v bytes.
 137 // Returns true if it succeeded. It may fail if either another thread
 138 // concurrently increased the limit or the new limit would be larger
 139 // than MaxMetaspaceSize.
 140 // On success, optionally returns new and old metaspace capacity in
 141 // new_cap_until_GC and old_cap_until_GC respectively.
 142 // On error, optionally sets can_retry to indicate whether if there is
 143 // actually enough space remaining to satisfy the request.
 144 bool MetaspaceGC::inc_capacity_until_GC(size_t v, size_t* new_cap_until_GC, size_t* old_cap_until_GC, bool* can_retry) {
 145   assert_is_aligned(v, Metaspace::commit_alignment());
 146 
 147   size_t old_capacity_until_GC = _capacity_until_GC;
 148   size_t new_value = old_capacity_until_GC + v;
 149 
 150   if (new_value &lt; old_capacity_until_GC) {
 151     // The addition wrapped around, set new_value to aligned max value.
 152     new_value = align_down(max_uintx, Metaspace::commit_alignment());
 153   }
 154 
 155   if (new_value &gt; MaxMetaspaceSize) {
 156     if (can_retry != NULL) {
 157       *can_retry = false;
 158     }
 159     return false;
 160   }
 161 
 162   if (can_retry != NULL) {
 163     *can_retry = true;
 164   }
 165   size_t prev_value = Atomic::cmpxchg(&amp;_capacity_until_GC, old_capacity_until_GC, new_value);
 166 
 167   if (old_capacity_until_GC != prev_value) {
 168     return false;
 169   }
 170 
 171   if (new_cap_until_GC != NULL) {
 172     *new_cap_until_GC = new_value;
 173   }
 174   if (old_cap_until_GC != NULL) {
 175     *old_cap_until_GC = old_capacity_until_GC;
 176   }
 177   return true;
 178 }
 179 
 180 size_t MetaspaceGC::dec_capacity_until_GC(size_t v) {
 181   assert_is_aligned(v, Metaspace::commit_alignment());
 182 
 183   return Atomic::sub(&amp;_capacity_until_GC, v);
 184 }
 185 
 186 void MetaspaceGC::initialize() {
 187   // Set the high-water mark to MaxMetapaceSize during VM initializaton since
 188   // we can't do a GC during initialization.
 189   _capacity_until_GC = MaxMetaspaceSize;
 190 }
 191 
 192 void MetaspaceGC::post_initialize() {
 193   // Reset the high-water mark once the VM initialization is done.
 194   _capacity_until_GC = MAX2(MetaspaceUtils::committed_bytes(), MetaspaceSize);
 195 }
 196 
 197 bool MetaspaceGC::can_expand(size_t word_size, bool is_class) {
 198   // Check if the compressed class space is full.
 199   if (is_class &amp;&amp; Metaspace::using_class_space()) {
 200     size_t class_committed = MetaspaceUtils::committed_bytes(Metaspace::ClassType);
 201     if (class_committed + word_size * BytesPerWord &gt; CompressedClassSpaceSize) {
 202       log_trace(gc, metaspace, freelist)("Cannot expand %s metaspace by " SIZE_FORMAT " words (CompressedClassSpaceSize = " SIZE_FORMAT " words)",
 203                 (is_class ? "class" : "non-class"), word_size, CompressedClassSpaceSize / sizeof(MetaWord));
 204       return false;
 205     }
 206   }
 207 
 208   // Check if the user has imposed a limit on the metaspace memory.
 209   size_t committed_bytes = MetaspaceUtils::committed_bytes();
 210   if (committed_bytes + word_size * BytesPerWord &gt; MaxMetaspaceSize) {
 211     log_trace(gc, metaspace, freelist)("Cannot expand %s metaspace by " SIZE_FORMAT " words (MaxMetaspaceSize = " SIZE_FORMAT " words)",
 212               (is_class ? "class" : "non-class"), word_size, MaxMetaspaceSize / sizeof(MetaWord));
 213     return false;
 214   }
 215 
 216   return true;
 217 }
 218 
 219 size_t MetaspaceGC::allowed_expansion() {
 220   size_t committed_bytes = MetaspaceUtils::committed_bytes();
 221   size_t capacity_until_gc = capacity_until_GC();
 222 
 223   assert(capacity_until_gc &gt;= committed_bytes,
 224          "capacity_until_gc: " SIZE_FORMAT " &lt; committed_bytes: " SIZE_FORMAT,
 225          capacity_until_gc, committed_bytes);
 226 
 227   size_t left_until_max  = MaxMetaspaceSize - committed_bytes;
 228   size_t left_until_GC = capacity_until_gc - committed_bytes;
 229   size_t left_to_commit = MIN2(left_until_GC, left_until_max);
 230   log_trace(gc, metaspace, freelist)("allowed expansion words: " SIZE_FORMAT
 231             " (left_until_max: " SIZE_FORMAT ", left_until_GC: " SIZE_FORMAT ".",
 232             left_to_commit / BytesPerWord, left_until_max / BytesPerWord, left_until_GC / BytesPerWord);
 233 
 234   return left_to_commit / BytesPerWord;
 235 }
 236 
 237 void MetaspaceGC::compute_new_size() {
 238   assert(_shrink_factor &lt;= 100, "invalid shrink factor");
 239   uint current_shrink_factor = _shrink_factor;
 240   _shrink_factor = 0;
 241 
 242   // Using committed_bytes() for used_after_gc is an overestimation, since the
 243   // chunk free lists are included in committed_bytes() and the memory in an
 244   // un-fragmented chunk free list is available for future allocations.
 245   // However, if the chunk free lists becomes fragmented, then the memory may
 246   // not be available for future allocations and the memory is therefore "in use".
 247   // Including the chunk free lists in the definition of "in use" is therefore
 248   // necessary. Not including the chunk free lists can cause capacity_until_GC to
 249   // shrink below committed_bytes() and this has caused serious bugs in the past.
 250   const size_t used_after_gc = MetaspaceUtils::committed_bytes();
 251   const size_t capacity_until_GC = MetaspaceGC::capacity_until_GC();
 252 
 253   const double minimum_free_percentage = MinMetaspaceFreeRatio / 100.0;
 254   const double maximum_used_percentage = 1.0 - minimum_free_percentage;
 255 
 256   const double min_tmp = used_after_gc / maximum_used_percentage;
 257   size_t minimum_desired_capacity =
 258     (size_t)MIN2(min_tmp, double(MaxMetaspaceSize));
 259   // Don't shrink less than the initial generation size
 260   minimum_desired_capacity = MAX2(minimum_desired_capacity,
 261                                   MetaspaceSize);
 262 
 263   log_trace(gc, metaspace)("MetaspaceGC::compute_new_size: ");
 264   log_trace(gc, metaspace)("    minimum_free_percentage: %6.2f  maximum_used_percentage: %6.2f",
 265                            minimum_free_percentage, maximum_used_percentage);
 266   log_trace(gc, metaspace)("     used_after_gc       : %6.1fKB", used_after_gc / (double) K);
 267 
 268 
 269   size_t shrink_bytes = 0;
 270   if (capacity_until_GC &lt; minimum_desired_capacity) {
 271     // If we have less capacity below the metaspace HWM, then
 272     // increment the HWM.
 273     size_t expand_bytes = minimum_desired_capacity - capacity_until_GC;
 274     expand_bytes = align_up(expand_bytes, Metaspace::commit_alignment());
 275     // Don't expand unless it's significant
 276     if (expand_bytes &gt;= MinMetaspaceExpansion) {
 277       size_t new_capacity_until_GC = 0;
 278       bool succeeded = MetaspaceGC::inc_capacity_until_GC(expand_bytes, &amp;new_capacity_until_GC);
 279       assert(succeeded, "Should always succesfully increment HWM when at safepoint");
 280 
 281       Metaspace::tracer()-&gt;report_gc_threshold(capacity_until_GC,
 282                                                new_capacity_until_GC,
 283                                                MetaspaceGCThresholdUpdater::ComputeNewSize);
 284       log_trace(gc, metaspace)("    expanding:  minimum_desired_capacity: %6.1fKB  expand_bytes: %6.1fKB  MinMetaspaceExpansion: %6.1fKB  new metaspace HWM:  %6.1fKB",
 285                                minimum_desired_capacity / (double) K,
 286                                expand_bytes / (double) K,
 287                                MinMetaspaceExpansion / (double) K,
 288                                new_capacity_until_GC / (double) K);
 289     }
 290     return;
 291   }
 292 
 293   // No expansion, now see if we want to shrink
 294   // We would never want to shrink more than this
 295   assert(capacity_until_GC &gt;= minimum_desired_capacity,
 296          SIZE_FORMAT " &gt;= " SIZE_FORMAT,
 297          capacity_until_GC, minimum_desired_capacity);
 298   size_t max_shrink_bytes = capacity_until_GC - minimum_desired_capacity;
 299 
 300   // Should shrinking be considered?
 301   if (MaxMetaspaceFreeRatio &lt; 100) {
 302     const double maximum_free_percentage = MaxMetaspaceFreeRatio / 100.0;
 303     const double minimum_used_percentage = 1.0 - maximum_free_percentage;
 304     const double max_tmp = used_after_gc / minimum_used_percentage;
 305     size_t maximum_desired_capacity = (size_t)MIN2(max_tmp, double(MaxMetaspaceSize));
 306     maximum_desired_capacity = MAX2(maximum_desired_capacity,
 307                                     MetaspaceSize);
 308     log_trace(gc, metaspace)("    maximum_free_percentage: %6.2f  minimum_used_percentage: %6.2f",
 309                              maximum_free_percentage, minimum_used_percentage);
 310     log_trace(gc, metaspace)("    minimum_desired_capacity: %6.1fKB  maximum_desired_capacity: %6.1fKB",
 311                              minimum_desired_capacity / (double) K, maximum_desired_capacity / (double) K);
 312 
 313     assert(minimum_desired_capacity &lt;= maximum_desired_capacity,
 314            "sanity check");
 315 
 316     if (capacity_until_GC &gt; maximum_desired_capacity) {
 317       // Capacity too large, compute shrinking size
 318       shrink_bytes = capacity_until_GC - maximum_desired_capacity;
 319       // We don't want shrink all the way back to initSize if people call
 320       // System.gc(), because some programs do that between "phases" and then
 321       // we'd just have to grow the heap up again for the next phase.  So we
 322       // damp the shrinking: 0% on the first call, 10% on the second call, 40%
 323       // on the third call, and 100% by the fourth call.  But if we recompute
 324       // size without shrinking, it goes back to 0%.
 325       shrink_bytes = shrink_bytes / 100 * current_shrink_factor;
 326 
 327       shrink_bytes = align_down(shrink_bytes, Metaspace::commit_alignment());
 328 
 329       assert(shrink_bytes &lt;= max_shrink_bytes,
 330              "invalid shrink size " SIZE_FORMAT " not &lt;= " SIZE_FORMAT,
 331              shrink_bytes, max_shrink_bytes);
 332       if (current_shrink_factor == 0) {
 333         _shrink_factor = 10;
 334       } else {
 335         _shrink_factor = MIN2(current_shrink_factor * 4, (uint) 100);
 336       }
 337       log_trace(gc, metaspace)("    shrinking:  initThreshold: %.1fK  maximum_desired_capacity: %.1fK",
 338                                MetaspaceSize / (double) K, maximum_desired_capacity / (double) K);
 339       log_trace(gc, metaspace)("    shrink_bytes: %.1fK  current_shrink_factor: %d  new shrink factor: %d  MinMetaspaceExpansion: %.1fK",
 340                                shrink_bytes / (double) K, current_shrink_factor, _shrink_factor, MinMetaspaceExpansion / (double) K);
 341     }
 342   }
 343 
 344   // Don't shrink unless it's significant
 345   if (shrink_bytes &gt;= MinMetaspaceExpansion &amp;&amp;
 346       ((capacity_until_GC - shrink_bytes) &gt;= MetaspaceSize)) {
 347     size_t new_capacity_until_GC = MetaspaceGC::dec_capacity_until_GC(shrink_bytes);
 348     Metaspace::tracer()-&gt;report_gc_threshold(capacity_until_GC,
 349                                              new_capacity_until_GC,
 350                                              MetaspaceGCThresholdUpdater::ComputeNewSize);
 351   }
 352 }
 353 
 354 // MetaspaceUtils
 355 size_t MetaspaceUtils::_capacity_words [Metaspace:: MetadataTypeCount] = {0, 0};
 356 size_t MetaspaceUtils::_overhead_words [Metaspace:: MetadataTypeCount] = {0, 0};
 357 volatile size_t MetaspaceUtils::_used_words [Metaspace:: MetadataTypeCount] = {0, 0};
 358 
 359 // Collect used metaspace statistics. This involves walking the CLDG. The resulting
 360 // output will be the accumulated values for all live metaspaces.
 361 // Note: method does not do any locking.
 362 void MetaspaceUtils::collect_statistics(ClassLoaderMetaspaceStatistics* out) {
 363   out-&gt;reset();
 364   ClassLoaderDataGraphMetaspaceIterator iter;
 365    while (iter.repeat()) {
 366      ClassLoaderMetaspace* msp = iter.get_next();
 367      if (msp != NULL) {
 368        msp-&gt;add_to_statistics(out);
 369      }
 370    }
 371 }
 372 
 373 size_t MetaspaceUtils::free_in_vs_bytes(Metaspace::MetadataType mdtype) {
 374   VirtualSpaceList* list = Metaspace::get_space_list(mdtype);
 375   return list == NULL ? 0 : list-&gt;free_bytes();
 376 }
 377 
 378 size_t MetaspaceUtils::free_in_vs_bytes() {
 379   return free_in_vs_bytes(Metaspace::ClassType) + free_in_vs_bytes(Metaspace::NonClassType);
 380 }
 381 
 382 static void inc_stat_nonatomically(size_t* pstat, size_t words) {
 383   assert_lock_strong(MetaspaceExpand_lock);
 384   (*pstat) += words;
 385 }
 386 
 387 static void dec_stat_nonatomically(size_t* pstat, size_t words) {
 388   assert_lock_strong(MetaspaceExpand_lock);
 389   const size_t size_now = *pstat;
 390   assert(size_now &gt;= words, "About to decrement counter below zero "
 391          "(current value: " SIZE_FORMAT ", decrement value: " SIZE_FORMAT ".",
 392          size_now, words);
 393   *pstat = size_now - words;
 394 }
 395 
 396 static void inc_stat_atomically(volatile size_t* pstat, size_t words) {
 397   Atomic::add(pstat, words);
 398 }
 399 
 400 static void dec_stat_atomically(volatile size_t* pstat, size_t words) {
 401   const size_t size_now = *pstat;
 402   assert(size_now &gt;= words, "About to decrement counter below zero "
 403          "(current value: " SIZE_FORMAT ", decrement value: " SIZE_FORMAT ".",
 404          size_now, words);
 405   Atomic::sub(pstat, words);
 406 }
 407 
 408 void MetaspaceUtils::dec_capacity(Metaspace::MetadataType mdtype, size_t words) {
 409   dec_stat_nonatomically(&amp;_capacity_words[mdtype], words);
 410 }
 411 void MetaspaceUtils::inc_capacity(Metaspace::MetadataType mdtype, size_t words) {
 412   inc_stat_nonatomically(&amp;_capacity_words[mdtype], words);
 413 }
 414 void MetaspaceUtils::dec_used(Metaspace::MetadataType mdtype, size_t words) {
 415   dec_stat_atomically(&amp;_used_words[mdtype], words);
 416 }
 417 void MetaspaceUtils::inc_used(Metaspace::MetadataType mdtype, size_t words) {
 418   inc_stat_atomically(&amp;_used_words[mdtype], words);
 419 }
 420 void MetaspaceUtils::dec_overhead(Metaspace::MetadataType mdtype, size_t words) {
 421   dec_stat_nonatomically(&amp;_overhead_words[mdtype], words);
 422 }
 423 void MetaspaceUtils::inc_overhead(Metaspace::MetadataType mdtype, size_t words) {
 424   inc_stat_nonatomically(&amp;_overhead_words[mdtype], words);
 425 }
 426 
 427 size_t MetaspaceUtils::reserved_bytes(Metaspace::MetadataType mdtype) {
 428   VirtualSpaceList* list = Metaspace::get_space_list(mdtype);
 429   return list == NULL ? 0 : list-&gt;reserved_bytes();
 430 }
 431 
 432 size_t MetaspaceUtils::committed_bytes(Metaspace::MetadataType mdtype) {
 433   VirtualSpaceList* list = Metaspace::get_space_list(mdtype);
 434   return list == NULL ? 0 : list-&gt;committed_bytes();
 435 }
 436 
 437 size_t MetaspaceUtils::min_chunk_size_words() { return Metaspace::first_chunk_word_size(); }
 438 
 439 size_t MetaspaceUtils::free_chunks_total_words(Metaspace::MetadataType mdtype) {
 440   ChunkManager* chunk_manager = Metaspace::get_chunk_manager(mdtype);
 441   if (chunk_manager == NULL) {
 442     return 0;
 443   }
 444   return chunk_manager-&gt;free_chunks_total_words();
 445 }
 446 
 447 size_t MetaspaceUtils::free_chunks_total_bytes(Metaspace::MetadataType mdtype) {
 448   return free_chunks_total_words(mdtype) * BytesPerWord;
 449 }
 450 
 451 size_t MetaspaceUtils::free_chunks_total_words() {
 452   return free_chunks_total_words(Metaspace::ClassType) +
 453          free_chunks_total_words(Metaspace::NonClassType);
 454 }
 455 
 456 size_t MetaspaceUtils::free_chunks_total_bytes() {
 457   return free_chunks_total_words() * BytesPerWord;
 458 }
 459 
 460 bool MetaspaceUtils::has_chunk_free_list(Metaspace::MetadataType mdtype) {
 461   return Metaspace::get_chunk_manager(mdtype) != NULL;
 462 }
 463 
 464 MetaspaceChunkFreeListSummary MetaspaceUtils::chunk_free_list_summary(Metaspace::MetadataType mdtype) {
 465   if (!has_chunk_free_list(mdtype)) {
 466     return MetaspaceChunkFreeListSummary();
 467   }
 468 
 469   const ChunkManager* cm = Metaspace::get_chunk_manager(mdtype);
 470   return cm-&gt;chunk_free_list_summary();
 471 }
 472 
 473 void MetaspaceUtils::print_metaspace_change(const metaspace::MetaspaceSizesSnapshot&amp; pre_meta_values) {
 474   const metaspace::MetaspaceSizesSnapshot meta_values;
 475 
 476   if (Metaspace::using_class_space()) {
 477     log_info(gc, metaspace)(HEAP_CHANGE_FORMAT" "
 478                             HEAP_CHANGE_FORMAT" "
 479                             HEAP_CHANGE_FORMAT,
 480                             HEAP_CHANGE_FORMAT_ARGS("Metaspace",
 481                                                     pre_meta_values.used(),
 482                                                     pre_meta_values.committed(),
 483                                                     meta_values.used(),
 484                                                     meta_values.committed()),
 485                             HEAP_CHANGE_FORMAT_ARGS("NonClass",
 486                                                     pre_meta_values.non_class_used(),
 487                                                     pre_meta_values.non_class_committed(),
 488                                                     meta_values.non_class_used(),
 489                                                     meta_values.non_class_committed()),
 490                             HEAP_CHANGE_FORMAT_ARGS("Class",
 491                                                     pre_meta_values.class_used(),
 492                                                     pre_meta_values.class_committed(),
 493                                                     meta_values.class_used(),
 494                                                     meta_values.class_committed()));
 495   } else {
 496     log_info(gc, metaspace)(HEAP_CHANGE_FORMAT,
 497                             HEAP_CHANGE_FORMAT_ARGS("Metaspace",
 498                                                     pre_meta_values.used(),
 499                                                     pre_meta_values.committed(),
 500                                                     meta_values.used(),
 501                                                     meta_values.committed()));
 502   }
 503 }
 504 
 505 void MetaspaceUtils::print_on(outputStream* out) {
 506   Metaspace::MetadataType nct = Metaspace::NonClassType;
 507 
 508   out-&gt;print_cr(" Metaspace       "
 509                 "used "      SIZE_FORMAT "K, "
 510                 "capacity "  SIZE_FORMAT "K, "
 511                 "committed " SIZE_FORMAT "K, "
 512                 "reserved "  SIZE_FORMAT "K",
 513                 used_bytes()/K,
 514                 capacity_bytes()/K,
 515                 committed_bytes()/K,
 516                 reserved_bytes()/K);
 517 
 518   if (Metaspace::using_class_space()) {
 519     Metaspace::MetadataType ct = Metaspace::ClassType;
 520     out-&gt;print_cr("  class space    "
 521                   "used "      SIZE_FORMAT "K, "
 522                   "capacity "  SIZE_FORMAT "K, "
 523                   "committed " SIZE_FORMAT "K, "
 524                   "reserved "  SIZE_FORMAT "K",
 525                   used_bytes(ct)/K,
 526                   capacity_bytes(ct)/K,
 527                   committed_bytes(ct)/K,
 528                   reserved_bytes(ct)/K);
 529   }
 530 }
 531 
 532 
 533 void MetaspaceUtils::print_vs(outputStream* out, size_t scale) {
 534   const size_t reserved_nonclass_words = reserved_bytes(Metaspace::NonClassType) / sizeof(MetaWord);
 535   const size_t committed_nonclass_words = committed_bytes(Metaspace::NonClassType) / sizeof(MetaWord);
 536   {
 537     if (Metaspace::using_class_space()) {
 538       out-&gt;print("  Non-class space:  ");
 539     }
 540     print_scaled_words(out, reserved_nonclass_words, scale, 7);
 541     out-&gt;print(" reserved, ");
 542     print_scaled_words_and_percentage(out, committed_nonclass_words, reserved_nonclass_words, scale, 7);
 543     out-&gt;print_cr(" committed ");
 544 
 545     if (Metaspace::using_class_space()) {
 546       const size_t reserved_class_words = reserved_bytes(Metaspace::ClassType) / sizeof(MetaWord);
 547       const size_t committed_class_words = committed_bytes(Metaspace::ClassType) / sizeof(MetaWord);
 548       out-&gt;print("      Class space:  ");
 549       print_scaled_words(out, reserved_class_words, scale, 7);
 550       out-&gt;print(" reserved, ");
 551       print_scaled_words_and_percentage(out, committed_class_words, reserved_class_words, scale, 7);
 552       out-&gt;print_cr(" committed ");
 553 
 554       const size_t reserved_words = reserved_nonclass_words + reserved_class_words;
 555       const size_t committed_words = committed_nonclass_words + committed_class_words;
 556       out-&gt;print("             Both:  ");
 557       print_scaled_words(out, reserved_words, scale, 7);
 558       out-&gt;print(" reserved, ");
 559       print_scaled_words_and_percentage(out, committed_words, reserved_words, scale, 7);
 560       out-&gt;print_cr(" committed ");
 561     }
 562   }
 563 }
 564 
 565 static void print_basic_switches(outputStream* out, size_t scale) {
 566   out-&gt;print("MaxMetaspaceSize: ");
 567   if (MaxMetaspaceSize &gt;= (max_uintx) - (2 * os::vm_page_size())) {
 568     // aka "very big". Default is max_uintx, but due to rounding in arg parsing the real
 569     // value is smaller.
 570     out-&gt;print("unlimited");
 571   } else {
 572     print_human_readable_size(out, MaxMetaspaceSize, scale);
 573   }
 574   out-&gt;cr();
 575   if (Metaspace::using_class_space()) {
 576     out-&gt;print("CompressedClassSpaceSize: ");
 577     print_human_readable_size(out, CompressedClassSpaceSize, scale);
 578   }
 579   out-&gt;cr();
 580 }
 581 
 582 // This will print out a basic metaspace usage report but
 583 // unlike print_report() is guaranteed not to lock or to walk the CLDG.
 584 void MetaspaceUtils::print_basic_report(outputStream* out, size_t scale) {
 585 
 586   if (!Metaspace::initialized()) {
 587     out-&gt;print_cr("Metaspace not yet initialized.");
 588     return;
 589   }
 590 
 591   out-&gt;cr();
 592   out-&gt;print_cr("Usage:");
 593 
 594   if (Metaspace::using_class_space()) {
 595     out-&gt;print("  Non-class:  ");
 596   }
 597 
 598   // In its most basic form, we do not require walking the CLDG. Instead, just print the running totals from
 599   // MetaspaceUtils.
 600   const size_t cap_nc = MetaspaceUtils::capacity_words(Metaspace::NonClassType);
 601   const size_t overhead_nc = MetaspaceUtils::overhead_words(Metaspace::NonClassType);
 602   const size_t used_nc = MetaspaceUtils::used_words(Metaspace::NonClassType);
 603   const size_t free_and_waste_nc = cap_nc - overhead_nc - used_nc;
 604 
 605   print_scaled_words(out, cap_nc, scale, 5);
 606   out-&gt;print(" capacity, ");
 607   print_scaled_words_and_percentage(out, used_nc, cap_nc, scale, 5);
 608   out-&gt;print(" used, ");
 609   print_scaled_words_and_percentage(out, free_and_waste_nc, cap_nc, scale, 5);
 610   out-&gt;print(" free+waste, ");
 611   print_scaled_words_and_percentage(out, overhead_nc, cap_nc, scale, 5);
 612   out-&gt;print(" overhead. ");
 613   out-&gt;cr();
 614 
 615   if (Metaspace::using_class_space()) {
 616     const size_t cap_c = MetaspaceUtils::capacity_words(Metaspace::ClassType);
 617     const size_t overhead_c = MetaspaceUtils::overhead_words(Metaspace::ClassType);
 618     const size_t used_c = MetaspaceUtils::used_words(Metaspace::ClassType);
 619     const size_t free_and_waste_c = cap_c - overhead_c - used_c;
 620     out-&gt;print("      Class:  ");
 621     print_scaled_words(out, cap_c, scale, 5);
 622     out-&gt;print(" capacity, ");
 623     print_scaled_words_and_percentage(out, used_c, cap_c, scale, 5);
 624     out-&gt;print(" used, ");
 625     print_scaled_words_and_percentage(out, free_and_waste_c, cap_c, scale, 5);
 626     out-&gt;print(" free+waste, ");
 627     print_scaled_words_and_percentage(out, overhead_c, cap_c, scale, 5);
 628     out-&gt;print(" overhead. ");
 629     out-&gt;cr();
 630 
 631     out-&gt;print("       Both:  ");
 632     const size_t cap = cap_nc + cap_c;
 633 
 634     print_scaled_words(out, cap, scale, 5);
 635     out-&gt;print(" capacity, ");
 636     print_scaled_words_and_percentage(out, used_nc + used_c, cap, scale, 5);
 637     out-&gt;print(" used, ");
 638     print_scaled_words_and_percentage(out, free_and_waste_nc + free_and_waste_c, cap, scale, 5);
 639     out-&gt;print(" free+waste, ");
 640     print_scaled_words_and_percentage(out, overhead_nc + overhead_c, cap, scale, 5);
 641     out-&gt;print(" overhead. ");
 642     out-&gt;cr();
 643   }
 644 
 645   out-&gt;cr();
 646   out-&gt;print_cr("Virtual space:");
 647 
 648   print_vs(out, scale);
 649 
 650   out-&gt;cr();
 651   out-&gt;print_cr("Chunk freelists:");
 652 
 653   if (Metaspace::using_class_space()) {
 654     out-&gt;print("   Non-Class:  ");
 655   }
 656   print_human_readable_size(out, Metaspace::chunk_manager_metadata()-&gt;free_chunks_total_bytes(), scale);
 657   out-&gt;cr();
 658   if (Metaspace::using_class_space()) {
 659     out-&gt;print("       Class:  ");
 660     print_human_readable_size(out, Metaspace::chunk_manager_class()-&gt;free_chunks_total_bytes(), scale);
 661     out-&gt;cr();
 662     out-&gt;print("        Both:  ");
 663     print_human_readable_size(out, Metaspace::chunk_manager_class()-&gt;free_chunks_total_bytes() +
 664                               Metaspace::chunk_manager_metadata()-&gt;free_chunks_total_bytes(), scale);
 665     out-&gt;cr();
 666   }
 667 
 668   out-&gt;cr();
 669 
 670   // Print basic settings
 671   print_basic_switches(out, scale);
 672 
 673   out-&gt;cr();
 674 
 675 }
 676 
 677 void MetaspaceUtils::print_report(outputStream* out, size_t scale, int flags) {
 678 
 679   if (!Metaspace::initialized()) {
 680     out-&gt;print_cr("Metaspace not yet initialized.");
 681     return;
 682   }
 683 
 684   const bool print_loaders = (flags &amp; rf_show_loaders) &gt; 0;
 685   const bool print_classes = (flags &amp; rf_show_classes) &gt; 0;
 686   const bool print_by_chunktype = (flags &amp; rf_break_down_by_chunktype) &gt; 0;
 687   const bool print_by_spacetype = (flags &amp; rf_break_down_by_spacetype) &gt; 0;
 688 
 689   // Some report options require walking the class loader data graph.
 690   PrintCLDMetaspaceInfoClosure cl(out, scale, print_loaders, print_classes, print_by_chunktype);
 691   if (print_loaders) {
 692     out-&gt;cr();
 693     out-&gt;print_cr("Usage per loader:");
 694     out-&gt;cr();
 695   }
 696 
 697   ClassLoaderDataGraph::loaded_cld_do(&amp;cl); // collect data and optionally print
 698 
 699   // Print totals, broken up by space type.
 700   if (print_by_spacetype) {
 701     out-&gt;cr();
 702     out-&gt;print_cr("Usage per space type:");
 703     out-&gt;cr();
 704     for (int space_type = (int)Metaspace::ZeroMetaspaceType;
 705          space_type &lt; (int)Metaspace::MetaspaceTypeCount; space_type ++)
 706     {
 707       uintx num_loaders = cl._num_loaders_by_spacetype[space_type];
 708       uintx num_classes = cl._num_classes_by_spacetype[space_type];
 709       out-&gt;print("%s - " UINTX_FORMAT " %s",
 710         space_type_name((Metaspace::MetaspaceType)space_type),
 711         num_loaders, loaders_plural(num_loaders));
 712       if (num_classes &gt; 0) {
 713         out-&gt;print(", ");
 714         print_number_of_classes(out, num_classes, cl._num_classes_shared_by_spacetype[space_type]);
 715         out-&gt;print(":");
 716         cl._stats_by_spacetype[space_type].print_on(out, scale, print_by_chunktype);
 717       } else {
 718         out-&gt;print(".");
 719         out-&gt;cr();
 720       }
 721       out-&gt;cr();
 722     }
 723   }
 724 
 725   // Print totals for in-use data:
 726   out-&gt;cr();
 727   {
 728     uintx num_loaders = cl._num_loaders;
 729     out-&gt;print("Total Usage - " UINTX_FORMAT " %s, ",
 730       num_loaders, loaders_plural(num_loaders));
 731     print_number_of_classes(out, cl._num_classes, cl._num_classes_shared);
 732     out-&gt;print(":");
 733     cl._stats_total.print_on(out, scale, print_by_chunktype);
 734     out-&gt;cr();
 735   }
 736 
 737   // -- Print Virtual space.
 738   out-&gt;cr();
 739   out-&gt;print_cr("Virtual space:");
 740 
 741   print_vs(out, scale);
 742 
 743   // -- Print VirtualSpaceList details.
 744   if ((flags &amp; rf_show_vslist) &gt; 0) {
 745     out-&gt;cr();
 746     out-&gt;print_cr("Virtual space list%s:", Metaspace::using_class_space() ? "s" : "");
 747 
 748     if (Metaspace::using_class_space()) {
 749       out-&gt;print_cr("   Non-Class:");
 750     }
 751     Metaspace::space_list()-&gt;print_on(out, scale);
 752     if (Metaspace::using_class_space()) {
 753       out-&gt;print_cr("       Class:");
 754       Metaspace::class_space_list()-&gt;print_on(out, scale);
 755     }
 756   }
 757   out-&gt;cr();
 758 
 759   // -- Print VirtualSpaceList map.
 760   if ((flags &amp; rf_show_vsmap) &gt; 0) {
 761     out-&gt;cr();
 762     out-&gt;print_cr("Virtual space map:");
 763 
 764     if (Metaspace::using_class_space()) {
 765       out-&gt;print_cr("   Non-Class:");
 766     }
 767     Metaspace::space_list()-&gt;print_map(out);
 768     if (Metaspace::using_class_space()) {
 769       out-&gt;print_cr("       Class:");
 770       Metaspace::class_space_list()-&gt;print_map(out);
 771     }
 772   }
 773   out-&gt;cr();
 774 
 775   // -- Print Freelists (ChunkManager) details
 776   out-&gt;cr();
 777   out-&gt;print_cr("Chunk freelist%s:", Metaspace::using_class_space() ? "s" : "");
 778 
 779   ChunkManagerStatistics non_class_cm_stat;
 780   Metaspace::chunk_manager_metadata()-&gt;collect_statistics(&amp;non_class_cm_stat);
 781 
 782   if (Metaspace::using_class_space()) {
 783     out-&gt;print_cr("   Non-Class:");
 784   }
 785   non_class_cm_stat.print_on(out, scale);
 786 
 787   if (Metaspace::using_class_space()) {
 788     ChunkManagerStatistics class_cm_stat;
 789     Metaspace::chunk_manager_class()-&gt;collect_statistics(&amp;class_cm_stat);
 790     out-&gt;print_cr("       Class:");
 791     class_cm_stat.print_on(out, scale);
 792   }
 793 
 794   // As a convenience, print a summary of common waste.
 795   out-&gt;cr();
 796   out-&gt;print("Waste ");
 797   // For all wastages, print percentages from total. As total use the total size of memory committed for metaspace.
 798   const size_t committed_words = committed_bytes() / BytesPerWord;
 799 
 800   out-&gt;print("(percentages refer to total committed size ");
 801   print_scaled_words(out, committed_words, scale);
 802   out-&gt;print_cr("):");
 803 
 804   // Print space committed but not yet used by any class loader
 805   const size_t unused_words_in_vs = MetaspaceUtils::free_in_vs_bytes() / BytesPerWord;
 806   out-&gt;print("              Committed unused: ");
 807   print_scaled_words_and_percentage(out, unused_words_in_vs, committed_words, scale, 6);
 808   out-&gt;cr();
 809 
 810   // Print waste for in-use chunks.
 811   UsedChunksStatistics ucs_nonclass = cl._stats_total.nonclass_sm_stats().totals();
 812   UsedChunksStatistics ucs_class = cl._stats_total.class_sm_stats().totals();
 813   UsedChunksStatistics ucs_all;
 814   ucs_all.add(ucs_nonclass);
 815   ucs_all.add(ucs_class);
 816 
 817   out-&gt;print("        Waste in chunks in use: ");
 818   print_scaled_words_and_percentage(out, ucs_all.waste(), committed_words, scale, 6);
 819   out-&gt;cr();
 820   out-&gt;print("         Free in chunks in use: ");
 821   print_scaled_words_and_percentage(out, ucs_all.free(), committed_words, scale, 6);
 822   out-&gt;cr();
 823   out-&gt;print("     Overhead in chunks in use: ");
 824   print_scaled_words_and_percentage(out, ucs_all.overhead(), committed_words, scale, 6);
 825   out-&gt;cr();
 826 
 827   // Print waste in free chunks.
 828   const size_t total_capacity_in_free_chunks =
 829       Metaspace::chunk_manager_metadata()-&gt;free_chunks_total_words() +
 830      (Metaspace::using_class_space() ? Metaspace::chunk_manager_class()-&gt;free_chunks_total_words() : 0);
 831   out-&gt;print("                In free chunks: ");
 832   print_scaled_words_and_percentage(out, total_capacity_in_free_chunks, committed_words, scale, 6);
 833   out-&gt;cr();
 834 
 835   // Print waste in deallocated blocks.
 836   const uintx free_blocks_num =
 837       cl._stats_total.nonclass_sm_stats().free_blocks_num() +
 838       cl._stats_total.class_sm_stats().free_blocks_num();
 839   const size_t free_blocks_cap_words =
 840       cl._stats_total.nonclass_sm_stats().free_blocks_cap_words() +
 841       cl._stats_total.class_sm_stats().free_blocks_cap_words();
 842   out-&gt;print("Deallocated from chunks in use: ");
 843   print_scaled_words_and_percentage(out, free_blocks_cap_words, committed_words, scale, 6);
 844   out-&gt;print(" (" UINTX_FORMAT " blocks)", free_blocks_num);
 845   out-&gt;cr();
 846 
 847   // Print total waste.
 848   const size_t total_waste = ucs_all.waste() + ucs_all.free() + ucs_all.overhead() + total_capacity_in_free_chunks
 849       + free_blocks_cap_words + unused_words_in_vs;
 850   out-&gt;print("                       -total-: ");
 851   print_scaled_words_and_percentage(out, total_waste, committed_words, scale, 6);
 852   out-&gt;cr();
 853 
 854   // Print internal statistics
 855 #ifdef ASSERT
 856   out-&gt;cr();
 857   out-&gt;cr();
 858   out-&gt;print_cr("Internal statistics:");
 859   out-&gt;cr();
 860   out-&gt;print_cr("Number of allocations: " UINTX_FORMAT ".", g_internal_statistics.num_allocs);
 861   out-&gt;print_cr("Number of space births: " UINTX_FORMAT ".", g_internal_statistics.num_metaspace_births);
 862   out-&gt;print_cr("Number of space deaths: " UINTX_FORMAT ".", g_internal_statistics.num_metaspace_deaths);
 863   out-&gt;print_cr("Number of virtual space node births: " UINTX_FORMAT ".", g_internal_statistics.num_vsnodes_created);
 864   out-&gt;print_cr("Number of virtual space node deaths: " UINTX_FORMAT ".", g_internal_statistics.num_vsnodes_purged);
 865   out-&gt;print_cr("Number of times virtual space nodes were expanded: " UINTX_FORMAT ".", g_internal_statistics.num_committed_space_expanded);
 866   out-&gt;print_cr("Number of deallocations: " UINTX_FORMAT " (" UINTX_FORMAT " external).", g_internal_statistics.num_deallocs, g_internal_statistics.num_external_deallocs);
 867   out-&gt;print_cr("Allocations from deallocated blocks: " UINTX_FORMAT ".", g_internal_statistics.num_allocs_from_deallocated_blocks);
 868   out-&gt;print_cr("Number of chunks added to freelist: " UINTX_FORMAT ".",
 869                 g_internal_statistics.num_chunks_added_to_freelist);
 870   out-&gt;print_cr("Number of chunks removed from freelist: " UINTX_FORMAT ".",
 871                 g_internal_statistics.num_chunks_removed_from_freelist);
 872   out-&gt;print_cr("Number of chunk merges: " UINTX_FORMAT ", split-ups: " UINTX_FORMAT ".",
 873                 g_internal_statistics.num_chunk_merges, g_internal_statistics.num_chunk_splits);
 874 
 875   out-&gt;cr();
 876 #endif
 877 
 878   // Print some interesting settings
 879   out-&gt;cr();
 880   out-&gt;cr();
 881   print_basic_switches(out, scale);
 882 
 883   out-&gt;cr();
 884   out-&gt;print("InitialBootClassLoaderMetaspaceSize: ");
 885   print_human_readable_size(out, InitialBootClassLoaderMetaspaceSize, scale);
 886 
 887   out-&gt;cr();
 888   out-&gt;cr();
 889 
 890 } // MetaspaceUtils::print_report()
 891 
 892 // Prints an ASCII representation of the given space.
 893 void MetaspaceUtils::print_metaspace_map(outputStream* out, Metaspace::MetadataType mdtype) {
 894   MutexLocker cl(MetaspaceExpand_lock, Mutex::_no_safepoint_check_flag);
 895   const bool for_class = mdtype == Metaspace::ClassType ? true : false;
 896   VirtualSpaceList* const vsl = for_class ? Metaspace::class_space_list() : Metaspace::space_list();
 897   if (vsl != NULL) {
 898     if (for_class) {
 899       if (!Metaspace::using_class_space()) {
 900         out-&gt;print_cr("No Class Space.");
 901         return;
 902       }
 903       out-&gt;print_raw("---- Metaspace Map (Class Space) ----");
 904     } else {
 905       out-&gt;print_raw("---- Metaspace Map (Non-Class Space) ----");
 906     }
 907     // Print legend:
 908     out-&gt;cr();
 909     out-&gt;print_cr("Chunk Types (uppercase chunks are in use): x-specialized, s-small, m-medium, h-humongous.");
 910     out-&gt;cr();
 911     VirtualSpaceList* const vsl = for_class ? Metaspace::class_space_list() : Metaspace::space_list();
 912     vsl-&gt;print_map(out);
 913     out-&gt;cr();
 914   }
 915 }
 916 
 917 void MetaspaceUtils::verify_free_chunks() {
 918 #ifdef ASSERT
 919   Metaspace::chunk_manager_metadata()-&gt;verify(false);
 920   if (Metaspace::using_class_space()) {
 921     Metaspace::chunk_manager_class()-&gt;verify(false);
 922   }
 923 #endif
 924 }
 925 
 926 void MetaspaceUtils::verify_metrics() {
 927 #ifdef ASSERT
 928   // Please note: there are time windows where the internal counters are out of sync with
 929   // reality. For example, when a newly created ClassLoaderMetaspace creates its first chunk -
 930   // the ClassLoaderMetaspace is not yet attached to its ClassLoaderData object and hence will
 931   // not be counted when iterating the CLDG. So be careful when you call this method.
 932   ClassLoaderMetaspaceStatistics total_stat;
 933   collect_statistics(&amp;total_stat);
 934   UsedChunksStatistics nonclass_chunk_stat = total_stat.nonclass_sm_stats().totals();
 935   UsedChunksStatistics class_chunk_stat = total_stat.class_sm_stats().totals();
 936 
 937   bool mismatch = false;
 938   for (int i = 0; i &lt; Metaspace::MetadataTypeCount; i ++) {
 939     Metaspace::MetadataType mdtype = (Metaspace::MetadataType)i;
 940     UsedChunksStatistics chunk_stat = total_stat.sm_stats(mdtype).totals();
 941     if (capacity_words(mdtype) != chunk_stat.cap() ||
 942         used_words(mdtype) != chunk_stat.used() ||
 943         overhead_words(mdtype) != chunk_stat.overhead()) {
 944       mismatch = true;
 945       tty-&gt;print_cr("MetaspaceUtils::verify_metrics: counter mismatch for mdtype=%u:", mdtype);
 946       tty-&gt;print_cr("Expected cap " SIZE_FORMAT ", used " SIZE_FORMAT ", overhead " SIZE_FORMAT ".",
 947                     capacity_words(mdtype), used_words(mdtype), overhead_words(mdtype));
 948       tty-&gt;print_cr("Got cap " SIZE_FORMAT ", used " SIZE_FORMAT ", overhead " SIZE_FORMAT ".",
 949                     chunk_stat.cap(), chunk_stat.used(), chunk_stat.overhead());
 950       tty-&gt;flush();
 951     }
 952   }
 953   assert(mismatch == false, "MetaspaceUtils::verify_metrics: counter mismatch.");
 954 #endif
 955 }
 956 
 957 // Metaspace methods
 958 
 959 size_t Metaspace::_first_chunk_word_size = 0;
 960 size_t Metaspace::_first_class_chunk_word_size = 0;
 961 
 962 size_t Metaspace::_commit_alignment = 0;
 963 size_t Metaspace::_reserve_alignment = 0;
 964 
 965 VirtualSpaceList* Metaspace::_space_list = NULL;
 966 VirtualSpaceList* Metaspace::_class_space_list = NULL;
 967 
 968 ChunkManager* Metaspace::_chunk_manager_metadata = NULL;
 969 ChunkManager* Metaspace::_chunk_manager_class = NULL;
 970 
 971 bool Metaspace::_initialized = false;
 972 
 973 #define VIRTUALSPACEMULTIPLIER 2
 974 
 975 #ifdef _LP64
 976 
 977 void Metaspace::print_compressed_class_space(outputStream* st) {
 978   if (_class_space_list != NULL) {
 979     address base = (address)_class_space_list-&gt;current_virtual_space()-&gt;bottom();
 980     address top = base + compressed_class_space_size();
 981     st-&gt;print("Compressed class space mapped at: " PTR_FORMAT "-" PTR_FORMAT ", size: " SIZE_FORMAT,
 982                p2i(base), p2i(top), top - base);
 983     st-&gt;cr();
 984   }
 985 }
 986 
 987 // Given a prereserved space, use that to set up the compressed class space list.
 988 void Metaspace::initialize_class_space(ReservedSpace rs) {
 989   assert(using_class_space(), "Must be using class space");
 990   assert(_class_space_list == NULL &amp;&amp; _chunk_manager_class == NULL, "Only call once");
 991 
 992   assert(rs.size() == CompressedClassSpaceSize, SIZE_FORMAT " != " SIZE_FORMAT,
 993          rs.size(), CompressedClassSpaceSize);
 994   assert(is_aligned(rs.base(), Metaspace::reserve_alignment()) &amp;&amp;
 995          is_aligned(rs.size(), Metaspace::reserve_alignment()),
 996          "wrong alignment");
 997 
 998   _class_space_list = new VirtualSpaceList(rs);
 999   _chunk_manager_class = new ChunkManager(true/*is_class*/);
1000 
1001   // This does currently not work because rs may be the result of a split
1002   // operation and NMT seems not to be able to handle splits.
1003   // Will be fixed with JDK-8243535.
1004   // MemTracker::record_virtual_memory_type((address)rs.base(), mtClass);
1005 
1006   if (!_class_space_list-&gt;initialization_succeeded()) {
1007     vm_exit_during_initialization("Failed to setup compressed class space virtual space list.");
1008   }
1009 
1010 }
1011 
1012 // Reserve a range of memory at an address suitable for en/decoding narrow
1013 // Klass pointers (see: CompressedClassPointers::is_valid_base()).
1014 // The returned address shall both be suitable as a compressed class pointers
1015 //  base, and aligned to Metaspace::reserve_alignment (which is equal to or a
1016 //  multiple of allocation granularity).
1017 // On error, returns an unreserved space.
1018 ReservedSpace Metaspace::reserve_address_space_for_compressed_classes(size_t size) {
1019 
1020 #ifdef AARCH64
1021   const size_t alignment = Metaspace::reserve_alignment();
1022 
1023   // AArch64: Try to align metaspace so that we can decode a compressed
1024   // klass with a single MOVK instruction. We can do this iff the
1025   // compressed class base is a multiple of 4G.
1026   // Additionally, above 32G, ensure the lower LogKlassAlignmentInBytes bits
1027   // of the upper 32-bits of the address are zero so we can handle a shift
1028   // when decoding.
1029 
1030   static const struct {
1031     address from;
1032     address to;
1033     size_t increment;
1034   } search_ranges[] = {
1035     {  (address)(4*G),   (address)(32*G),   4*G, },
1036     {  (address)(32*G),  (address)(1024*G), (4 &lt;&lt; LogKlassAlignmentInBytes) * G },
1037     {  NULL, NULL, 0 }
1038   };
1039 
1040   for (int i = 0; search_ranges[i].from != NULL; i ++) {
1041     address a = search_ranges[i].from;
1042     assert(CompressedKlassPointers::is_valid_base(a), "Sanity");
1043     while (a &lt; search_ranges[i].to) {
1044       ReservedSpace rs(size, Metaspace::reserve_alignment(),
1045                        false /*large_pages*/, (char*)a);
1046       if (rs.is_reserved()) {
1047         assert(a == (address)rs.base(), "Sanity");
1048         return rs;
1049       }
1050       a +=  search_ranges[i].increment;
1051     }
1052   }
1053 
1054   // Note: on AARCH64, if the code above does not find any good placement, we
1055   // have no recourse. We return an empty space and the VM will exit.
1056   return ReservedSpace();
1057 #else
1058   // Default implementation: Just reserve anywhere.
1059   return ReservedSpace(size, Metaspace::reserve_alignment(), false, (char*)NULL);
1060 #endif // AARCH64
1061 }
1062 
1063 #endif // _LP64
1064 
1065 
1066 void Metaspace::ergo_initialize() {
1067   if (DumpSharedSpaces) {
1068     // Using large pages when dumping the shared archive is currently not implemented.
1069     FLAG_SET_ERGO(UseLargePagesInMetaspace, false);
1070   }
1071 
1072   size_t page_size = os::vm_page_size();
1073   if (UseLargePages &amp;&amp; UseLargePagesInMetaspace) {
1074     page_size = os::large_page_size();
1075   }
1076 
1077   _commit_alignment  = page_size;
1078   _reserve_alignment = MAX2(page_size, (size_t)os::vm_allocation_granularity());
1079 
1080   // The upcoming Metaspace rewrite will impose a higher alignment granularity.
1081   // To prepare for that and to catch/prevent any misuse of Metaspace alignment
1082   // which may creep in, up the alignment a bit.
1083   if (_reserve_alignment == 4 * K) {
1084     _reserve_alignment *= 4;
1085   }
1086 
1087   // Do not use FLAG_SET_ERGO to update MaxMetaspaceSize, since this will
1088   // override if MaxMetaspaceSize was set on the command line or not.
1089   // This information is needed later to conform to the specification of the
1090   // java.lang.management.MemoryUsage API.
1091   //
1092   // Ideally, we would be able to set the default value of MaxMetaspaceSize in
1093   // globals.hpp to the aligned value, but this is not possible, since the
1094   // alignment depends on other flags being parsed.
1095   MaxMetaspaceSize = align_down_bounded(MaxMetaspaceSize, _reserve_alignment);
1096 
1097   if (MetaspaceSize &gt; MaxMetaspaceSize) {
1098     MetaspaceSize = MaxMetaspaceSize;
1099   }
1100 
1101   MetaspaceSize = align_down_bounded(MetaspaceSize, _commit_alignment);
1102 
1103   assert(MetaspaceSize &lt;= MaxMetaspaceSize, "MetaspaceSize should be limited by MaxMetaspaceSize");
1104 
1105   MinMetaspaceExpansion = align_down_bounded(MinMetaspaceExpansion, _commit_alignment);
1106   MaxMetaspaceExpansion = align_down_bounded(MaxMetaspaceExpansion, _commit_alignment);
1107 
1108   CompressedClassSpaceSize = align_down_bounded(CompressedClassSpaceSize, _reserve_alignment);
1109 
1110   // Initial virtual space size will be calculated at global_initialize()
1111   size_t min_metaspace_sz =
1112       VIRTUALSPACEMULTIPLIER * InitialBootClassLoaderMetaspaceSize;
1113   if (UseCompressedClassPointers) {
1114     if ((min_metaspace_sz + CompressedClassSpaceSize) &gt;  MaxMetaspaceSize) {
1115       if (min_metaspace_sz &gt;= MaxMetaspaceSize) {
1116         vm_exit_during_initialization("MaxMetaspaceSize is too small.");
1117       } else {
1118         FLAG_SET_ERGO(CompressedClassSpaceSize,
1119                       MaxMetaspaceSize - min_metaspace_sz);
1120       }
1121     }
1122   } else if (min_metaspace_sz &gt;= MaxMetaspaceSize) {
1123     FLAG_SET_ERGO(InitialBootClassLoaderMetaspaceSize,
1124                   min_metaspace_sz);
1125   }
1126 
1127   set_compressed_class_space_size(CompressedClassSpaceSize);
1128 }
1129 
1130 void Metaspace::global_initialize() {
1131   MetaspaceGC::initialize();
1132 
1133   // If UseCompressedClassPointers=1, we have two cases:
1134   // a) if CDS is active (either dump time or runtime), it will create the ccs
1135   //    for us, initialize it and set up CompressedKlassPointers encoding.
1136   //    Class space will be reserved above the mapped archives.
1137   // b) if CDS is not active, we will create the ccs on our own. It will be
1138   //    placed above the java heap, since we assume it has been placed in low
1139   //    address regions. We may rethink this (see JDK-8244943). Failing that,
1140   //    it will be placed anywhere.
1141 
1142 #if INCLUDE_CDS
1143   // case (a)
1144   if (DumpSharedSpaces) {
1145     MetaspaceShared::initialize_dumptime_shared_and_meta_spaces();
1146   } else if (UseSharedSpaces) {
1147     // If any of the archived space fails to map, UseSharedSpaces
1148     // is reset to false.
1149     MetaspaceShared::initialize_runtime_shared_and_meta_spaces();
1150   }
1151 
1152   if (DynamicDumpSharedSpaces &amp;&amp; !UseSharedSpaces) {
1153     vm_exit_during_initialization("DynamicDumpSharedSpaces is unsupported when base CDS archive is not loaded", NULL);
1154   }
1155 #endif // INCLUDE_CDS
1156 
1157 #ifdef _LP64
1158 
1159   if (using_class_space() &amp;&amp; !class_space_is_initialized()) {
1160     assert(!UseSharedSpaces &amp;&amp; !DumpSharedSpaces, "CDS should be off at this point");
1161 
1162     // case (b)
1163     ReservedSpace rs;
1164 
1165     // If UseCompressedOops=1, java heap may have been placed in coops-friendly
1166     //  territory already (lower address regions), so we attempt to place ccs
1167     //  right above the java heap.
1168     // If UseCompressedOops=0, the heap has been placed anywhere - probably in
1169     //  high memory regions. In that case, try to place ccs at the lowest allowed
1170     //  mapping address.
1171     address base = UseCompressedOops ? CompressedOops::end() : (address)HeapBaseMinAddress;
1172     base = align_up(base, Metaspace::reserve_alignment());
1173 
1174     const size_t size = align_up(CompressedClassSpaceSize, Metaspace::reserve_alignment());
1175     if (base != NULL) {
1176       if (CompressedKlassPointers::is_valid_base(base)) {
1177         rs = ReservedSpace(size, Metaspace::reserve_alignment(),
1178                            false /* large */, (char*)base);
1179       }
1180     }
1181 
1182     // ...failing that, reserve anywhere, but let platform do optimized placement:
1183     if (!rs.is_reserved()) {
1184       rs = Metaspace::reserve_address_space_for_compressed_classes(size);
1185     }
1186 
1187     // ...failing that, give up.
1188     if (!rs.is_reserved()) {
1189       vm_exit_during_initialization(
1190           err_msg("Could not allocate compressed class space: " SIZE_FORMAT " bytes",
1191                    compressed_class_space_size()));
1192     }
1193 
1194     // Initialize space
1195     Metaspace::initialize_class_space(rs);
1196 
1197     // Set up compressed class pointer encoding.
1198     CompressedKlassPointers::initialize((address)rs.base(), rs.size());
1199   }
1200 
1201 #endif
1202 
1203   // Initialize these before initializing the VirtualSpaceList
1204   _first_chunk_word_size = InitialBootClassLoaderMetaspaceSize / BytesPerWord;
1205   _first_chunk_word_size = align_word_size_up(_first_chunk_word_size);
1206   // Make the first class chunk bigger than a medium chunk so it's not put
1207   // on the medium chunk list.   The next chunk will be small and progress
1208   // from there.  This size calculated by -version.
1209   _first_class_chunk_word_size = MIN2((size_t)MediumChunk*6,
1210                                      (CompressedClassSpaceSize/BytesPerWord)*2);
1211   _first_class_chunk_word_size = align_word_size_up(_first_class_chunk_word_size);
1212   // Arbitrarily set the initial virtual space to a multiple
1213   // of the boot class loader size.
1214   size_t word_size = VIRTUALSPACEMULTIPLIER * _first_chunk_word_size;
1215   word_size = align_up(word_size, Metaspace::reserve_alignment_words());
1216 
1217   // Initialize the list of virtual spaces.
1218   _space_list = new VirtualSpaceList(word_size);
1219   _chunk_manager_metadata = new ChunkManager(false/*metaspace*/);
1220 
1221   if (!_space_list-&gt;initialization_succeeded()) {
1222     vm_exit_during_initialization("Unable to setup metadata virtual space list.", NULL);
1223   }
1224 
1225   _tracer = new MetaspaceTracer();
1226 
1227   _initialized = true;
1228 
1229 #ifdef _LP64
1230   if (UseCompressedClassPointers) {
1231     // Note: "cds" would be a better fit but keep this for backward compatibility.
1232     LogTarget(Info, gc, metaspace) lt;
1233     if (lt.is_enabled()) {
1234       ResourceMark rm;
1235       LogStream ls(lt);
1236       CDS_ONLY(MetaspaceShared::print_on(&amp;ls);)
1237       Metaspace::print_compressed_class_space(&amp;ls);
1238       CompressedKlassPointers::print_mode(&amp;ls);
1239     }
1240   }
1241 #endif
1242 
1243 }
1244 
1245 void Metaspace::post_initialize() {
1246   MetaspaceGC::post_initialize();
1247 }
1248 
1249 void Metaspace::verify_global_initialization() {
1250   assert(space_list() != NULL, "Metadata VirtualSpaceList has not been initialized");
1251   assert(chunk_manager_metadata() != NULL, "Metadata ChunkManager has not been initialized");
1252 
1253   if (using_class_space()) {
1254     assert(class_space_list() != NULL, "Class VirtualSpaceList has not been initialized");
1255     assert(chunk_manager_class() != NULL, "Class ChunkManager has not been initialized");
1256   }
1257 }
1258 
1259 size_t Metaspace::align_word_size_up(size_t word_size) {
1260   size_t byte_size = word_size * wordSize;
1261   return ReservedSpace::allocation_align_size_up(byte_size) / wordSize;
1262 }
1263 
1264 MetaWord* Metaspace::allocate(ClassLoaderData* loader_data, size_t word_size,
1265                               MetaspaceObj::Type type, TRAPS) {
1266   assert(!_frozen, "sanity");
1267   assert(!(DumpSharedSpaces &amp;&amp; THREAD-&gt;is_VM_thread()), "sanity");
1268 
1269   if (HAS_PENDING_EXCEPTION) {
1270     assert(false, "Should not allocate with exception pending");
1271     return NULL;  // caller does a CHECK_NULL too
1272   }
1273 
1274   assert(loader_data != NULL, "Should never pass around a NULL loader_data. "
1275         "ClassLoaderData::the_null_class_loader_data() should have been used.");
1276 
1277   MetadataType mdtype = (type == MetaspaceObj::ClassType) ? ClassType : NonClassType;
1278 
1279   // Try to allocate metadata.
1280   MetaWord* result = loader_data-&gt;metaspace_non_null()-&gt;allocate(word_size, mdtype);
1281 
1282   if (result == NULL) {
1283     tracer()-&gt;report_metaspace_allocation_failure(loader_data, word_size, type, mdtype);
1284 
1285     // Allocation failed.
1286     if (is_init_completed()) {
1287       // Only start a GC if the bootstrapping has completed.
1288       // Try to clean out some heap memory and retry. This can prevent premature
1289       // expansion of the metaspace.
1290       result = Universe::heap()-&gt;satisfy_failed_metadata_allocation(loader_data, word_size, mdtype);
1291     }
1292   }
1293 
1294   if (result == NULL) {
1295     if (DumpSharedSpaces) {
1296       // CDS dumping keeps loading classes, so if we hit an OOM we probably will keep hitting OOM.
1297       // We should abort to avoid generating a potentially bad archive.
1298       vm_exit_during_cds_dumping(err_msg("Failed allocating metaspace object type %s of size " SIZE_FORMAT ". CDS dump aborted.",
1299           MetaspaceObj::type_name(type), word_size * BytesPerWord),
1300         err_msg("Please increase MaxMetaspaceSize (currently " SIZE_FORMAT " bytes).", MaxMetaspaceSize));
1301     }
1302     report_metadata_oome(loader_data, word_size, type, mdtype, THREAD);
1303     assert(HAS_PENDING_EXCEPTION, "sanity");
1304     return NULL;
1305   }
1306 
1307   // Zero initialize.
1308   Copy::fill_to_words((HeapWord*)result, word_size, 0);
1309 
1310   return result;
1311 }
1312 
1313 void Metaspace::report_metadata_oome(ClassLoaderData* loader_data, size_t word_size, MetaspaceObj::Type type, MetadataType mdtype, TRAPS) {
1314   tracer()-&gt;report_metadata_oom(loader_data, word_size, type, mdtype);
1315 
1316   // If result is still null, we are out of memory.
1317   Log(gc, metaspace, freelist, oom) log;
1318   if (log.is_info()) {
1319     log.info("Metaspace (%s) allocation failed for size " SIZE_FORMAT,
1320              is_class_space_allocation(mdtype) ? "class" : "data", word_size);
1321     ResourceMark rm;
1322     if (log.is_debug()) {
1323       if (loader_data-&gt;metaspace_or_null() != NULL) {
1324         LogStream ls(log.debug());
1325         loader_data-&gt;print_value_on(&amp;ls);
1326       }
1327     }
1328     LogStream ls(log.info());
1329     // In case of an OOM, log out a short but still useful report.
1330     MetaspaceUtils::print_basic_report(&amp;ls, 0);
1331   }
1332 
1333   bool out_of_compressed_class_space = false;
1334   if (is_class_space_allocation(mdtype)) {
1335     ClassLoaderMetaspace* metaspace = loader_data-&gt;metaspace_non_null();
1336     out_of_compressed_class_space =
1337       MetaspaceUtils::committed_bytes(Metaspace::ClassType) +
1338       (metaspace-&gt;class_chunk_size(word_size) * BytesPerWord) &gt;
1339       CompressedClassSpaceSize;
1340   }
1341 
1342   // -XX:+HeapDumpOnOutOfMemoryError and -XX:OnOutOfMemoryError support
1343   const char* space_string = out_of_compressed_class_space ?
1344     "Compressed class space" : "Metaspace";
1345 
1346   report_java_out_of_memory(space_string);
1347 
1348   if (JvmtiExport::should_post_resource_exhausted()) {
1349     JvmtiExport::post_resource_exhausted(
1350         JVMTI_RESOURCE_EXHAUSTED_OOM_ERROR,
1351         space_string);
1352   }
1353 
1354   if (!is_init_completed()) {
1355     vm_exit_during_initialization("OutOfMemoryError", space_string);
1356   }
1357 
1358   if (out_of_compressed_class_space) {
1359     THROW_OOP(Universe::out_of_memory_error_class_metaspace());
1360   } else {
1361     THROW_OOP(Universe::out_of_memory_error_metaspace());
1362   }
1363 }
1364 
1365 const char* Metaspace::metadata_type_name(Metaspace::MetadataType mdtype) {
1366   switch (mdtype) {
1367     case Metaspace::ClassType: return "Class";
1368     case Metaspace::NonClassType: return "Metadata";
1369     default:
1370       assert(false, "Got bad mdtype: %d", (int) mdtype);
1371       return NULL;
1372   }
1373 }
1374 
1375 void Metaspace::purge(MetadataType mdtype) {
1376   get_space_list(mdtype)-&gt;purge(get_chunk_manager(mdtype));
1377 }
1378 
1379 void Metaspace::purge() {
1380   MutexLocker cl(MetaspaceExpand_lock,
1381                  Mutex::_no_safepoint_check_flag);
1382   purge(NonClassType);
1383   if (using_class_space()) {
1384     purge(ClassType);
1385   }
1386 }
1387 
1388 bool Metaspace::contains(const void* ptr) {
1389   if (MetaspaceShared::is_in_shared_metaspace(ptr)) {
1390     return true;
1391   }
1392   return contains_non_shared(ptr);
1393 }
1394 
1395 bool Metaspace::contains_non_shared(const void* ptr) {
1396   if (using_class_space() &amp;&amp; get_space_list(ClassType)-&gt;contains(ptr)) {
1397      return true;
1398   }
1399 
1400   return get_space_list(NonClassType)-&gt;contains(ptr);
1401 }
1402 
1403 // ClassLoaderMetaspace
1404 
1405 ClassLoaderMetaspace::ClassLoaderMetaspace(Mutex* lock, Metaspace::MetaspaceType type)
1406   : _space_type(type)
1407   , _lock(lock)
1408   , _vsm(NULL)
1409   , _class_vsm(NULL)
1410 {
1411   initialize(lock, type);
1412 }
1413 
1414 ClassLoaderMetaspace::~ClassLoaderMetaspace() {
1415   Metaspace::assert_not_frozen();
1416   DEBUG_ONLY(Atomic::inc(&amp;g_internal_statistics.num_metaspace_deaths));
1417   delete _vsm;
1418   if (Metaspace::using_class_space()) {
1419     delete _class_vsm;
1420   }
1421 }
1422 
1423 void ClassLoaderMetaspace::initialize_first_chunk(Metaspace::MetaspaceType type, Metaspace::MetadataType mdtype) {
1424   Metachunk* chunk = get_initialization_chunk(type, mdtype);
1425   if (chunk != NULL) {
1426     // Add to this manager's list of chunks in use and make it the current_chunk().
1427     get_space_manager(mdtype)-&gt;add_chunk(chunk, true);
1428   }
1429 }
1430 
1431 Metachunk* ClassLoaderMetaspace::get_initialization_chunk(Metaspace::MetaspaceType type, Metaspace::MetadataType mdtype) {
1432   size_t chunk_word_size = get_space_manager(mdtype)-&gt;get_initial_chunk_size(type);
1433 
1434   // Get a chunk from the chunk freelist
1435   Metachunk* chunk = Metaspace::get_chunk_manager(mdtype)-&gt;chunk_freelist_allocate(chunk_word_size);
1436 
1437   if (chunk == NULL) {
1438     chunk = Metaspace::get_space_list(mdtype)-&gt;get_new_chunk(chunk_word_size,
1439                                                   get_space_manager(mdtype)-&gt;medium_chunk_bunch());
1440   }
1441 
1442   return chunk;
1443 }
1444 
1445 void ClassLoaderMetaspace::initialize(Mutex* lock, Metaspace::MetaspaceType type) {
1446   Metaspace::verify_global_initialization();
1447 
1448   DEBUG_ONLY(Atomic::inc(&amp;g_internal_statistics.num_metaspace_births));
1449 
1450   // Allocate SpaceManager for metadata objects.
1451   _vsm = new SpaceManager(Metaspace::NonClassType, type, lock);
1452 
1453   if (Metaspace::using_class_space()) {
1454     // Allocate SpaceManager for classes.
1455     _class_vsm = new SpaceManager(Metaspace::ClassType, type, lock);
1456   }
1457 
1458   MutexLocker cl(MetaspaceExpand_lock, Mutex::_no_safepoint_check_flag);
1459 
1460   // Allocate chunk for metadata objects
1461   initialize_first_chunk(type, Metaspace::NonClassType);
1462 
1463   // Allocate chunk for class metadata objects
1464   if (Metaspace::using_class_space()) {
1465     initialize_first_chunk(type, Metaspace::ClassType);
1466   }
1467 }
1468 
1469 MetaWord* ClassLoaderMetaspace::allocate(size_t word_size, Metaspace::MetadataType mdtype) {
1470   Metaspace::assert_not_frozen();
1471 
1472   DEBUG_ONLY(Atomic::inc(&amp;g_internal_statistics.num_allocs));
1473 
1474   // Don't use class_vsm() unless UseCompressedClassPointers is true.
1475   if (Metaspace::is_class_space_allocation(mdtype)) {
1476     return  class_vsm()-&gt;allocate(word_size);
1477   } else {
1478     return  vsm()-&gt;allocate(word_size);
1479   }
1480 }
1481 
1482 MetaWord* ClassLoaderMetaspace::expand_and_allocate(size_t word_size, Metaspace::MetadataType mdtype) {
1483   Metaspace::assert_not_frozen();
1484   size_t delta_bytes = MetaspaceGC::delta_capacity_until_GC(word_size * BytesPerWord);
1485   assert(delta_bytes &gt; 0, "Must be");
1486 
1487   size_t before = 0;
1488   size_t after = 0;
1489   bool can_retry = true;
1490   MetaWord* res;
1491   bool incremented;
1492 
1493   // Each thread increments the HWM at most once. Even if the thread fails to increment
1494   // the HWM, an allocation is still attempted. This is because another thread must then
1495   // have incremented the HWM and therefore the allocation might still succeed.
1496   do {
1497     incremented = MetaspaceGC::inc_capacity_until_GC(delta_bytes, &amp;after, &amp;before, &amp;can_retry);
1498     res = allocate(word_size, mdtype);
1499   } while (!incremented &amp;&amp; res == NULL &amp;&amp; can_retry);
1500 
1501   if (incremented) {
1502     Metaspace::tracer()-&gt;report_gc_threshold(before, after,
1503                                   MetaspaceGCThresholdUpdater::ExpandAndAllocate);
1504     log_trace(gc, metaspace)("Increase capacity to GC from " SIZE_FORMAT " to " SIZE_FORMAT, before, after);
1505   }
1506 
1507   return res;
1508 }
1509 
1510 size_t ClassLoaderMetaspace::allocated_blocks_bytes() const {
1511   return (vsm()-&gt;used_words() +
1512       (Metaspace::using_class_space() ? class_vsm()-&gt;used_words() : 0)) * BytesPerWord;
1513 }
1514 
1515 size_t ClassLoaderMetaspace::allocated_chunks_bytes() const {
1516   return (vsm()-&gt;capacity_words() +
1517       (Metaspace::using_class_space() ? class_vsm()-&gt;capacity_words() : 0)) * BytesPerWord;
1518 }
1519 
1520 void ClassLoaderMetaspace::deallocate(MetaWord* ptr, size_t word_size, bool is_class) {
1521   Metaspace::assert_not_frozen();
1522   assert(!SafepointSynchronize::is_at_safepoint()
1523          || Thread::current()-&gt;is_VM_thread(), "should be the VM thread");
1524 
1525   DEBUG_ONLY(Atomic::inc(&amp;g_internal_statistics.num_external_deallocs));
1526 
1527   MutexLocker ml(vsm()-&gt;lock(), Mutex::_no_safepoint_check_flag);
1528 
1529   if (is_class &amp;&amp; Metaspace::using_class_space()) {
1530     class_vsm()-&gt;deallocate(ptr, word_size);
1531   } else {
1532     vsm()-&gt;deallocate(ptr, word_size);
1533   }
1534 }
1535 
1536 size_t ClassLoaderMetaspace::class_chunk_size(size_t word_size) {
1537   assert(Metaspace::using_class_space(), "Has to use class space");
1538   return class_vsm()-&gt;calc_chunk_size(word_size);
1539 }
1540 
1541 void ClassLoaderMetaspace::print_on(outputStream* out) const {
1542   // Print both class virtual space counts and metaspace.
1543   if (Verbose) {
1544     vsm()-&gt;print_on(out);
1545     if (Metaspace::using_class_space()) {
1546       class_vsm()-&gt;print_on(out);
1547     }
1548   }
1549 }
1550 
1551 void ClassLoaderMetaspace::verify() {
1552   vsm()-&gt;verify();
1553   if (Metaspace::using_class_space()) {
1554     class_vsm()-&gt;verify();
1555   }
1556 }
1557 
1558 void ClassLoaderMetaspace::add_to_statistics_locked(ClassLoaderMetaspaceStatistics* out) const {
1559   assert_lock_strong(lock());
1560   vsm()-&gt;add_to_statistics_locked(&amp;out-&gt;nonclass_sm_stats());
1561   if (Metaspace::using_class_space()) {
1562     class_vsm()-&gt;add_to_statistics_locked(&amp;out-&gt;class_sm_stats());
1563   }
1564 }
1565 
1566 void ClassLoaderMetaspace::add_to_statistics(ClassLoaderMetaspaceStatistics* out) const {
1567   MutexLocker cl(lock(), Mutex::_no_safepoint_check_flag);
1568   add_to_statistics_locked(out);
1569 }
1570 
1571 /////////////// Unit tests ///////////////
1572 
1573 struct chunkmanager_statistics_t {
1574   int num_specialized_chunks;
1575   int num_small_chunks;
1576   int num_medium_chunks;
1577   int num_humongous_chunks;
1578 };
1579 
1580 extern void test_metaspace_retrieve_chunkmanager_statistics(Metaspace::MetadataType mdType, chunkmanager_statistics_t* out) {
1581   ChunkManager* const chunk_manager = Metaspace::get_chunk_manager(mdType);
1582   ChunkManagerStatistics stat;
1583   chunk_manager-&gt;collect_statistics(&amp;stat);
1584   out-&gt;num_specialized_chunks = (int)stat.chunk_stats(SpecializedIndex).num();
1585   out-&gt;num_small_chunks = (int)stat.chunk_stats(SmallIndex).num();
1586   out-&gt;num_medium_chunks = (int)stat.chunk_stats(MediumIndex).num();
1587   out-&gt;num_humongous_chunks = (int)stat.chunk_stats(HumongousIndex).num();
1588 }
1589 
1590 struct chunk_geometry_t {
1591   size_t specialized_chunk_word_size;
1592   size_t small_chunk_word_size;
1593   size_t medium_chunk_word_size;
1594 };
1595 
1596 extern void test_metaspace_retrieve_chunk_geometry(Metaspace::MetadataType mdType, chunk_geometry_t* out) {
1597   if (mdType == Metaspace::NonClassType) {
1598     out-&gt;specialized_chunk_word_size = SpecializedChunk;
1599     out-&gt;small_chunk_word_size = SmallChunk;
1600     out-&gt;medium_chunk_word_size = MediumChunk;
1601   } else {
1602     out-&gt;specialized_chunk_word_size = ClassSpecializedChunk;
1603     out-&gt;small_chunk_word_size = ClassSmallChunk;
1604     out-&gt;medium_chunk_word_size = ClassMediumChunk;
1605   }
1606 }
</pre></body></html>
