<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>New src/hotspot/share/gc/shared/gcVMOperations.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 2005, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "classfile/classLoader.hpp"
  27 #include "classfile/javaClasses.hpp"
  28 #include "gc/shared/allocTracer.hpp"
  29 #include "gc/shared/gcId.hpp"
  30 #include "gc/shared/gcLocker.hpp"
  31 #include "gc/shared/gcVMOperations.hpp"
  32 #include "gc/shared/genCollectedHeap.hpp"
  33 #include "interpreter/oopMapCache.hpp"
  34 #include "logging/log.hpp"
  35 #include "memory/metaspace/classLoaderMetaspace.hpp"
  36 #include "memory/oopFactory.hpp"
  37 #include "memory/universe.hpp"
  38 #include "runtime/handles.inline.hpp"
  39 #include "runtime/init.hpp"
  40 #include "utilities/dtrace.hpp"
  41 #include "utilities/macros.hpp"
  42 #include "utilities/preserveException.hpp"
  43 #if INCLUDE_G1GC
  44 #include "gc/g1/g1CollectedHeap.inline.hpp"
  45 #include "gc/g1/g1Policy.hpp"
  46 #endif // INCLUDE_G1GC
  47 
  48 VM_GC_Operation::~VM_GC_Operation() {
  49   CollectedHeap* ch = Universe::heap();
  50   ch-&gt;soft_ref_policy()-&gt;set_all_soft_refs_clear(false);
  51 }
  52 
  53 // The same dtrace probe can't be inserted in two different files, so we
  54 // have to call it here, so it's only in one file.  Can't create new probes
  55 // for the other file anymore.   The dtrace probes have to remain stable.
  56 void VM_GC_Operation::notify_gc_begin(bool full) {
  57   HOTSPOT_GC_BEGIN(
  58                    full);
  59 }
  60 
  61 void VM_GC_Operation::notify_gc_end() {
  62   HOTSPOT_GC_END();
  63 }
  64 
  65 // Allocations may fail in several threads at about the same time,
  66 // resulting in multiple gc requests.  We only want to do one of them.
  67 // In case a GC locker is active and the need for a GC is already signaled,
  68 // we want to skip this GC attempt altogether, without doing a futile
  69 // safepoint operation.
  70 bool VM_GC_Operation::skip_operation() const {
  71   bool skip = (_gc_count_before != Universe::heap()-&gt;total_collections());
  72   if (_full &amp;&amp; skip) {
  73     skip = (_full_gc_count_before != Universe::heap()-&gt;total_full_collections());
  74   }
  75   if (!skip &amp;&amp; GCLocker::is_active_and_needs_gc()) {
  76     skip = Universe::heap()-&gt;is_maximal_no_gc();
  77     assert(!(skip &amp;&amp; (_gc_cause == GCCause::_gc_locker)),
  78            "GCLocker cannot be active when initiating GC");
  79   }
  80   return skip;
  81 }
  82 
  83 bool VM_GC_Operation::doit_prologue() {
  84   assert(((_gc_cause != GCCause::_no_gc) &amp;&amp;
  85           (_gc_cause != GCCause::_no_cause_specified)), "Illegal GCCause");
  86 
  87   // To be able to handle a GC the VM initialization needs to be completed.
  88   if (!is_init_completed()) {
  89     vm_exit_during_initialization(
  90       err_msg("GC triggered before VM initialization completed. Try increasing "
  91               "NewSize, current value " SIZE_FORMAT "%s.",
  92               byte_size_in_proper_unit(NewSize),
  93               proper_unit_for_byte_size(NewSize)));
  94   }
  95 
  96   // If the GC count has changed someone beat us to the collection
  97   Heap_lock-&gt;lock();
  98 
  99   // Check invocations
 100   if (skip_operation()) {
 101     // skip collection
 102     Heap_lock-&gt;unlock();
 103     _prologue_succeeded = false;
 104   } else {
 105     _prologue_succeeded = true;
 106   }
 107   return _prologue_succeeded;
 108 }
 109 
 110 
 111 void VM_GC_Operation::doit_epilogue() {
 112   // Clean up old interpreter OopMap entries that were replaced
 113   // during the GC thread root traversal.
 114   OopMapCache::cleanup_old_entries();
 115   if (Universe::has_reference_pending_list()) {
 116     Heap_lock-&gt;notify_all();
 117   }
 118   Heap_lock-&gt;unlock();
 119 }
 120 
 121 bool VM_GC_HeapInspection::skip_operation() const {
 122   return false;
 123 }
 124 
 125 bool VM_GC_HeapInspection::collect() {
 126   if (GCLocker::is_active()) {
 127     return false;
 128   }
 129   Universe::heap()-&gt;collect_as_vm_thread(GCCause::_heap_inspection);
 130   return true;
 131 }
 132 
 133 void VM_GC_HeapInspection::doit() {
 134   Universe::heap()-&gt;ensure_parsability(false); // must happen, even if collection does
 135                                                // not happen (e.g. due to GCLocker)
 136                                                // or _full_gc being false
 137   if (_full_gc) {
 138     if (!collect()) {
 139       // The collection attempt was skipped because the gc locker is held.
 140       // The following dump may then be a tad misleading to someone expecting
 141       // only live objects to show up in the dump (see CR 6944195). Just issue
 142       // a suitable warning in that case and do not attempt to do a collection.
 143       // The latter is a subtle point, because even a failed attempt
 144       // to GC will, in fact, induce one in the future, which we
 145       // probably want to avoid in this case because the GC that we may
 146       // be about to attempt holds value for us only
 147       // if it happens now and not if it happens in the eventual
 148       // future.
 149       log_warning(gc)("GC locker is held; pre-dump GC was skipped");
 150     }
 151   }
 152   HeapInspection inspect;
 153   inspect.heap_inspection(_out);
 154 }
 155 
 156 
 157 void VM_GenCollectForAllocation::doit() {
 158   SvcGCMarker sgcm(SvcGCMarker::MINOR);
 159 
 160   GenCollectedHeap* gch = GenCollectedHeap::heap();
 161   GCCauseSetter gccs(gch, _gc_cause);
 162   _result = gch-&gt;satisfy_failed_allocation(_word_size, _tlab);
 163   assert(_result == NULL || gch-&gt;is_in_reserved(_result), "result not in heap");
 164 
 165   if (_result == NULL &amp;&amp; GCLocker::is_active_and_needs_gc()) {
 166     set_gc_locked();
 167   }
 168 }
 169 
 170 void VM_GenCollectFull::doit() {
 171   SvcGCMarker sgcm(SvcGCMarker::FULL);
 172 
 173   GenCollectedHeap* gch = GenCollectedHeap::heap();
 174   GCCauseSetter gccs(gch, _gc_cause);
 175   gch-&gt;do_full_collection(gch-&gt;must_clear_all_soft_refs(), _max_generation);
 176 }
 177 
 178 VM_CollectForMetadataAllocation::VM_CollectForMetadataAllocation(ClassLoaderData* loader_data,
 179                                                                  size_t size,
 180                                                                  Metaspace::MetadataType mdtype,
 181                                                                  uint gc_count_before,
 182                                                                  uint full_gc_count_before,
 183                                                                  GCCause::Cause gc_cause)
 184     : VM_GC_Operation(gc_count_before, gc_cause, full_gc_count_before, true),
 185       _result(NULL), _size(size), _mdtype(mdtype), _loader_data(loader_data) {
 186   assert(_size != 0, "An allocation should always be requested with this operation.");
 187   AllocTracer::send_allocation_requiring_gc_event(_size * HeapWordSize, GCId::peek());
 188 }
 189 
 190 // Returns true iff concurrent GCs unloads metadata.
 191 bool VM_CollectForMetadataAllocation::initiate_concurrent_GC() {
 192 #if INCLUDE_G1GC
 193   if (UseG1GC &amp;&amp; ClassUnloadingWithConcurrentMark) {
 194     G1CollectedHeap* g1h = G1CollectedHeap::heap();
 195     g1h-&gt;policy()-&gt;collector_state()-&gt;set_initiate_conc_mark_if_possible(true);
 196 
 197     GCCauseSetter x(g1h, _gc_cause);
 198 
 199     // At this point we are supposed to start a concurrent cycle. We
 200     // will do so if one is not already in progress.
 201     bool should_start = g1h-&gt;policy()-&gt;force_concurrent_start_if_outside_cycle(_gc_cause);
 202 
 203     if (should_start) {
 204       double pause_target = g1h-&gt;policy()-&gt;max_pause_time_ms();
 205       g1h-&gt;do_collection_pause_at_safepoint(pause_target);
 206     }
 207     return true;
 208   }
 209 #endif
 210 
 211   return false;
 212 }
 213 
 214 void VM_CollectForMetadataAllocation::doit() {
 215   SvcGCMarker sgcm(SvcGCMarker::FULL);
 216 
 217   CollectedHeap* heap = Universe::heap();
 218   GCCauseSetter gccs(heap, _gc_cause);
 219 
 220   // Check again if the space is available.  Another thread
 221   // may have similarly failed a metadata allocation and induced
 222   // a GC that freed space for the allocation.
 223   if (!MetadataAllocationFailALot) {
 224     _result = _loader_data-&gt;metaspace_non_null()-&gt;allocate(_size, _mdtype);
 225     if (_result != NULL) {
 226       return;
 227     }
 228   }
 229 
 230   if (initiate_concurrent_GC()) {
 231     // For G1 expand since the collection is going to be concurrent.
 232     _result = _loader_data-&gt;metaspace_non_null()-&gt;expand_and_allocate(_size, _mdtype);
 233     if (_result != NULL) {
 234       return;
 235     }
 236 
 237     log_debug(gc)("G1 full GC for Metaspace");
 238   }
 239 
 240   // Don't clear the soft refs yet.
 241   heap-&gt;collect_as_vm_thread(GCCause::_metadata_GC_threshold);
 242   // After a GC try to allocate without expanding.  Could fail
 243   // and expansion will be tried below.
 244   _result = _loader_data-&gt;metaspace_non_null()-&gt;allocate(_size, _mdtype);
 245   if (_result != NULL) {
 246     return;
 247   }
 248 
 249   // If still failing, allow the Metaspace to expand.
 250   // See delta_capacity_until_GC() for explanation of the
 251   // amount of the expansion.
 252   // This should work unless there really is no more space
 253   // or a MaxMetaspaceSize has been specified on the command line.
 254   _result = _loader_data-&gt;metaspace_non_null()-&gt;expand_and_allocate(_size, _mdtype);
 255   if (_result != NULL) {
 256     return;
 257   }
 258 
 259   // If expansion failed, do a collection clearing soft references.
 260   heap-&gt;collect_as_vm_thread(GCCause::_metadata_GC_clear_soft_refs);
 261   _result = _loader_data-&gt;metaspace_non_null()-&gt;allocate(_size, _mdtype);
 262   if (_result != NULL) {
 263     return;
 264   }
 265 
 266   log_debug(gc)("After Metaspace GC failed to allocate size " SIZE_FORMAT, _size);
 267 
 268   if (GCLocker::is_active_and_needs_gc()) {
 269     set_gc_locked();
 270   }
 271 }
 272 
 273 VM_CollectForAllocation::VM_CollectForAllocation(size_t word_size, uint gc_count_before, GCCause::Cause cause)
 274     : VM_GC_Operation(gc_count_before, cause), _word_size(word_size), _result(NULL) {
 275   // Only report if operation was really caused by an allocation.
 276   if (_word_size != 0) {
 277     AllocTracer::send_allocation_requiring_gc_event(_word_size * HeapWordSize, GCId::peek());
 278   }
 279 }
</pre></body></html>
