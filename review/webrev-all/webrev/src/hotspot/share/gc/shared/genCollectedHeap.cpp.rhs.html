<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre>rev <a href="https://bugs.openjdk.java.net/browse/JDK-60538">60538</a> : imported patch jep387-all.patch</pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "aot/aotLoader.hpp"
  27 #include "classfile/classLoaderDataGraph.hpp"
  28 #include "classfile/symbolTable.hpp"
  29 #include "classfile/stringTable.hpp"
  30 #include "classfile/vmSymbols.hpp"
  31 #include "code/codeCache.hpp"
  32 #include "code/icBuffer.hpp"
  33 #include "gc/serial/defNewGeneration.hpp"
  34 #include "gc/shared/adaptiveSizePolicy.hpp"
  35 #include "gc/shared/cardTableBarrierSet.hpp"
  36 #include "gc/shared/cardTableRS.hpp"
  37 #include "gc/shared/collectedHeap.inline.hpp"
  38 #include "gc/shared/collectorCounters.hpp"
  39 #include "gc/shared/gcId.hpp"
  40 #include "gc/shared/gcLocker.hpp"
  41 #include "gc/shared/gcPolicyCounters.hpp"
  42 #include "gc/shared/gcTrace.hpp"
  43 #include "gc/shared/gcTraceTime.inline.hpp"
  44 #include "gc/shared/genArguments.hpp"
  45 #include "gc/shared/gcVMOperations.hpp"
  46 #include "gc/shared/genCollectedHeap.hpp"
  47 #include "gc/shared/genOopClosures.inline.hpp"
  48 #include "gc/shared/generationSpec.hpp"
  49 #include "gc/shared/gcInitLogger.hpp"
  50 #include "gc/shared/locationPrinter.inline.hpp"
  51 #include "gc/shared/oopStorage.inline.hpp"
  52 #include "gc/shared/oopStorageSet.inline.hpp"
  53 #include "gc/shared/oopStorageParState.inline.hpp"
  54 #include "gc/shared/scavengableNMethods.hpp"
  55 #include "gc/shared/space.hpp"
  56 #include "gc/shared/strongRootsScope.hpp"
  57 #include "gc/shared/weakProcessor.hpp"
  58 #include "gc/shared/workgroup.hpp"
  59 #include "memory/filemap.hpp"
  60 #include "memory/iterator.hpp"
  61 #include "memory/metaspaceCounters.hpp"
<a name="1" id="anc1"></a><span class="new">  62 #include "memory/metaspace/metaspaceSizesSnapshot.hpp"</span>
  63 #include "memory/resourceArea.hpp"
  64 #include "memory/universe.hpp"
  65 #include "oops/oop.inline.hpp"
  66 #include "runtime/biasedLocking.hpp"
  67 #include "runtime/handles.hpp"
  68 #include "runtime/handles.inline.hpp"
  69 #include "runtime/java.hpp"
  70 #include "runtime/vmThread.hpp"
  71 #include "services/memoryService.hpp"
  72 #include "utilities/autoRestore.hpp"
  73 #include "utilities/debug.hpp"
  74 #include "utilities/formatBuffer.hpp"
  75 #include "utilities/macros.hpp"
  76 #include "utilities/stack.inline.hpp"
  77 #include "utilities/vmError.hpp"
  78 #if INCLUDE_JVMCI
  79 #include "jvmci/jvmci.hpp"
  80 #endif
  81 
  82 GenCollectedHeap::GenCollectedHeap(Generation::Name young,
  83                                    Generation::Name old,
  84                                    const char* policy_counters_name) :
  85   CollectedHeap(),
  86   _young_gen(NULL),
  87   _old_gen(NULL),
  88   _young_gen_spec(new GenerationSpec(young,
  89                                      NewSize,
  90                                      MaxNewSize,
  91                                      GenAlignment)),
  92   _old_gen_spec(new GenerationSpec(old,
  93                                    OldSize,
  94                                    MaxOldSize,
  95                                    GenAlignment)),
  96   _rem_set(NULL),
  97   _soft_ref_gen_policy(),
  98   _size_policy(NULL),
  99   _gc_policy_counters(new GCPolicyCounters(policy_counters_name, 2, 2)),
 100   _incremental_collection_failed(false),
 101   _full_collections_completed(0),
 102   _process_strong_tasks(new SubTasksDone(GCH_PS_NumElements)),
 103   _young_manager(NULL),
 104   _old_manager(NULL) {
 105 }
 106 
 107 jint GenCollectedHeap::initialize() {
 108   // While there are no constraints in the GC code that HeapWordSize
 109   // be any particular value, there are multiple other areas in the
 110   // system which believe this to be true (e.g. oop-&gt;object_size in some
 111   // cases incorrectly returns the size in wordSize units rather than
 112   // HeapWordSize).
 113   guarantee(HeapWordSize == wordSize, "HeapWordSize must equal wordSize");
 114 
 115   // Allocate space for the heap.
 116 
 117   ReservedHeapSpace heap_rs = allocate(HeapAlignment);
 118 
 119   if (!heap_rs.is_reserved()) {
 120     vm_shutdown_during_initialization(
 121       "Could not reserve enough space for object heap");
 122     return JNI_ENOMEM;
 123   }
 124 
 125   initialize_reserved_region(heap_rs);
 126 
 127   _rem_set = create_rem_set(heap_rs.region());
 128   _rem_set-&gt;initialize();
 129   CardTableBarrierSet *bs = new CardTableBarrierSet(_rem_set);
 130   bs-&gt;initialize();
 131   BarrierSet::set_barrier_set(bs);
 132 
 133   ReservedSpace young_rs = heap_rs.first_part(_young_gen_spec-&gt;max_size());
 134   _young_gen = _young_gen_spec-&gt;init(young_rs, rem_set());
 135   ReservedSpace old_rs = heap_rs.last_part(_young_gen_spec-&gt;max_size());
 136 
 137   old_rs = old_rs.first_part(_old_gen_spec-&gt;max_size());
 138   _old_gen = _old_gen_spec-&gt;init(old_rs, rem_set());
 139 
 140   GCInitLogger::print();
 141 
 142   return JNI_OK;
 143 }
 144 
 145 CardTableRS* GenCollectedHeap::create_rem_set(const MemRegion&amp; reserved_region) {
 146   return new CardTableRS(reserved_region, false /* scan_concurrently */);
 147 }
 148 
 149 void GenCollectedHeap::initialize_size_policy(size_t init_eden_size,
 150                                               size_t init_promo_size,
 151                                               size_t init_survivor_size) {
 152   const double max_gc_pause_sec = ((double) MaxGCPauseMillis) / 1000.0;
 153   _size_policy = new AdaptiveSizePolicy(init_eden_size,
 154                                         init_promo_size,
 155                                         init_survivor_size,
 156                                         max_gc_pause_sec,
 157                                         GCTimeRatio);
 158 }
 159 
 160 ReservedHeapSpace GenCollectedHeap::allocate(size_t alignment) {
 161   // Now figure out the total size.
 162   const size_t pageSize = UseLargePages ? os::large_page_size() : os::vm_page_size();
 163   assert(alignment % pageSize == 0, "Must be");
 164 
 165   // Check for overflow.
 166   size_t total_reserved = _young_gen_spec-&gt;max_size() + _old_gen_spec-&gt;max_size();
 167   if (total_reserved &lt; _young_gen_spec-&gt;max_size()) {
 168     vm_exit_during_initialization("The size of the object heap + VM data exceeds "
 169                                   "the maximum representable size");
 170   }
 171   assert(total_reserved % alignment == 0,
 172          "Gen size; total_reserved=" SIZE_FORMAT ", alignment="
 173          SIZE_FORMAT, total_reserved, alignment);
 174 
 175   ReservedHeapSpace heap_rs = Universe::reserve_heap(total_reserved, alignment);
 176 
 177   os::trace_page_sizes("Heap",
 178                        MinHeapSize,
 179                        total_reserved,
 180                        alignment,
 181                        heap_rs.base(),
 182                        heap_rs.size());
 183 
 184   return heap_rs;
 185 }
 186 
 187 class GenIsScavengable : public BoolObjectClosure {
 188 public:
 189   bool do_object_b(oop obj) {
 190     return GenCollectedHeap::heap()-&gt;is_in_young(obj);
 191   }
 192 };
 193 
 194 static GenIsScavengable _is_scavengable;
 195 
 196 void GenCollectedHeap::post_initialize() {
 197   CollectedHeap::post_initialize();
 198   ref_processing_init();
 199 
 200   DefNewGeneration* def_new_gen = (DefNewGeneration*)_young_gen;
 201 
 202   initialize_size_policy(def_new_gen-&gt;eden()-&gt;capacity(),
 203                          _old_gen-&gt;capacity(),
 204                          def_new_gen-&gt;from()-&gt;capacity());
 205 
 206   MarkSweep::initialize();
 207 
 208   ScavengableNMethods::initialize(&amp;_is_scavengable);
 209 }
 210 
 211 void GenCollectedHeap::ref_processing_init() {
 212   _young_gen-&gt;ref_processor_init();
 213   _old_gen-&gt;ref_processor_init();
 214 }
 215 
 216 PreGenGCValues GenCollectedHeap::get_pre_gc_values() const {
 217   const DefNewGeneration* const def_new_gen = (DefNewGeneration*) young_gen();
 218 
 219   return PreGenGCValues(def_new_gen-&gt;used(),
 220                         def_new_gen-&gt;capacity(),
 221                         def_new_gen-&gt;eden()-&gt;used(),
 222                         def_new_gen-&gt;eden()-&gt;capacity(),
 223                         def_new_gen-&gt;from()-&gt;used(),
 224                         def_new_gen-&gt;from()-&gt;capacity(),
 225                         old_gen()-&gt;used(),
 226                         old_gen()-&gt;capacity());
 227 }
 228 
 229 GenerationSpec* GenCollectedHeap::young_gen_spec() const {
 230   return _young_gen_spec;
 231 }
 232 
 233 GenerationSpec* GenCollectedHeap::old_gen_spec() const {
 234   return _old_gen_spec;
 235 }
 236 
 237 size_t GenCollectedHeap::capacity() const {
 238   return _young_gen-&gt;capacity() + _old_gen-&gt;capacity();
 239 }
 240 
 241 size_t GenCollectedHeap::used() const {
 242   return _young_gen-&gt;used() + _old_gen-&gt;used();
 243 }
 244 
 245 void GenCollectedHeap::save_used_regions() {
 246   _old_gen-&gt;save_used_region();
 247   _young_gen-&gt;save_used_region();
 248 }
 249 
 250 size_t GenCollectedHeap::max_capacity() const {
 251   return _young_gen-&gt;max_capacity() + _old_gen-&gt;max_capacity();
 252 }
 253 
 254 // Update the _full_collections_completed counter
 255 // at the end of a stop-world full GC.
 256 unsigned int GenCollectedHeap::update_full_collections_completed() {
 257   MonitorLocker ml(FullGCCount_lock, Mutex::_no_safepoint_check_flag);
 258   assert(_full_collections_completed &lt;= _total_full_collections,
 259          "Can't complete more collections than were started");
 260   _full_collections_completed = _total_full_collections;
 261   ml.notify_all();
 262   return _full_collections_completed;
 263 }
 264 
 265 // Update the _full_collections_completed counter, as appropriate,
 266 // at the end of a concurrent GC cycle. Note the conditional update
 267 // below to allow this method to be called by a concurrent collector
 268 // without synchronizing in any manner with the VM thread (which
 269 // may already have initiated a STW full collection "concurrently").
 270 unsigned int GenCollectedHeap::update_full_collections_completed(unsigned int count) {
 271   MonitorLocker ml(FullGCCount_lock, Mutex::_no_safepoint_check_flag);
 272   assert((_full_collections_completed &lt;= _total_full_collections) &amp;&amp;
 273          (count &lt;= _total_full_collections),
 274          "Can't complete more collections than were started");
 275   if (count &gt; _full_collections_completed) {
 276     _full_collections_completed = count;
 277     ml.notify_all();
 278   }
 279   return _full_collections_completed;
 280 }
 281 
 282 // Return true if any of the following is true:
 283 // . the allocation won't fit into the current young gen heap
 284 // . gc locker is occupied (jni critical section)
 285 // . heap memory is tight -- the most recent previous collection
 286 //   was a full collection because a partial collection (would
 287 //   have) failed and is likely to fail again
 288 bool GenCollectedHeap::should_try_older_generation_allocation(size_t word_size) const {
 289   size_t young_capacity = _young_gen-&gt;capacity_before_gc();
 290   return    (word_size &gt; heap_word_size(young_capacity))
 291          || GCLocker::is_active_and_needs_gc()
 292          || incremental_collection_failed();
 293 }
 294 
 295 HeapWord* GenCollectedHeap::expand_heap_and_allocate(size_t size, bool   is_tlab) {
 296   HeapWord* result = NULL;
 297   if (_old_gen-&gt;should_allocate(size, is_tlab)) {
 298     result = _old_gen-&gt;expand_and_allocate(size, is_tlab);
 299   }
 300   if (result == NULL) {
 301     if (_young_gen-&gt;should_allocate(size, is_tlab)) {
 302       result = _young_gen-&gt;expand_and_allocate(size, is_tlab);
 303     }
 304   }
 305   assert(result == NULL || is_in_reserved(result), "result not in heap");
 306   return result;
 307 }
 308 
 309 HeapWord* GenCollectedHeap::mem_allocate_work(size_t size,
 310                                               bool is_tlab,
 311                                               bool* gc_overhead_limit_was_exceeded) {
 312   // In general gc_overhead_limit_was_exceeded should be false so
 313   // set it so here and reset it to true only if the gc time
 314   // limit is being exceeded as checked below.
 315   *gc_overhead_limit_was_exceeded = false;
 316 
 317   HeapWord* result = NULL;
 318 
 319   // Loop until the allocation is satisfied, or unsatisfied after GC.
 320   for (uint try_count = 1, gclocker_stalled_count = 0; /* return or throw */; try_count += 1) {
 321 
 322     // First allocation attempt is lock-free.
 323     Generation *young = _young_gen;
 324     assert(young-&gt;supports_inline_contig_alloc(),
 325       "Otherwise, must do alloc within heap lock");
 326     if (young-&gt;should_allocate(size, is_tlab)) {
 327       result = young-&gt;par_allocate(size, is_tlab);
 328       if (result != NULL) {
 329         assert(is_in_reserved(result), "result not in heap");
 330         return result;
 331       }
 332     }
 333     uint gc_count_before;  // Read inside the Heap_lock locked region.
 334     {
 335       MutexLocker ml(Heap_lock);
 336       log_trace(gc, alloc)("GenCollectedHeap::mem_allocate_work: attempting locked slow path allocation");
 337       // Note that only large objects get a shot at being
 338       // allocated in later generations.
 339       bool first_only = !should_try_older_generation_allocation(size);
 340 
 341       result = attempt_allocation(size, is_tlab, first_only);
 342       if (result != NULL) {
 343         assert(is_in_reserved(result), "result not in heap");
 344         return result;
 345       }
 346 
 347       if (GCLocker::is_active_and_needs_gc()) {
 348         if (is_tlab) {
 349           return NULL;  // Caller will retry allocating individual object.
 350         }
 351         if (!is_maximal_no_gc()) {
 352           // Try and expand heap to satisfy request.
 353           result = expand_heap_and_allocate(size, is_tlab);
 354           // Result could be null if we are out of space.
 355           if (result != NULL) {
 356             return result;
 357           }
 358         }
 359 
 360         if (gclocker_stalled_count &gt; GCLockerRetryAllocationCount) {
 361           return NULL; // We didn't get to do a GC and we didn't get any memory.
 362         }
 363 
 364         // If this thread is not in a jni critical section, we stall
 365         // the requestor until the critical section has cleared and
 366         // GC allowed. When the critical section clears, a GC is
 367         // initiated by the last thread exiting the critical section; so
 368         // we retry the allocation sequence from the beginning of the loop,
 369         // rather than causing more, now probably unnecessary, GC attempts.
 370         JavaThread* jthr = JavaThread::current();
 371         if (!jthr-&gt;in_critical()) {
 372           MutexUnlocker mul(Heap_lock);
 373           // Wait for JNI critical section to be exited
 374           GCLocker::stall_until_clear();
 375           gclocker_stalled_count += 1;
 376           continue;
 377         } else {
 378           if (CheckJNICalls) {
 379             fatal("Possible deadlock due to allocating while"
 380                   " in jni critical section");
 381           }
 382           return NULL;
 383         }
 384       }
 385 
 386       // Read the gc count while the heap lock is held.
 387       gc_count_before = total_collections();
 388     }
 389 
 390     VM_GenCollectForAllocation op(size, is_tlab, gc_count_before);
 391     VMThread::execute(&amp;op);
 392     if (op.prologue_succeeded()) {
 393       result = op.result();
 394       if (op.gc_locked()) {
 395          assert(result == NULL, "must be NULL if gc_locked() is true");
 396          continue;  // Retry and/or stall as necessary.
 397       }
 398 
 399       // Allocation has failed and a collection
 400       // has been done.  If the gc time limit was exceeded the
 401       // this time, return NULL so that an out-of-memory
 402       // will be thrown.  Clear gc_overhead_limit_exceeded
 403       // so that the overhead exceeded does not persist.
 404 
 405       const bool limit_exceeded = size_policy()-&gt;gc_overhead_limit_exceeded();
 406       const bool softrefs_clear = soft_ref_policy()-&gt;all_soft_refs_clear();
 407 
 408       if (limit_exceeded &amp;&amp; softrefs_clear) {
 409         *gc_overhead_limit_was_exceeded = true;
 410         size_policy()-&gt;set_gc_overhead_limit_exceeded(false);
 411         if (op.result() != NULL) {
 412           CollectedHeap::fill_with_object(op.result(), size);
 413         }
 414         return NULL;
 415       }
 416       assert(result == NULL || is_in_reserved(result),
 417              "result not in heap");
 418       return result;
 419     }
 420 
 421     // Give a warning if we seem to be looping forever.
 422     if ((QueuedAllocationWarningCount &gt; 0) &amp;&amp;
 423         (try_count % QueuedAllocationWarningCount == 0)) {
 424           log_warning(gc, ergo)("GenCollectedHeap::mem_allocate_work retries %d times,"
 425                                 " size=" SIZE_FORMAT " %s", try_count, size, is_tlab ? "(TLAB)" : "");
 426     }
 427   }
 428 }
 429 
 430 HeapWord* GenCollectedHeap::attempt_allocation(size_t size,
 431                                                bool is_tlab,
 432                                                bool first_only) {
 433   HeapWord* res = NULL;
 434 
 435   if (_young_gen-&gt;should_allocate(size, is_tlab)) {
 436     res = _young_gen-&gt;allocate(size, is_tlab);
 437     if (res != NULL || first_only) {
 438       return res;
 439     }
 440   }
 441 
 442   if (_old_gen-&gt;should_allocate(size, is_tlab)) {
 443     res = _old_gen-&gt;allocate(size, is_tlab);
 444   }
 445 
 446   return res;
 447 }
 448 
 449 HeapWord* GenCollectedHeap::mem_allocate(size_t size,
 450                                          bool* gc_overhead_limit_was_exceeded) {
 451   return mem_allocate_work(size,
 452                            false /* is_tlab */,
 453                            gc_overhead_limit_was_exceeded);
 454 }
 455 
 456 bool GenCollectedHeap::must_clear_all_soft_refs() {
 457   return _gc_cause == GCCause::_metadata_GC_clear_soft_refs ||
 458          _gc_cause == GCCause::_wb_full_gc;
 459 }
 460 
 461 void GenCollectedHeap::collect_generation(Generation* gen, bool full, size_t size,
 462                                           bool is_tlab, bool run_verification, bool clear_soft_refs,
 463                                           bool restore_marks_for_biased_locking) {
 464   FormatBuffer&lt;&gt; title("Collect gen: %s", gen-&gt;short_name());
 465   GCTraceTime(Trace, gc, phases) t1(title);
 466   TraceCollectorStats tcs(gen-&gt;counters());
 467   TraceMemoryManagerStats tmms(gen-&gt;gc_manager(), gc_cause());
 468 
 469   gen-&gt;stat_record()-&gt;invocations++;
 470   gen-&gt;stat_record()-&gt;accumulated_time.start();
 471 
 472   // Must be done anew before each collection because
 473   // a previous collection will do mangling and will
 474   // change top of some spaces.
 475   record_gen_tops_before_GC();
 476 
 477   log_trace(gc)("%s invoke=%d size=" SIZE_FORMAT, heap()-&gt;is_young_gen(gen) ? "Young" : "Old", gen-&gt;stat_record()-&gt;invocations, size * HeapWordSize);
 478 
 479   if (run_verification &amp;&amp; VerifyBeforeGC) {
 480     Universe::verify("Before GC");
 481   }
 482   COMPILER2_OR_JVMCI_PRESENT(DerivedPointerTable::clear());
 483 
 484   if (restore_marks_for_biased_locking) {
 485     // We perform this mark word preservation work lazily
 486     // because it's only at this point that we know whether we
 487     // absolutely have to do it; we want to avoid doing it for
 488     // scavenge-only collections where it's unnecessary
 489     BiasedLocking::preserve_marks();
 490   }
 491 
 492   // Do collection work
 493   {
 494     // Note on ref discovery: For what appear to be historical reasons,
 495     // GCH enables and disabled (by enqueing) refs discovery.
 496     // In the future this should be moved into the generation's
 497     // collect method so that ref discovery and enqueueing concerns
 498     // are local to a generation. The collect method could return
 499     // an appropriate indication in the case that notification on
 500     // the ref lock was needed. This will make the treatment of
 501     // weak refs more uniform (and indeed remove such concerns
 502     // from GCH). XXX
 503 
 504     save_marks();   // save marks for all gens
 505     // We want to discover references, but not process them yet.
 506     // This mode is disabled in process_discovered_references if the
 507     // generation does some collection work, or in
 508     // enqueue_discovered_references if the generation returns
 509     // without doing any work.
 510     ReferenceProcessor* rp = gen-&gt;ref_processor();
 511     // If the discovery of ("weak") refs in this generation is
 512     // atomic wrt other collectors in this configuration, we
 513     // are guaranteed to have empty discovered ref lists.
 514     if (rp-&gt;discovery_is_atomic()) {
 515       rp-&gt;enable_discovery();
 516       rp-&gt;setup_policy(clear_soft_refs);
 517     } else {
 518       // collect() below will enable discovery as appropriate
 519     }
 520     gen-&gt;collect(full, clear_soft_refs, size, is_tlab);
 521     if (!rp-&gt;enqueuing_is_done()) {
 522       rp-&gt;disable_discovery();
 523     } else {
 524       rp-&gt;set_enqueuing_is_done(false);
 525     }
 526     rp-&gt;verify_no_references_recorded();
 527   }
 528 
 529   COMPILER2_OR_JVMCI_PRESENT(DerivedPointerTable::update_pointers());
 530 
 531   gen-&gt;stat_record()-&gt;accumulated_time.stop();
 532 
 533   update_gc_stats(gen, full);
 534 
 535   if (run_verification &amp;&amp; VerifyAfterGC) {
 536     Universe::verify("After GC");
 537   }
 538 }
 539 
 540 void GenCollectedHeap::do_collection(bool           full,
 541                                      bool           clear_all_soft_refs,
 542                                      size_t         size,
 543                                      bool           is_tlab,
 544                                      GenerationType max_generation) {
 545   ResourceMark rm;
 546   DEBUG_ONLY(Thread* my_thread = Thread::current();)
 547 
 548   assert(SafepointSynchronize::is_at_safepoint(), "should be at safepoint");
 549   assert(my_thread-&gt;is_VM_thread() ||
 550          my_thread-&gt;is_ConcurrentGC_thread(),
 551          "incorrect thread type capability");
 552   assert(Heap_lock-&gt;is_locked(),
 553          "the requesting thread should have the Heap_lock");
 554   guarantee(!is_gc_active(), "collection is not reentrant");
 555 
 556   if (GCLocker::check_active_before_gc()) {
 557     return; // GC is disabled (e.g. JNI GetXXXCritical operation)
 558   }
 559 
 560   const bool do_clear_all_soft_refs = clear_all_soft_refs ||
 561                           soft_ref_policy()-&gt;should_clear_all_soft_refs();
 562 
 563   ClearedAllSoftRefs casr(do_clear_all_soft_refs, soft_ref_policy());
 564 
 565   AutoModifyRestore&lt;bool&gt; temporarily(_is_gc_active, true);
 566 
 567   bool complete = full &amp;&amp; (max_generation == OldGen);
 568   bool old_collects_young = complete &amp;&amp; !ScavengeBeforeFullGC;
 569   bool do_young_collection = !old_collects_young &amp;&amp; _young_gen-&gt;should_collect(full, size, is_tlab);
 570 
 571   const PreGenGCValues pre_gc_values = get_pre_gc_values();
 572 
 573   bool run_verification = total_collections() &gt;= VerifyGCStartAt;
 574   bool prepared_for_verification = false;
 575   bool do_full_collection = false;
 576 
 577   if (do_young_collection) {
 578     GCIdMark gc_id_mark;
 579     GCTraceCPUTime tcpu;
 580     GCTraceTime(Info, gc) t("Pause Young", NULL, gc_cause(), true);
 581 
 582     print_heap_before_gc();
 583 
 584     if (run_verification &amp;&amp; VerifyGCLevel &lt;= 0 &amp;&amp; VerifyBeforeGC) {
 585       prepare_for_verify();
 586       prepared_for_verification = true;
 587     }
 588 
 589     gc_prologue(complete);
 590     increment_total_collections(complete);
 591 
 592     collect_generation(_young_gen,
 593                        full,
 594                        size,
 595                        is_tlab,
 596                        run_verification &amp;&amp; VerifyGCLevel &lt;= 0,
 597                        do_clear_all_soft_refs,
 598                        false);
 599 
 600     if (size &gt; 0 &amp;&amp; (!is_tlab || _young_gen-&gt;supports_tlab_allocation()) &amp;&amp;
 601         size * HeapWordSize &lt;= _young_gen-&gt;unsafe_max_alloc_nogc()) {
 602       // Allocation request was met by young GC.
 603       size = 0;
 604     }
 605 
 606     // Ask if young collection is enough. If so, do the final steps for young collection,
 607     // and fallthrough to the end.
 608     do_full_collection = should_do_full_collection(size, full, is_tlab, max_generation);
 609     if (!do_full_collection) {
 610       // Adjust generation sizes.
 611       _young_gen-&gt;compute_new_size();
 612 
 613       print_heap_change(pre_gc_values);
 614 
 615       // Track memory usage and detect low memory after GC finishes
 616       MemoryService::track_memory_usage();
 617 
 618       gc_epilogue(complete);
 619     }
 620 
 621     print_heap_after_gc();
 622 
 623   } else {
 624     // No young collection, ask if we need to perform Full collection.
 625     do_full_collection = should_do_full_collection(size, full, is_tlab, max_generation);
 626   }
 627 
 628   if (do_full_collection) {
 629     GCIdMark gc_id_mark;
 630     GCTraceCPUTime tcpu;
 631     GCTraceTime(Info, gc) t("Pause Full", NULL, gc_cause(), true);
 632 
 633     print_heap_before_gc();
 634 
 635     if (!prepared_for_verification &amp;&amp; run_verification &amp;&amp;
 636         VerifyGCLevel &lt;= 1 &amp;&amp; VerifyBeforeGC) {
 637       prepare_for_verify();
 638     }
 639 
 640     if (!do_young_collection) {
 641       gc_prologue(complete);
 642       increment_total_collections(complete);
 643     }
 644 
 645     // Accounting quirk: total full collections would be incremented when "complete"
 646     // is set, by calling increment_total_collections above. However, we also need to
 647     // account Full collections that had "complete" unset.
 648     if (!complete) {
 649       increment_total_full_collections();
 650     }
 651 
 652     collect_generation(_old_gen,
 653                        full,
 654                        size,
 655                        is_tlab,
 656                        run_verification &amp;&amp; VerifyGCLevel &lt;= 1,
 657                        do_clear_all_soft_refs,
 658                        true);
 659 
 660     // Adjust generation sizes.
 661     _old_gen-&gt;compute_new_size();
 662     _young_gen-&gt;compute_new_size();
 663 
 664     // Delete metaspaces for unloaded class loaders and clean up loader_data graph
 665     ClassLoaderDataGraph::purge();
<a name="2" id="anc2"></a><span class="changed"> 666     DEBUG_ONLY(MetaspaceUtils::verify(false);)</span>
 667     // Resize the metaspace capacity after full collections
 668     MetaspaceGC::compute_new_size();
 669     update_full_collections_completed();
 670 
 671     print_heap_change(pre_gc_values);
 672 
 673     // Track memory usage and detect low memory after GC finishes
 674     MemoryService::track_memory_usage();
 675 
 676     // Need to tell the epilogue code we are done with Full GC, regardless what was
 677     // the initial value for "complete" flag.
 678     gc_epilogue(true);
 679 
 680     BiasedLocking::restore_marks();
 681 
 682     print_heap_after_gc();
 683   }
 684 }
 685 
 686 bool GenCollectedHeap::should_do_full_collection(size_t size, bool full, bool is_tlab,
 687                                                  GenCollectedHeap::GenerationType max_gen) const {
 688   return max_gen == OldGen &amp;&amp; _old_gen-&gt;should_collect(full, size, is_tlab);
 689 }
 690 
 691 void GenCollectedHeap::register_nmethod(nmethod* nm) {
 692   ScavengableNMethods::register_nmethod(nm);
 693 }
 694 
 695 void GenCollectedHeap::unregister_nmethod(nmethod* nm) {
 696   ScavengableNMethods::unregister_nmethod(nm);
 697 }
 698 
 699 void GenCollectedHeap::verify_nmethod(nmethod* nm) {
 700   ScavengableNMethods::verify_nmethod(nm);
 701 }
 702 
 703 void GenCollectedHeap::flush_nmethod(nmethod* nm) {
 704   // Do nothing.
 705 }
 706 
 707 void GenCollectedHeap::prune_scavengable_nmethods() {
 708   ScavengableNMethods::prune_nmethods();
 709 }
 710 
 711 HeapWord* GenCollectedHeap::satisfy_failed_allocation(size_t size, bool is_tlab) {
 712   GCCauseSetter x(this, GCCause::_allocation_failure);
 713   HeapWord* result = NULL;
 714 
 715   assert(size != 0, "Precondition violated");
 716   if (GCLocker::is_active_and_needs_gc()) {
 717     // GC locker is active; instead of a collection we will attempt
 718     // to expand the heap, if there's room for expansion.
 719     if (!is_maximal_no_gc()) {
 720       result = expand_heap_and_allocate(size, is_tlab);
 721     }
 722     return result;   // Could be null if we are out of space.
 723   } else if (!incremental_collection_will_fail(false /* don't consult_young */)) {
 724     // Do an incremental collection.
 725     do_collection(false,                     // full
 726                   false,                     // clear_all_soft_refs
 727                   size,                      // size
 728                   is_tlab,                   // is_tlab
 729                   GenCollectedHeap::OldGen); // max_generation
 730   } else {
 731     log_trace(gc)(" :: Trying full because partial may fail :: ");
 732     // Try a full collection; see delta for bug id 6266275
 733     // for the original code and why this has been simplified
 734     // with from-space allocation criteria modified and
 735     // such allocation moved out of the safepoint path.
 736     do_collection(true,                      // full
 737                   false,                     // clear_all_soft_refs
 738                   size,                      // size
 739                   is_tlab,                   // is_tlab
 740                   GenCollectedHeap::OldGen); // max_generation
 741   }
 742 
 743   result = attempt_allocation(size, is_tlab, false /*first_only*/);
 744 
 745   if (result != NULL) {
 746     assert(is_in_reserved(result), "result not in heap");
 747     return result;
 748   }
 749 
 750   // OK, collection failed, try expansion.
 751   result = expand_heap_and_allocate(size, is_tlab);
 752   if (result != NULL) {
 753     return result;
 754   }
 755 
 756   // If we reach this point, we're really out of memory. Try every trick
 757   // we can to reclaim memory. Force collection of soft references. Force
 758   // a complete compaction of the heap. Any additional methods for finding
 759   // free memory should be here, especially if they are expensive. If this
 760   // attempt fails, an OOM exception will be thrown.
 761   {
 762     UIntFlagSetting flag_change(MarkSweepAlwaysCompactCount, 1); // Make sure the heap is fully compacted
 763 
 764     do_collection(true,                      // full
 765                   true,                      // clear_all_soft_refs
 766                   size,                      // size
 767                   is_tlab,                   // is_tlab
 768                   GenCollectedHeap::OldGen); // max_generation
 769   }
 770 
 771   result = attempt_allocation(size, is_tlab, false /* first_only */);
 772   if (result != NULL) {
 773     assert(is_in_reserved(result), "result not in heap");
 774     return result;
 775   }
 776 
 777   assert(!soft_ref_policy()-&gt;should_clear_all_soft_refs(),
 778     "Flag should have been handled and cleared prior to this point");
 779 
 780   // What else?  We might try synchronous finalization later.  If the total
 781   // space available is large enough for the allocation, then a more
 782   // complete compaction phase than we've tried so far might be
 783   // appropriate.
 784   return NULL;
 785 }
 786 
 787 #ifdef ASSERT
 788 class AssertNonScavengableClosure: public OopClosure {
 789 public:
 790   virtual void do_oop(oop* p) {
 791     assert(!GenCollectedHeap::heap()-&gt;is_in_partial_collection(*p),
 792       "Referent should not be scavengable.");  }
 793   virtual void do_oop(narrowOop* p) { ShouldNotReachHere(); }
 794 };
 795 static AssertNonScavengableClosure assert_is_non_scavengable_closure;
 796 #endif
 797 
 798 void GenCollectedHeap::process_roots(StrongRootsScope* scope,
 799                                      ScanningOption so,
 800                                      OopClosure* strong_roots,
 801                                      CLDClosure* strong_cld_closure,
 802                                      CLDClosure* weak_cld_closure,
 803                                      CodeBlobToOopClosure* code_roots) {
 804   // General roots.
 805   assert(code_roots != NULL, "code root closure should always be set");
 806   // _n_termination for _process_strong_tasks should be set up stream
 807   // in a method not running in a GC worker.  Otherwise the GC worker
 808   // could be trying to change the termination condition while the task
 809   // is executing in another GC worker.
 810 
 811   if (_process_strong_tasks-&gt;try_claim_task(GCH_PS_ClassLoaderDataGraph_oops_do)) {
 812     ClassLoaderDataGraph::roots_cld_do(strong_cld_closure, weak_cld_closure);
 813   }
 814 
 815   // Only process code roots from thread stacks if we aren't visiting the entire CodeCache anyway
 816   CodeBlobToOopClosure* roots_from_code_p = (so &amp; SO_AllCodeCache) ? NULL : code_roots;
 817 
 818   bool is_par = scope-&gt;n_threads() &gt; 1;
 819   Threads::possibly_parallel_oops_do(is_par, strong_roots, roots_from_code_p);
 820 
 821   if (_process_strong_tasks-&gt;try_claim_task(GCH_PS_ObjectSynchronizer_oops_do)) {
 822     ObjectSynchronizer::oops_do(strong_roots);
 823   }
 824 #if INCLUDE_AOT
 825   if (UseAOT &amp;&amp; _process_strong_tasks-&gt;try_claim_task(GCH_PS_aot_oops_do)) {
 826     AOTLoader::oops_do(strong_roots);
 827   }
 828 #endif
 829   if (_process_strong_tasks-&gt;try_claim_task(GCH_PS_OopStorageSet_oops_do)) {
 830     OopStorageSet::strong_oops_do(strong_roots);
 831   }
 832 
 833   if (_process_strong_tasks-&gt;try_claim_task(GCH_PS_CodeCache_oops_do)) {
 834     if (so &amp; SO_ScavengeCodeCache) {
 835       assert(code_roots != NULL, "must supply closure for code cache");
 836 
 837       // We only visit parts of the CodeCache when scavenging.
 838       ScavengableNMethods::nmethods_do(code_roots);
 839     }
 840     if (so &amp; SO_AllCodeCache) {
 841       assert(code_roots != NULL, "must supply closure for code cache");
 842 
 843       // CMSCollector uses this to do intermediate-strength collections.
 844       // We scan the entire code cache, since CodeCache::do_unloading is not called.
 845       CodeCache::blobs_do(code_roots);
 846     }
 847     // Verify that the code cache contents are not subject to
 848     // movement by a scavenging collection.
 849     DEBUG_ONLY(CodeBlobToOopClosure assert_code_is_non_scavengable(&amp;assert_is_non_scavengable_closure, !CodeBlobToOopClosure::FixRelocations));
 850     DEBUG_ONLY(ScavengableNMethods::asserted_non_scavengable_nmethods_do(&amp;assert_code_is_non_scavengable));
 851   }
 852 }
 853 
 854 void GenCollectedHeap::young_process_roots(StrongRootsScope* scope,
 855                                            OopsInGenClosure* root_closure,
 856                                            OopsInGenClosure* old_gen_closure,
 857                                            CLDClosure* cld_closure) {
 858   MarkingCodeBlobClosure mark_code_closure(root_closure, CodeBlobToOopClosure::FixRelocations);
 859 
 860   process_roots(scope, SO_ScavengeCodeCache, root_closure,
 861                 cld_closure, cld_closure, &amp;mark_code_closure);
 862 
 863   if (_process_strong_tasks-&gt;try_claim_task(GCH_PS_younger_gens)) {
 864     root_closure-&gt;reset_generation();
 865   }
 866 
 867   // When collection is parallel, all threads get to cooperate to do
 868   // old generation scanning.
 869   old_gen_closure-&gt;set_generation(_old_gen);
 870   rem_set()-&gt;younger_refs_iterate(_old_gen, old_gen_closure, scope-&gt;n_threads());
 871   old_gen_closure-&gt;reset_generation();
 872 
 873   _process_strong_tasks-&gt;all_tasks_completed(scope-&gt;n_threads());
 874 }
 875 
 876 void GenCollectedHeap::full_process_roots(StrongRootsScope* scope,
 877                                           bool is_adjust_phase,
 878                                           ScanningOption so,
 879                                           bool only_strong_roots,
 880                                           OopsInGenClosure* root_closure,
 881                                           CLDClosure* cld_closure) {
 882   MarkingCodeBlobClosure mark_code_closure(root_closure, is_adjust_phase);
 883   CLDClosure* weak_cld_closure = only_strong_roots ? NULL : cld_closure;
 884 
 885   process_roots(scope, so, root_closure, cld_closure, weak_cld_closure, &amp;mark_code_closure);
 886   _process_strong_tasks-&gt;all_tasks_completed(scope-&gt;n_threads());
 887 }
 888 
 889 void GenCollectedHeap::gen_process_weak_roots(OopClosure* root_closure) {
 890   WeakProcessor::oops_do(root_closure);
 891   _young_gen-&gt;ref_processor()-&gt;weak_oops_do(root_closure);
 892   _old_gen-&gt;ref_processor()-&gt;weak_oops_do(root_closure);
 893 }
 894 
 895 bool GenCollectedHeap::no_allocs_since_save_marks() {
 896   return _young_gen-&gt;no_allocs_since_save_marks() &amp;&amp;
 897          _old_gen-&gt;no_allocs_since_save_marks();
 898 }
 899 
 900 bool GenCollectedHeap::supports_inline_contig_alloc() const {
 901   return _young_gen-&gt;supports_inline_contig_alloc();
 902 }
 903 
 904 HeapWord* volatile* GenCollectedHeap::top_addr() const {
 905   return _young_gen-&gt;top_addr();
 906 }
 907 
 908 HeapWord** GenCollectedHeap::end_addr() const {
 909   return _young_gen-&gt;end_addr();
 910 }
 911 
 912 // public collection interfaces
 913 
 914 void GenCollectedHeap::collect(GCCause::Cause cause) {
 915   if ((cause == GCCause::_wb_young_gc) ||
 916       (cause == GCCause::_gc_locker)) {
 917     // Young collection for WhiteBox or GCLocker.
 918     collect(cause, YoungGen);
 919   } else {
 920 #ifdef ASSERT
 921   if (cause == GCCause::_scavenge_alot) {
 922     // Young collection only.
 923     collect(cause, YoungGen);
 924   } else {
 925     // Stop-the-world full collection.
 926     collect(cause, OldGen);
 927   }
 928 #else
 929     // Stop-the-world full collection.
 930     collect(cause, OldGen);
 931 #endif
 932   }
 933 }
 934 
 935 void GenCollectedHeap::collect(GCCause::Cause cause, GenerationType max_generation) {
 936   // The caller doesn't have the Heap_lock
 937   assert(!Heap_lock-&gt;owned_by_self(), "this thread should not own the Heap_lock");
 938   MutexLocker ml(Heap_lock);
 939   collect_locked(cause, max_generation);
 940 }
 941 
 942 void GenCollectedHeap::collect_locked(GCCause::Cause cause) {
 943   // The caller has the Heap_lock
 944   assert(Heap_lock-&gt;owned_by_self(), "this thread should own the Heap_lock");
 945   collect_locked(cause, OldGen);
 946 }
 947 
 948 // this is the private collection interface
 949 // The Heap_lock is expected to be held on entry.
 950 
 951 void GenCollectedHeap::collect_locked(GCCause::Cause cause, GenerationType max_generation) {
 952   // Read the GC count while holding the Heap_lock
 953   unsigned int gc_count_before      = total_collections();
 954   unsigned int full_gc_count_before = total_full_collections();
 955 
 956   if (GCLocker::should_discard(cause, gc_count_before)) {
 957     return;
 958   }
 959 
 960   {
 961     MutexUnlocker mu(Heap_lock);  // give up heap lock, execute gets it back
 962     VM_GenCollectFull op(gc_count_before, full_gc_count_before,
 963                          cause, max_generation);
 964     VMThread::execute(&amp;op);
 965   }
 966 }
 967 
 968 void GenCollectedHeap::do_full_collection(bool clear_all_soft_refs) {
 969    do_full_collection(clear_all_soft_refs, OldGen);
 970 }
 971 
 972 void GenCollectedHeap::do_full_collection(bool clear_all_soft_refs,
 973                                           GenerationType last_generation) {
 974   do_collection(true,                   // full
 975                 clear_all_soft_refs,    // clear_all_soft_refs
 976                 0,                      // size
 977                 false,                  // is_tlab
 978                 last_generation);       // last_generation
 979   // Hack XXX FIX ME !!!
 980   // A scavenge may not have been attempted, or may have
 981   // been attempted and failed, because the old gen was too full
 982   if (gc_cause() == GCCause::_gc_locker &amp;&amp; incremental_collection_failed()) {
 983     log_debug(gc, jni)("GC locker: Trying a full collection because scavenge failed");
 984     // This time allow the old gen to be collected as well
 985     do_collection(true,                // full
 986                   clear_all_soft_refs, // clear_all_soft_refs
 987                   0,                   // size
 988                   false,               // is_tlab
 989                   OldGen);             // last_generation
 990   }
 991 }
 992 
 993 bool GenCollectedHeap::is_in_young(oop p) {
 994   bool result = cast_from_oop&lt;HeapWord*&gt;(p) &lt; _old_gen-&gt;reserved().start();
 995   assert(result == _young_gen-&gt;is_in_reserved(p),
 996          "incorrect test - result=%d, p=" INTPTR_FORMAT, result, p2i((void*)p));
 997   return result;
 998 }
 999 
1000 // Returns "TRUE" iff "p" points into the committed areas of the heap.
1001 bool GenCollectedHeap::is_in(const void* p) const {
1002   return _young_gen-&gt;is_in(p) || _old_gen-&gt;is_in(p);
1003 }
1004 
1005 #ifdef ASSERT
1006 // Don't implement this by using is_in_young().  This method is used
1007 // in some cases to check that is_in_young() is correct.
1008 bool GenCollectedHeap::is_in_partial_collection(const void* p) {
1009   assert(is_in_reserved(p) || p == NULL,
1010     "Does not work if address is non-null and outside of the heap");
1011   return p &lt; _young_gen-&gt;reserved().end() &amp;&amp; p != NULL;
1012 }
1013 #endif
1014 
1015 void GenCollectedHeap::oop_iterate(OopIterateClosure* cl) {
1016   _young_gen-&gt;oop_iterate(cl);
1017   _old_gen-&gt;oop_iterate(cl);
1018 }
1019 
1020 void GenCollectedHeap::object_iterate(ObjectClosure* cl) {
1021   _young_gen-&gt;object_iterate(cl);
1022   _old_gen-&gt;object_iterate(cl);
1023 }
1024 
1025 Space* GenCollectedHeap::space_containing(const void* addr) const {
1026   Space* res = _young_gen-&gt;space_containing(addr);
1027   if (res != NULL) {
1028     return res;
1029   }
1030   res = _old_gen-&gt;space_containing(addr);
1031   assert(res != NULL, "Could not find containing space");
1032   return res;
1033 }
1034 
1035 HeapWord* GenCollectedHeap::block_start(const void* addr) const {
1036   assert(is_in_reserved(addr), "block_start of address outside of heap");
1037   if (_young_gen-&gt;is_in_reserved(addr)) {
1038     assert(_young_gen-&gt;is_in(addr), "addr should be in allocated part of generation");
1039     return _young_gen-&gt;block_start(addr);
1040   }
1041 
1042   assert(_old_gen-&gt;is_in_reserved(addr), "Some generation should contain the address");
1043   assert(_old_gen-&gt;is_in(addr), "addr should be in allocated part of generation");
1044   return _old_gen-&gt;block_start(addr);
1045 }
1046 
1047 bool GenCollectedHeap::block_is_obj(const HeapWord* addr) const {
1048   assert(is_in_reserved(addr), "block_is_obj of address outside of heap");
1049   assert(block_start(addr) == addr, "addr must be a block start");
1050   if (_young_gen-&gt;is_in_reserved(addr)) {
1051     return _young_gen-&gt;block_is_obj(addr);
1052   }
1053 
1054   assert(_old_gen-&gt;is_in_reserved(addr), "Some generation should contain the address");
1055   return _old_gen-&gt;block_is_obj(addr);
1056 }
1057 
1058 bool GenCollectedHeap::supports_tlab_allocation() const {
1059   assert(!_old_gen-&gt;supports_tlab_allocation(), "Old gen supports TLAB allocation?!");
1060   return _young_gen-&gt;supports_tlab_allocation();
1061 }
1062 
1063 size_t GenCollectedHeap::tlab_capacity(Thread* thr) const {
1064   assert(!_old_gen-&gt;supports_tlab_allocation(), "Old gen supports TLAB allocation?!");
1065   if (_young_gen-&gt;supports_tlab_allocation()) {
1066     return _young_gen-&gt;tlab_capacity();
1067   }
1068   return 0;
1069 }
1070 
1071 size_t GenCollectedHeap::tlab_used(Thread* thr) const {
1072   assert(!_old_gen-&gt;supports_tlab_allocation(), "Old gen supports TLAB allocation?!");
1073   if (_young_gen-&gt;supports_tlab_allocation()) {
1074     return _young_gen-&gt;tlab_used();
1075   }
1076   return 0;
1077 }
1078 
1079 size_t GenCollectedHeap::unsafe_max_tlab_alloc(Thread* thr) const {
1080   assert(!_old_gen-&gt;supports_tlab_allocation(), "Old gen supports TLAB allocation?!");
1081   if (_young_gen-&gt;supports_tlab_allocation()) {
1082     return _young_gen-&gt;unsafe_max_tlab_alloc();
1083   }
1084   return 0;
1085 }
1086 
1087 HeapWord* GenCollectedHeap::allocate_new_tlab(size_t min_size,
1088                                               size_t requested_size,
1089                                               size_t* actual_size) {
1090   bool gc_overhead_limit_was_exceeded;
1091   HeapWord* result = mem_allocate_work(requested_size /* size */,
1092                                        true /* is_tlab */,
1093                                        &amp;gc_overhead_limit_was_exceeded);
1094   if (result != NULL) {
1095     *actual_size = requested_size;
1096   }
1097 
1098   return result;
1099 }
1100 
1101 // Requires "*prev_ptr" to be non-NULL.  Deletes and a block of minimal size
1102 // from the list headed by "*prev_ptr".
1103 static ScratchBlock *removeSmallestScratch(ScratchBlock **prev_ptr) {
1104   bool first = true;
1105   size_t min_size = 0;   // "first" makes this conceptually infinite.
1106   ScratchBlock **smallest_ptr, *smallest;
1107   ScratchBlock  *cur = *prev_ptr;
1108   while (cur) {
1109     assert(*prev_ptr == cur, "just checking");
1110     if (first || cur-&gt;num_words &lt; min_size) {
1111       smallest_ptr = prev_ptr;
1112       smallest     = cur;
1113       min_size     = smallest-&gt;num_words;
1114       first        = false;
1115     }
1116     prev_ptr = &amp;cur-&gt;next;
1117     cur     =  cur-&gt;next;
1118   }
1119   smallest      = *smallest_ptr;
1120   *smallest_ptr = smallest-&gt;next;
1121   return smallest;
1122 }
1123 
1124 // Sort the scratch block list headed by res into decreasing size order,
1125 // and set "res" to the result.
1126 static void sort_scratch_list(ScratchBlock*&amp; list) {
1127   ScratchBlock* sorted = NULL;
1128   ScratchBlock* unsorted = list;
1129   while (unsorted) {
1130     ScratchBlock *smallest = removeSmallestScratch(&amp;unsorted);
1131     smallest-&gt;next  = sorted;
1132     sorted          = smallest;
1133   }
1134   list = sorted;
1135 }
1136 
1137 ScratchBlock* GenCollectedHeap::gather_scratch(Generation* requestor,
1138                                                size_t max_alloc_words) {
1139   ScratchBlock* res = NULL;
1140   _young_gen-&gt;contribute_scratch(res, requestor, max_alloc_words);
1141   _old_gen-&gt;contribute_scratch(res, requestor, max_alloc_words);
1142   sort_scratch_list(res);
1143   return res;
1144 }
1145 
1146 void GenCollectedHeap::release_scratch() {
1147   _young_gen-&gt;reset_scratch();
1148   _old_gen-&gt;reset_scratch();
1149 }
1150 
1151 class GenPrepareForVerifyClosure: public GenCollectedHeap::GenClosure {
1152   void do_generation(Generation* gen) {
1153     gen-&gt;prepare_for_verify();
1154   }
1155 };
1156 
1157 void GenCollectedHeap::prepare_for_verify() {
1158   ensure_parsability(false);        // no need to retire TLABs
1159   GenPrepareForVerifyClosure blk;
1160   generation_iterate(&amp;blk, false);
1161 }
1162 
1163 void GenCollectedHeap::generation_iterate(GenClosure* cl,
1164                                           bool old_to_young) {
1165   if (old_to_young) {
1166     cl-&gt;do_generation(_old_gen);
1167     cl-&gt;do_generation(_young_gen);
1168   } else {
1169     cl-&gt;do_generation(_young_gen);
1170     cl-&gt;do_generation(_old_gen);
1171   }
1172 }
1173 
1174 bool GenCollectedHeap::is_maximal_no_gc() const {
1175   return _young_gen-&gt;is_maximal_no_gc() &amp;&amp; _old_gen-&gt;is_maximal_no_gc();
1176 }
1177 
1178 void GenCollectedHeap::save_marks() {
1179   _young_gen-&gt;save_marks();
1180   _old_gen-&gt;save_marks();
1181 }
1182 
1183 GenCollectedHeap* GenCollectedHeap::heap() {
1184   // SerialHeap is the only subtype of GenCollectedHeap.
1185   return named_heap&lt;GenCollectedHeap&gt;(CollectedHeap::Serial);
1186 }
1187 
1188 #if INCLUDE_SERIALGC
1189 void GenCollectedHeap::prepare_for_compaction() {
1190   // Start by compacting into same gen.
1191   CompactPoint cp(_old_gen);
1192   _old_gen-&gt;prepare_for_compaction(&amp;cp);
1193   _young_gen-&gt;prepare_for_compaction(&amp;cp);
1194 }
1195 #endif // INCLUDE_SERIALGC
1196 
1197 void GenCollectedHeap::verify(VerifyOption option /* ignored */) {
1198   log_debug(gc, verify)("%s", _old_gen-&gt;name());
1199   _old_gen-&gt;verify();
1200 
1201   log_debug(gc, verify)("%s", _old_gen-&gt;name());
1202   _young_gen-&gt;verify();
1203 
1204   log_debug(gc, verify)("RemSet");
1205   rem_set()-&gt;verify();
1206 }
1207 
1208 void GenCollectedHeap::print_on(outputStream* st) const {
1209   if (_young_gen != NULL) {
1210     _young_gen-&gt;print_on(st);
1211   }
1212   if (_old_gen != NULL) {
1213     _old_gen-&gt;print_on(st);
1214   }
1215   MetaspaceUtils::print_on(st);
1216 }
1217 
1218 void GenCollectedHeap::gc_threads_do(ThreadClosure* tc) const {
1219 }
1220 
1221 bool GenCollectedHeap::print_location(outputStream* st, void* addr) const {
1222   return BlockLocationPrinter&lt;GenCollectedHeap&gt;::print_location(st, addr);
1223 }
1224 
1225 void GenCollectedHeap::print_tracing_info() const {
1226   if (log_is_enabled(Debug, gc, heap, exit)) {
1227     LogStreamHandle(Debug, gc, heap, exit) lsh;
1228     _young_gen-&gt;print_summary_info_on(&amp;lsh);
1229     _old_gen-&gt;print_summary_info_on(&amp;lsh);
1230   }
1231 }
1232 
1233 void GenCollectedHeap::print_heap_change(const PreGenGCValues&amp; pre_gc_values) const {
1234   const DefNewGeneration* const def_new_gen = (DefNewGeneration*) young_gen();
1235 
1236   log_info(gc, heap)(HEAP_CHANGE_FORMAT" "
1237                      HEAP_CHANGE_FORMAT" "
1238                      HEAP_CHANGE_FORMAT,
1239                      HEAP_CHANGE_FORMAT_ARGS(def_new_gen-&gt;short_name(),
1240                                              pre_gc_values.young_gen_used(),
1241                                              pre_gc_values.young_gen_capacity(),
1242                                              def_new_gen-&gt;used(),
1243                                              def_new_gen-&gt;capacity()),
1244                      HEAP_CHANGE_FORMAT_ARGS("Eden",
1245                                              pre_gc_values.eden_used(),
1246                                              pre_gc_values.eden_capacity(),
1247                                              def_new_gen-&gt;eden()-&gt;used(),
1248                                              def_new_gen-&gt;eden()-&gt;capacity()),
1249                      HEAP_CHANGE_FORMAT_ARGS("From",
1250                                              pre_gc_values.from_used(),
1251                                              pre_gc_values.from_capacity(),
1252                                              def_new_gen-&gt;from()-&gt;used(),
1253                                              def_new_gen-&gt;from()-&gt;capacity()));
1254   log_info(gc, heap)(HEAP_CHANGE_FORMAT,
1255                      HEAP_CHANGE_FORMAT_ARGS(old_gen()-&gt;short_name(),
1256                                              pre_gc_values.old_gen_used(),
1257                                              pre_gc_values.old_gen_capacity(),
1258                                              old_gen()-&gt;used(),
1259                                              old_gen()-&gt;capacity()));
1260   MetaspaceUtils::print_metaspace_change(pre_gc_values.metaspace_sizes());
1261 }
1262 
1263 class GenGCPrologueClosure: public GenCollectedHeap::GenClosure {
1264  private:
1265   bool _full;
1266  public:
1267   void do_generation(Generation* gen) {
1268     gen-&gt;gc_prologue(_full);
1269   }
1270   GenGCPrologueClosure(bool full) : _full(full) {};
1271 };
1272 
1273 void GenCollectedHeap::gc_prologue(bool full) {
1274   assert(InlineCacheBuffer::is_empty(), "should have cleaned up ICBuffer");
1275 
1276   // Fill TLAB's and such
1277   ensure_parsability(true);   // retire TLABs
1278 
1279   // Walk generations
1280   GenGCPrologueClosure blk(full);
1281   generation_iterate(&amp;blk, false);  // not old-to-young.
1282 };
1283 
1284 class GenGCEpilogueClosure: public GenCollectedHeap::GenClosure {
1285  private:
1286   bool _full;
1287  public:
1288   void do_generation(Generation* gen) {
1289     gen-&gt;gc_epilogue(_full);
1290   }
1291   GenGCEpilogueClosure(bool full) : _full(full) {};
1292 };
1293 
1294 void GenCollectedHeap::gc_epilogue(bool full) {
1295 #if COMPILER2_OR_JVMCI
1296   assert(DerivedPointerTable::is_empty(), "derived pointer present");
1297   size_t actual_gap = pointer_delta((HeapWord*) (max_uintx-3), *(end_addr()));
1298   guarantee(is_client_compilation_mode_vm() || actual_gap &gt; (size_t)FastAllocateSizeLimit, "inline allocation wraps");
1299 #endif // COMPILER2_OR_JVMCI
1300 
1301   resize_all_tlabs();
1302 
1303   GenGCEpilogueClosure blk(full);
1304   generation_iterate(&amp;blk, false);  // not old-to-young.
1305 
1306   if (!CleanChunkPoolAsync) {
1307     Chunk::clean_chunk_pool();
1308   }
1309 
1310   MetaspaceCounters::update_performance_counters();
1311   CompressedClassSpaceCounters::update_performance_counters();
1312 };
1313 
1314 #ifndef PRODUCT
1315 class GenGCSaveTopsBeforeGCClosure: public GenCollectedHeap::GenClosure {
1316  private:
1317  public:
1318   void do_generation(Generation* gen) {
1319     gen-&gt;record_spaces_top();
1320   }
1321 };
1322 
1323 void GenCollectedHeap::record_gen_tops_before_GC() {
1324   if (ZapUnusedHeapArea) {
1325     GenGCSaveTopsBeforeGCClosure blk;
1326     generation_iterate(&amp;blk, false);  // not old-to-young.
1327   }
1328 }
1329 #endif  // not PRODUCT
1330 
1331 class GenEnsureParsabilityClosure: public GenCollectedHeap::GenClosure {
1332  public:
1333   void do_generation(Generation* gen) {
1334     gen-&gt;ensure_parsability();
1335   }
1336 };
1337 
1338 void GenCollectedHeap::ensure_parsability(bool retire_tlabs) {
1339   CollectedHeap::ensure_parsability(retire_tlabs);
1340   GenEnsureParsabilityClosure ep_cl;
1341   generation_iterate(&amp;ep_cl, false);
1342 }
1343 
1344 oop GenCollectedHeap::handle_failed_promotion(Generation* old_gen,
1345                                               oop obj,
1346                                               size_t obj_size) {
1347   guarantee(old_gen == _old_gen, "We only get here with an old generation");
1348   assert(obj_size == (size_t)obj-&gt;size(), "bad obj_size passed in");
1349   HeapWord* result = NULL;
1350 
1351   result = old_gen-&gt;expand_and_allocate(obj_size, false);
1352 
1353   if (result != NULL) {
1354     Copy::aligned_disjoint_words(cast_from_oop&lt;HeapWord*&gt;(obj), result, obj_size);
1355   }
1356   return oop(result);
1357 }
<a name="3" id="anc3"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="3" type="hidden" /></form></body></html>
