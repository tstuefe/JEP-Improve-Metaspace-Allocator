<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre>rev <a href="https://bugs.openjdk.java.net/browse/JDK-60800">60800</a> : imported patch jep387-all.patch</pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "aot/aotLoader.hpp"
  27 #include "classfile/classLoaderDataGraph.hpp"
  28 #include "classfile/symbolTable.hpp"
  29 #include "classfile/stringTable.hpp"
  30 #include "classfile/vmSymbols.hpp"
  31 #include "code/codeCache.hpp"
  32 #include "code/icBuffer.hpp"
  33 #include "gc/serial/defNewGeneration.hpp"
  34 #include "gc/shared/adaptiveSizePolicy.hpp"
  35 #include "gc/shared/cardTableBarrierSet.hpp"
  36 #include "gc/shared/cardTableRS.hpp"
  37 #include "gc/shared/collectedHeap.inline.hpp"
  38 #include "gc/shared/collectorCounters.hpp"
  39 #include "gc/shared/gcId.hpp"
  40 #include "gc/shared/gcLocker.hpp"
  41 #include "gc/shared/gcPolicyCounters.hpp"
  42 #include "gc/shared/gcTrace.hpp"
  43 #include "gc/shared/gcTraceTime.inline.hpp"
  44 #include "gc/shared/genArguments.hpp"
  45 #include "gc/shared/gcVMOperations.hpp"
  46 #include "gc/shared/genCollectedHeap.hpp"
  47 #include "gc/shared/genOopClosures.inline.hpp"
  48 #include "gc/shared/generationSpec.hpp"
  49 #include "gc/shared/gcInitLogger.hpp"
  50 #include "gc/shared/locationPrinter.inline.hpp"
  51 #include "gc/shared/oopStorage.inline.hpp"
  52 #include "gc/shared/oopStorageSet.inline.hpp"
  53 #include "gc/shared/oopStorageParState.inline.hpp"
  54 #include "gc/shared/scavengableNMethods.hpp"
  55 #include "gc/shared/space.hpp"
  56 #include "gc/shared/strongRootsScope.hpp"
  57 #include "gc/shared/weakProcessor.hpp"
  58 #include "gc/shared/workgroup.hpp"
  59 #include "memory/filemap.hpp"
  60 #include "memory/iterator.hpp"
  61 #include "memory/metaspaceCounters.hpp"
<a name="1" id="anc1"></a>
  62 #include "memory/resourceArea.hpp"
  63 #include "memory/universe.hpp"
  64 #include "oops/oop.inline.hpp"
  65 #include "runtime/biasedLocking.hpp"
  66 #include "runtime/handles.hpp"
  67 #include "runtime/handles.inline.hpp"
  68 #include "runtime/java.hpp"
  69 #include "runtime/vmThread.hpp"
  70 #include "services/memoryService.hpp"
  71 #include "utilities/autoRestore.hpp"
  72 #include "utilities/debug.hpp"
  73 #include "utilities/formatBuffer.hpp"
  74 #include "utilities/macros.hpp"
  75 #include "utilities/stack.inline.hpp"
  76 #include "utilities/vmError.hpp"
  77 #if INCLUDE_JVMCI
  78 #include "jvmci/jvmci.hpp"
  79 #endif
  80 
  81 GenCollectedHeap::GenCollectedHeap(Generation::Name young,
  82                                    Generation::Name old,
  83                                    const char* policy_counters_name) :
  84   CollectedHeap(),
  85   _young_gen(NULL),
  86   _old_gen(NULL),
  87   _young_gen_spec(new GenerationSpec(young,
  88                                      NewSize,
  89                                      MaxNewSize,
  90                                      GenAlignment)),
  91   _old_gen_spec(new GenerationSpec(old,
  92                                    OldSize,
  93                                    MaxOldSize,
  94                                    GenAlignment)),
  95   _rem_set(NULL),
  96   _soft_ref_gen_policy(),
  97   _size_policy(NULL),
  98   _gc_policy_counters(new GCPolicyCounters(policy_counters_name, 2, 2)),
  99   _incremental_collection_failed(false),
 100   _full_collections_completed(0),
 101   _process_strong_tasks(new SubTasksDone(GCH_PS_NumElements)),
 102   _young_manager(NULL),
 103   _old_manager(NULL) {
 104 }
 105 
 106 jint GenCollectedHeap::initialize() {
 107   // While there are no constraints in the GC code that HeapWordSize
 108   // be any particular value, there are multiple other areas in the
 109   // system which believe this to be true (e.g. oop-&gt;object_size in some
 110   // cases incorrectly returns the size in wordSize units rather than
 111   // HeapWordSize).
 112   guarantee(HeapWordSize == wordSize, "HeapWordSize must equal wordSize");
 113 
 114   // Allocate space for the heap.
 115 
 116   ReservedHeapSpace heap_rs = allocate(HeapAlignment);
 117 
 118   if (!heap_rs.is_reserved()) {
 119     vm_shutdown_during_initialization(
 120       "Could not reserve enough space for object heap");
 121     return JNI_ENOMEM;
 122   }
 123 
 124   initialize_reserved_region(heap_rs);
 125 
 126   _rem_set = create_rem_set(heap_rs.region());
 127   _rem_set-&gt;initialize();
 128   CardTableBarrierSet *bs = new CardTableBarrierSet(_rem_set);
 129   bs-&gt;initialize();
 130   BarrierSet::set_barrier_set(bs);
 131 
 132   ReservedSpace young_rs = heap_rs.first_part(_young_gen_spec-&gt;max_size());
 133   _young_gen = _young_gen_spec-&gt;init(young_rs, rem_set());
 134   ReservedSpace old_rs = heap_rs.last_part(_young_gen_spec-&gt;max_size());
 135 
 136   old_rs = old_rs.first_part(_old_gen_spec-&gt;max_size());
 137   _old_gen = _old_gen_spec-&gt;init(old_rs, rem_set());
 138 
 139   GCInitLogger::print();
 140 
 141   return JNI_OK;
 142 }
 143 
 144 CardTableRS* GenCollectedHeap::create_rem_set(const MemRegion&amp; reserved_region) {
 145   return new CardTableRS(reserved_region, false /* scan_concurrently */);
 146 }
 147 
 148 void GenCollectedHeap::initialize_size_policy(size_t init_eden_size,
 149                                               size_t init_promo_size,
 150                                               size_t init_survivor_size) {
 151   const double max_gc_pause_sec = ((double) MaxGCPauseMillis) / 1000.0;
 152   _size_policy = new AdaptiveSizePolicy(init_eden_size,
 153                                         init_promo_size,
 154                                         init_survivor_size,
 155                                         max_gc_pause_sec,
 156                                         GCTimeRatio);
 157 }
 158 
 159 ReservedHeapSpace GenCollectedHeap::allocate(size_t alignment) {
 160   // Now figure out the total size.
 161   const size_t pageSize = UseLargePages ? os::large_page_size() : os::vm_page_size();
 162   assert(alignment % pageSize == 0, "Must be");
 163 
 164   // Check for overflow.
 165   size_t total_reserved = _young_gen_spec-&gt;max_size() + _old_gen_spec-&gt;max_size();
 166   if (total_reserved &lt; _young_gen_spec-&gt;max_size()) {
 167     vm_exit_during_initialization("The size of the object heap + VM data exceeds "
 168                                   "the maximum representable size");
 169   }
 170   assert(total_reserved % alignment == 0,
 171          "Gen size; total_reserved=" SIZE_FORMAT ", alignment="
 172          SIZE_FORMAT, total_reserved, alignment);
 173 
 174   ReservedHeapSpace heap_rs = Universe::reserve_heap(total_reserved, alignment);
 175 
 176   os::trace_page_sizes("Heap",
 177                        MinHeapSize,
 178                        total_reserved,
 179                        alignment,
 180                        heap_rs.base(),
 181                        heap_rs.size());
 182 
 183   return heap_rs;
 184 }
 185 
 186 class GenIsScavengable : public BoolObjectClosure {
 187 public:
 188   bool do_object_b(oop obj) {
 189     return GenCollectedHeap::heap()-&gt;is_in_young(obj);
 190   }
 191 };
 192 
 193 static GenIsScavengable _is_scavengable;
 194 
 195 void GenCollectedHeap::post_initialize() {
 196   CollectedHeap::post_initialize();
 197   ref_processing_init();
 198 
 199   DefNewGeneration* def_new_gen = (DefNewGeneration*)_young_gen;
 200 
 201   initialize_size_policy(def_new_gen-&gt;eden()-&gt;capacity(),
 202                          _old_gen-&gt;capacity(),
 203                          def_new_gen-&gt;from()-&gt;capacity());
 204 
 205   MarkSweep::initialize();
 206 
 207   ScavengableNMethods::initialize(&amp;_is_scavengable);
 208 }
 209 
 210 void GenCollectedHeap::ref_processing_init() {
 211   _young_gen-&gt;ref_processor_init();
 212   _old_gen-&gt;ref_processor_init();
 213 }
 214 
 215 PreGenGCValues GenCollectedHeap::get_pre_gc_values() const {
 216   const DefNewGeneration* const def_new_gen = (DefNewGeneration*) young_gen();
 217 
 218   return PreGenGCValues(def_new_gen-&gt;used(),
 219                         def_new_gen-&gt;capacity(),
 220                         def_new_gen-&gt;eden()-&gt;used(),
 221                         def_new_gen-&gt;eden()-&gt;capacity(),
 222                         def_new_gen-&gt;from()-&gt;used(),
 223                         def_new_gen-&gt;from()-&gt;capacity(),
 224                         old_gen()-&gt;used(),
 225                         old_gen()-&gt;capacity());
 226 }
 227 
 228 GenerationSpec* GenCollectedHeap::young_gen_spec() const {
 229   return _young_gen_spec;
 230 }
 231 
 232 GenerationSpec* GenCollectedHeap::old_gen_spec() const {
 233   return _old_gen_spec;
 234 }
 235 
 236 size_t GenCollectedHeap::capacity() const {
 237   return _young_gen-&gt;capacity() + _old_gen-&gt;capacity();
 238 }
 239 
 240 size_t GenCollectedHeap::used() const {
 241   return _young_gen-&gt;used() + _old_gen-&gt;used();
 242 }
 243 
 244 void GenCollectedHeap::save_used_regions() {
 245   _old_gen-&gt;save_used_region();
 246   _young_gen-&gt;save_used_region();
 247 }
 248 
 249 size_t GenCollectedHeap::max_capacity() const {
 250   return _young_gen-&gt;max_capacity() + _old_gen-&gt;max_capacity();
 251 }
 252 
 253 // Update the _full_collections_completed counter
 254 // at the end of a stop-world full GC.
 255 unsigned int GenCollectedHeap::update_full_collections_completed() {
 256   MonitorLocker ml(FullGCCount_lock, Mutex::_no_safepoint_check_flag);
 257   assert(_full_collections_completed &lt;= _total_full_collections,
 258          "Can't complete more collections than were started");
 259   _full_collections_completed = _total_full_collections;
 260   ml.notify_all();
 261   return _full_collections_completed;
 262 }
 263 
 264 // Update the _full_collections_completed counter, as appropriate,
 265 // at the end of a concurrent GC cycle. Note the conditional update
 266 // below to allow this method to be called by a concurrent collector
 267 // without synchronizing in any manner with the VM thread (which
 268 // may already have initiated a STW full collection "concurrently").
 269 unsigned int GenCollectedHeap::update_full_collections_completed(unsigned int count) {
 270   MonitorLocker ml(FullGCCount_lock, Mutex::_no_safepoint_check_flag);
 271   assert((_full_collections_completed &lt;= _total_full_collections) &amp;&amp;
 272          (count &lt;= _total_full_collections),
 273          "Can't complete more collections than were started");
 274   if (count &gt; _full_collections_completed) {
 275     _full_collections_completed = count;
 276     ml.notify_all();
 277   }
 278   return _full_collections_completed;
 279 }
 280 
 281 // Return true if any of the following is true:
 282 // . the allocation won't fit into the current young gen heap
 283 // . gc locker is occupied (jni critical section)
 284 // . heap memory is tight -- the most recent previous collection
 285 //   was a full collection because a partial collection (would
 286 //   have) failed and is likely to fail again
 287 bool GenCollectedHeap::should_try_older_generation_allocation(size_t word_size) const {
 288   size_t young_capacity = _young_gen-&gt;capacity_before_gc();
 289   return    (word_size &gt; heap_word_size(young_capacity))
 290          || GCLocker::is_active_and_needs_gc()
 291          || incremental_collection_failed();
 292 }
 293 
 294 HeapWord* GenCollectedHeap::expand_heap_and_allocate(size_t size, bool   is_tlab) {
 295   HeapWord* result = NULL;
 296   if (_old_gen-&gt;should_allocate(size, is_tlab)) {
 297     result = _old_gen-&gt;expand_and_allocate(size, is_tlab);
 298   }
 299   if (result == NULL) {
 300     if (_young_gen-&gt;should_allocate(size, is_tlab)) {
 301       result = _young_gen-&gt;expand_and_allocate(size, is_tlab);
 302     }
 303   }
 304   assert(result == NULL || is_in_reserved(result), "result not in heap");
 305   return result;
 306 }
 307 
 308 HeapWord* GenCollectedHeap::mem_allocate_work(size_t size,
 309                                               bool is_tlab,
 310                                               bool* gc_overhead_limit_was_exceeded) {
 311   // In general gc_overhead_limit_was_exceeded should be false so
 312   // set it so here and reset it to true only if the gc time
 313   // limit is being exceeded as checked below.
 314   *gc_overhead_limit_was_exceeded = false;
 315 
 316   HeapWord* result = NULL;
 317 
 318   // Loop until the allocation is satisfied, or unsatisfied after GC.
 319   for (uint try_count = 1, gclocker_stalled_count = 0; /* return or throw */; try_count += 1) {
 320 
 321     // First allocation attempt is lock-free.
 322     Generation *young = _young_gen;
 323     assert(young-&gt;supports_inline_contig_alloc(),
 324       "Otherwise, must do alloc within heap lock");
 325     if (young-&gt;should_allocate(size, is_tlab)) {
 326       result = young-&gt;par_allocate(size, is_tlab);
 327       if (result != NULL) {
 328         assert(is_in_reserved(result), "result not in heap");
 329         return result;
 330       }
 331     }
 332     uint gc_count_before;  // Read inside the Heap_lock locked region.
 333     {
 334       MutexLocker ml(Heap_lock);
 335       log_trace(gc, alloc)("GenCollectedHeap::mem_allocate_work: attempting locked slow path allocation");
 336       // Note that only large objects get a shot at being
 337       // allocated in later generations.
 338       bool first_only = !should_try_older_generation_allocation(size);
 339 
 340       result = attempt_allocation(size, is_tlab, first_only);
 341       if (result != NULL) {
 342         assert(is_in_reserved(result), "result not in heap");
 343         return result;
 344       }
 345 
 346       if (GCLocker::is_active_and_needs_gc()) {
 347         if (is_tlab) {
 348           return NULL;  // Caller will retry allocating individual object.
 349         }
 350         if (!is_maximal_no_gc()) {
 351           // Try and expand heap to satisfy request.
 352           result = expand_heap_and_allocate(size, is_tlab);
 353           // Result could be null if we are out of space.
 354           if (result != NULL) {
 355             return result;
 356           }
 357         }
 358 
 359         if (gclocker_stalled_count &gt; GCLockerRetryAllocationCount) {
 360           return NULL; // We didn't get to do a GC and we didn't get any memory.
 361         }
 362 
 363         // If this thread is not in a jni critical section, we stall
 364         // the requestor until the critical section has cleared and
 365         // GC allowed. When the critical section clears, a GC is
 366         // initiated by the last thread exiting the critical section; so
 367         // we retry the allocation sequence from the beginning of the loop,
 368         // rather than causing more, now probably unnecessary, GC attempts.
 369         JavaThread* jthr = JavaThread::current();
 370         if (!jthr-&gt;in_critical()) {
 371           MutexUnlocker mul(Heap_lock);
 372           // Wait for JNI critical section to be exited
 373           GCLocker::stall_until_clear();
 374           gclocker_stalled_count += 1;
 375           continue;
 376         } else {
 377           if (CheckJNICalls) {
 378             fatal("Possible deadlock due to allocating while"
 379                   " in jni critical section");
 380           }
 381           return NULL;
 382         }
 383       }
 384 
 385       // Read the gc count while the heap lock is held.
 386       gc_count_before = total_collections();
 387     }
 388 
 389     VM_GenCollectForAllocation op(size, is_tlab, gc_count_before);
 390     VMThread::execute(&amp;op);
 391     if (op.prologue_succeeded()) {
 392       result = op.result();
 393       if (op.gc_locked()) {
 394          assert(result == NULL, "must be NULL if gc_locked() is true");
 395          continue;  // Retry and/or stall as necessary.
 396       }
 397 
 398       // Allocation has failed and a collection
 399       // has been done.  If the gc time limit was exceeded the
 400       // this time, return NULL so that an out-of-memory
 401       // will be thrown.  Clear gc_overhead_limit_exceeded
 402       // so that the overhead exceeded does not persist.
 403 
 404       const bool limit_exceeded = size_policy()-&gt;gc_overhead_limit_exceeded();
 405       const bool softrefs_clear = soft_ref_policy()-&gt;all_soft_refs_clear();
 406 
 407       if (limit_exceeded &amp;&amp; softrefs_clear) {
 408         *gc_overhead_limit_was_exceeded = true;
 409         size_policy()-&gt;set_gc_overhead_limit_exceeded(false);
 410         if (op.result() != NULL) {
 411           CollectedHeap::fill_with_object(op.result(), size);
 412         }
 413         return NULL;
 414       }
 415       assert(result == NULL || is_in_reserved(result),
 416              "result not in heap");
 417       return result;
 418     }
 419 
 420     // Give a warning if we seem to be looping forever.
 421     if ((QueuedAllocationWarningCount &gt; 0) &amp;&amp;
 422         (try_count % QueuedAllocationWarningCount == 0)) {
 423           log_warning(gc, ergo)("GenCollectedHeap::mem_allocate_work retries %d times,"
 424                                 " size=" SIZE_FORMAT " %s", try_count, size, is_tlab ? "(TLAB)" : "");
 425     }
 426   }
 427 }
 428 
 429 HeapWord* GenCollectedHeap::attempt_allocation(size_t size,
 430                                                bool is_tlab,
 431                                                bool first_only) {
 432   HeapWord* res = NULL;
 433 
 434   if (_young_gen-&gt;should_allocate(size, is_tlab)) {
 435     res = _young_gen-&gt;allocate(size, is_tlab);
 436     if (res != NULL || first_only) {
 437       return res;
 438     }
 439   }
 440 
 441   if (_old_gen-&gt;should_allocate(size, is_tlab)) {
 442     res = _old_gen-&gt;allocate(size, is_tlab);
 443   }
 444 
 445   return res;
 446 }
 447 
 448 HeapWord* GenCollectedHeap::mem_allocate(size_t size,
 449                                          bool* gc_overhead_limit_was_exceeded) {
 450   return mem_allocate_work(size,
 451                            false /* is_tlab */,
 452                            gc_overhead_limit_was_exceeded);
 453 }
 454 
 455 bool GenCollectedHeap::must_clear_all_soft_refs() {
 456   return _gc_cause == GCCause::_metadata_GC_clear_soft_refs ||
 457          _gc_cause == GCCause::_wb_full_gc;
 458 }
 459 
 460 void GenCollectedHeap::collect_generation(Generation* gen, bool full, size_t size,
 461                                           bool is_tlab, bool run_verification, bool clear_soft_refs,
 462                                           bool restore_marks_for_biased_locking) {
 463   FormatBuffer&lt;&gt; title("Collect gen: %s", gen-&gt;short_name());
 464   GCTraceTime(Trace, gc, phases) t1(title);
 465   TraceCollectorStats tcs(gen-&gt;counters());
 466   TraceMemoryManagerStats tmms(gen-&gt;gc_manager(), gc_cause());
 467 
 468   gen-&gt;stat_record()-&gt;invocations++;
 469   gen-&gt;stat_record()-&gt;accumulated_time.start();
 470 
 471   // Must be done anew before each collection because
 472   // a previous collection will do mangling and will
 473   // change top of some spaces.
 474   record_gen_tops_before_GC();
 475 
 476   log_trace(gc)("%s invoke=%d size=" SIZE_FORMAT, heap()-&gt;is_young_gen(gen) ? "Young" : "Old", gen-&gt;stat_record()-&gt;invocations, size * HeapWordSize);
 477 
 478   if (run_verification &amp;&amp; VerifyBeforeGC) {
 479     Universe::verify("Before GC");
 480   }
 481   COMPILER2_OR_JVMCI_PRESENT(DerivedPointerTable::clear());
 482 
 483   if (restore_marks_for_biased_locking) {
 484     // We perform this mark word preservation work lazily
 485     // because it's only at this point that we know whether we
 486     // absolutely have to do it; we want to avoid doing it for
 487     // scavenge-only collections where it's unnecessary
 488     BiasedLocking::preserve_marks();
 489   }
 490 
 491   // Do collection work
 492   {
 493     // Note on ref discovery: For what appear to be historical reasons,
 494     // GCH enables and disabled (by enqueing) refs discovery.
 495     // In the future this should be moved into the generation's
 496     // collect method so that ref discovery and enqueueing concerns
 497     // are local to a generation. The collect method could return
 498     // an appropriate indication in the case that notification on
 499     // the ref lock was needed. This will make the treatment of
 500     // weak refs more uniform (and indeed remove such concerns
 501     // from GCH). XXX
 502 
 503     save_marks();   // save marks for all gens
 504     // We want to discover references, but not process them yet.
 505     // This mode is disabled in process_discovered_references if the
 506     // generation does some collection work, or in
 507     // enqueue_discovered_references if the generation returns
 508     // without doing any work.
 509     ReferenceProcessor* rp = gen-&gt;ref_processor();
 510     // If the discovery of ("weak") refs in this generation is
 511     // atomic wrt other collectors in this configuration, we
 512     // are guaranteed to have empty discovered ref lists.
 513     if (rp-&gt;discovery_is_atomic()) {
 514       rp-&gt;enable_discovery();
 515       rp-&gt;setup_policy(clear_soft_refs);
 516     } else {
 517       // collect() below will enable discovery as appropriate
 518     }
 519     gen-&gt;collect(full, clear_soft_refs, size, is_tlab);
 520     if (!rp-&gt;enqueuing_is_done()) {
 521       rp-&gt;disable_discovery();
 522     } else {
 523       rp-&gt;set_enqueuing_is_done(false);
 524     }
 525     rp-&gt;verify_no_references_recorded();
 526   }
 527 
 528   COMPILER2_OR_JVMCI_PRESENT(DerivedPointerTable::update_pointers());
 529 
 530   gen-&gt;stat_record()-&gt;accumulated_time.stop();
 531 
 532   update_gc_stats(gen, full);
 533 
 534   if (run_verification &amp;&amp; VerifyAfterGC) {
 535     Universe::verify("After GC");
 536   }
 537 }
 538 
 539 void GenCollectedHeap::do_collection(bool           full,
 540                                      bool           clear_all_soft_refs,
 541                                      size_t         size,
 542                                      bool           is_tlab,
 543                                      GenerationType max_generation) {
 544   ResourceMark rm;
 545   DEBUG_ONLY(Thread* my_thread = Thread::current();)
 546 
 547   assert(SafepointSynchronize::is_at_safepoint(), "should be at safepoint");
 548   assert(my_thread-&gt;is_VM_thread() ||
 549          my_thread-&gt;is_ConcurrentGC_thread(),
 550          "incorrect thread type capability");
 551   assert(Heap_lock-&gt;is_locked(),
 552          "the requesting thread should have the Heap_lock");
 553   guarantee(!is_gc_active(), "collection is not reentrant");
 554 
 555   if (GCLocker::check_active_before_gc()) {
 556     return; // GC is disabled (e.g. JNI GetXXXCritical operation)
 557   }
 558 
 559   const bool do_clear_all_soft_refs = clear_all_soft_refs ||
 560                           soft_ref_policy()-&gt;should_clear_all_soft_refs();
 561 
 562   ClearedAllSoftRefs casr(do_clear_all_soft_refs, soft_ref_policy());
 563 
 564   AutoModifyRestore&lt;bool&gt; temporarily(_is_gc_active, true);
 565 
 566   bool complete = full &amp;&amp; (max_generation == OldGen);
 567   bool old_collects_young = complete &amp;&amp; !ScavengeBeforeFullGC;
 568   bool do_young_collection = !old_collects_young &amp;&amp; _young_gen-&gt;should_collect(full, size, is_tlab);
 569 
 570   const PreGenGCValues pre_gc_values = get_pre_gc_values();
 571 
 572   bool run_verification = total_collections() &gt;= VerifyGCStartAt;
 573   bool prepared_for_verification = false;
 574   bool do_full_collection = false;
 575 
 576   if (do_young_collection) {
 577     GCIdMark gc_id_mark;
 578     GCTraceCPUTime tcpu;
 579     GCTraceTime(Info, gc) t("Pause Young", NULL, gc_cause(), true);
 580 
 581     print_heap_before_gc();
 582 
 583     if (run_verification &amp;&amp; VerifyGCLevel &lt;= 0 &amp;&amp; VerifyBeforeGC) {
 584       prepare_for_verify();
 585       prepared_for_verification = true;
 586     }
 587 
 588     gc_prologue(complete);
 589     increment_total_collections(complete);
 590 
 591     collect_generation(_young_gen,
 592                        full,
 593                        size,
 594                        is_tlab,
 595                        run_verification &amp;&amp; VerifyGCLevel &lt;= 0,
 596                        do_clear_all_soft_refs,
 597                        false);
 598 
 599     if (size &gt; 0 &amp;&amp; (!is_tlab || _young_gen-&gt;supports_tlab_allocation()) &amp;&amp;
 600         size * HeapWordSize &lt;= _young_gen-&gt;unsafe_max_alloc_nogc()) {
 601       // Allocation request was met by young GC.
 602       size = 0;
 603     }
 604 
 605     // Ask if young collection is enough. If so, do the final steps for young collection,
 606     // and fallthrough to the end.
 607     do_full_collection = should_do_full_collection(size, full, is_tlab, max_generation);
 608     if (!do_full_collection) {
 609       // Adjust generation sizes.
 610       _young_gen-&gt;compute_new_size();
 611 
 612       print_heap_change(pre_gc_values);
 613 
 614       // Track memory usage and detect low memory after GC finishes
 615       MemoryService::track_memory_usage();
 616 
 617       gc_epilogue(complete);
 618     }
 619 
 620     print_heap_after_gc();
 621 
 622   } else {
 623     // No young collection, ask if we need to perform Full collection.
 624     do_full_collection = should_do_full_collection(size, full, is_tlab, max_generation);
 625   }
 626 
 627   if (do_full_collection) {
 628     GCIdMark gc_id_mark;
 629     GCTraceCPUTime tcpu;
 630     GCTraceTime(Info, gc) t("Pause Full", NULL, gc_cause(), true);
 631 
 632     print_heap_before_gc();
 633 
 634     if (!prepared_for_verification &amp;&amp; run_verification &amp;&amp;
 635         VerifyGCLevel &lt;= 1 &amp;&amp; VerifyBeforeGC) {
 636       prepare_for_verify();
 637     }
 638 
 639     if (!do_young_collection) {
 640       gc_prologue(complete);
 641       increment_total_collections(complete);
 642     }
 643 
 644     // Accounting quirk: total full collections would be incremented when "complete"
 645     // is set, by calling increment_total_collections above. However, we also need to
 646     // account Full collections that had "complete" unset.
 647     if (!complete) {
 648       increment_total_full_collections();
 649     }
 650 
 651     collect_generation(_old_gen,
 652                        full,
 653                        size,
 654                        is_tlab,
 655                        run_verification &amp;&amp; VerifyGCLevel &lt;= 1,
 656                        do_clear_all_soft_refs,
 657                        true);
 658 
 659     // Adjust generation sizes.
 660     _old_gen-&gt;compute_new_size();
 661     _young_gen-&gt;compute_new_size();
 662 
 663     // Delete metaspaces for unloaded class loaders and clean up loader_data graph
 664     ClassLoaderDataGraph::purge(/*at_safepoint*/true);
<a name="2" id="anc2"></a><span class="changed"> 665     MetaspaceUtils::verify_metrics();</span>
 666     // Resize the metaspace capacity after full collections
 667     MetaspaceGC::compute_new_size();
 668     update_full_collections_completed();
 669 
 670     print_heap_change(pre_gc_values);
 671 
 672     // Track memory usage and detect low memory after GC finishes
 673     MemoryService::track_memory_usage();
 674 
 675     // Need to tell the epilogue code we are done with Full GC, regardless what was
 676     // the initial value for "complete" flag.
 677     gc_epilogue(true);
 678 
 679     BiasedLocking::restore_marks();
 680 
 681     print_heap_after_gc();
 682   }
 683 }
 684 
 685 bool GenCollectedHeap::should_do_full_collection(size_t size, bool full, bool is_tlab,
 686                                                  GenCollectedHeap::GenerationType max_gen) const {
 687   return max_gen == OldGen &amp;&amp; _old_gen-&gt;should_collect(full, size, is_tlab);
 688 }
 689 
 690 void GenCollectedHeap::register_nmethod(nmethod* nm) {
 691   ScavengableNMethods::register_nmethod(nm);
 692 }
 693 
 694 void GenCollectedHeap::unregister_nmethod(nmethod* nm) {
 695   ScavengableNMethods::unregister_nmethod(nm);
 696 }
 697 
 698 void GenCollectedHeap::verify_nmethod(nmethod* nm) {
 699   ScavengableNMethods::verify_nmethod(nm);
 700 }
 701 
 702 void GenCollectedHeap::flush_nmethod(nmethod* nm) {
 703   // Do nothing.
 704 }
 705 
 706 void GenCollectedHeap::prune_scavengable_nmethods() {
 707   ScavengableNMethods::prune_nmethods();
 708 }
 709 
 710 HeapWord* GenCollectedHeap::satisfy_failed_allocation(size_t size, bool is_tlab) {
 711   GCCauseSetter x(this, GCCause::_allocation_failure);
 712   HeapWord* result = NULL;
 713 
 714   assert(size != 0, "Precondition violated");
 715   if (GCLocker::is_active_and_needs_gc()) {
 716     // GC locker is active; instead of a collection we will attempt
 717     // to expand the heap, if there's room for expansion.
 718     if (!is_maximal_no_gc()) {
 719       result = expand_heap_and_allocate(size, is_tlab);
 720     }
 721     return result;   // Could be null if we are out of space.
 722   } else if (!incremental_collection_will_fail(false /* don't consult_young */)) {
 723     // Do an incremental collection.
 724     do_collection(false,                     // full
 725                   false,                     // clear_all_soft_refs
 726                   size,                      // size
 727                   is_tlab,                   // is_tlab
 728                   GenCollectedHeap::OldGen); // max_generation
 729   } else {
 730     log_trace(gc)(" :: Trying full because partial may fail :: ");
 731     // Try a full collection; see delta for bug id 6266275
 732     // for the original code and why this has been simplified
 733     // with from-space allocation criteria modified and
 734     // such allocation moved out of the safepoint path.
 735     do_collection(true,                      // full
 736                   false,                     // clear_all_soft_refs
 737                   size,                      // size
 738                   is_tlab,                   // is_tlab
 739                   GenCollectedHeap::OldGen); // max_generation
 740   }
 741 
 742   result = attempt_allocation(size, is_tlab, false /*first_only*/);
 743 
 744   if (result != NULL) {
 745     assert(is_in_reserved(result), "result not in heap");
 746     return result;
 747   }
 748 
 749   // OK, collection failed, try expansion.
 750   result = expand_heap_and_allocate(size, is_tlab);
 751   if (result != NULL) {
 752     return result;
 753   }
 754 
 755   // If we reach this point, we're really out of memory. Try every trick
 756   // we can to reclaim memory. Force collection of soft references. Force
 757   // a complete compaction of the heap. Any additional methods for finding
 758   // free memory should be here, especially if they are expensive. If this
 759   // attempt fails, an OOM exception will be thrown.
 760   {
 761     UIntFlagSetting flag_change(MarkSweepAlwaysCompactCount, 1); // Make sure the heap is fully compacted
 762 
 763     do_collection(true,                      // full
 764                   true,                      // clear_all_soft_refs
 765                   size,                      // size
 766                   is_tlab,                   // is_tlab
 767                   GenCollectedHeap::OldGen); // max_generation
 768   }
 769 
 770   result = attempt_allocation(size, is_tlab, false /* first_only */);
 771   if (result != NULL) {
 772     assert(is_in_reserved(result), "result not in heap");
 773     return result;
 774   }
 775 
 776   assert(!soft_ref_policy()-&gt;should_clear_all_soft_refs(),
 777     "Flag should have been handled and cleared prior to this point");
 778 
 779   // What else?  We might try synchronous finalization later.  If the total
 780   // space available is large enough for the allocation, then a more
 781   // complete compaction phase than we've tried so far might be
 782   // appropriate.
 783   return NULL;
 784 }
 785 
 786 #ifdef ASSERT
 787 class AssertNonScavengableClosure: public OopClosure {
 788 public:
 789   virtual void do_oop(oop* p) {
 790     assert(!GenCollectedHeap::heap()-&gt;is_in_partial_collection(*p),
 791       "Referent should not be scavengable.");  }
 792   virtual void do_oop(narrowOop* p) { ShouldNotReachHere(); }
 793 };
 794 static AssertNonScavengableClosure assert_is_non_scavengable_closure;
 795 #endif
 796 
 797 void GenCollectedHeap::process_roots(StrongRootsScope* scope,
 798                                      ScanningOption so,
 799                                      OopClosure* strong_roots,
 800                                      CLDClosure* strong_cld_closure,
 801                                      CLDClosure* weak_cld_closure,
 802                                      CodeBlobToOopClosure* code_roots) {
 803   // General roots.
 804   assert(code_roots != NULL, "code root closure should always be set");
 805   // _n_termination for _process_strong_tasks should be set up stream
 806   // in a method not running in a GC worker.  Otherwise the GC worker
 807   // could be trying to change the termination condition while the task
 808   // is executing in another GC worker.
 809 
 810   if (_process_strong_tasks-&gt;try_claim_task(GCH_PS_ClassLoaderDataGraph_oops_do)) {
 811     ClassLoaderDataGraph::roots_cld_do(strong_cld_closure, weak_cld_closure);
 812   }
 813 
 814   // Only process code roots from thread stacks if we aren't visiting the entire CodeCache anyway
 815   CodeBlobToOopClosure* roots_from_code_p = (so &amp; SO_AllCodeCache) ? NULL : code_roots;
 816 
 817   bool is_par = scope-&gt;n_threads() &gt; 1;
 818   Threads::possibly_parallel_oops_do(is_par, strong_roots, roots_from_code_p);
 819 
 820   if (_process_strong_tasks-&gt;try_claim_task(GCH_PS_ObjectSynchronizer_oops_do)) {
 821     ObjectSynchronizer::oops_do(strong_roots);
 822   }
 823 #if INCLUDE_AOT
 824   if (UseAOT &amp;&amp; _process_strong_tasks-&gt;try_claim_task(GCH_PS_aot_oops_do)) {
 825     AOTLoader::oops_do(strong_roots);
 826   }
 827 #endif
 828   if (_process_strong_tasks-&gt;try_claim_task(GCH_PS_OopStorageSet_oops_do)) {
 829     OopStorageSet::strong_oops_do(strong_roots);
 830   }
 831 
 832   if (_process_strong_tasks-&gt;try_claim_task(GCH_PS_CodeCache_oops_do)) {
 833     if (so &amp; SO_ScavengeCodeCache) {
 834       assert(code_roots != NULL, "must supply closure for code cache");
 835 
 836       // We only visit parts of the CodeCache when scavenging.
 837       ScavengableNMethods::nmethods_do(code_roots);
 838     }
 839     if (so &amp; SO_AllCodeCache) {
 840       assert(code_roots != NULL, "must supply closure for code cache");
 841 
 842       // CMSCollector uses this to do intermediate-strength collections.
 843       // We scan the entire code cache, since CodeCache::do_unloading is not called.
 844       CodeCache::blobs_do(code_roots);
 845     }
 846     // Verify that the code cache contents are not subject to
 847     // movement by a scavenging collection.
 848     DEBUG_ONLY(CodeBlobToOopClosure assert_code_is_non_scavengable(&amp;assert_is_non_scavengable_closure, !CodeBlobToOopClosure::FixRelocations));
 849     DEBUG_ONLY(ScavengableNMethods::asserted_non_scavengable_nmethods_do(&amp;assert_code_is_non_scavengable));
 850   }
 851 }
 852 
 853 void GenCollectedHeap::full_process_roots(StrongRootsScope* scope,
 854                                           bool is_adjust_phase,
 855                                           ScanningOption so,
 856                                           bool only_strong_roots,
 857                                           OopClosure* root_closure,
 858                                           CLDClosure* cld_closure) {
 859   MarkingCodeBlobClosure mark_code_closure(root_closure, is_adjust_phase);
 860   CLDClosure* weak_cld_closure = only_strong_roots ? NULL : cld_closure;
 861 
 862   process_roots(scope, so, root_closure, cld_closure, weak_cld_closure, &amp;mark_code_closure);
 863   _process_strong_tasks-&gt;all_tasks_completed(scope-&gt;n_threads());
 864 }
 865 
 866 void GenCollectedHeap::gen_process_weak_roots(OopClosure* root_closure) {
 867   WeakProcessor::oops_do(root_closure);
 868   _young_gen-&gt;ref_processor()-&gt;weak_oops_do(root_closure);
 869   _old_gen-&gt;ref_processor()-&gt;weak_oops_do(root_closure);
 870 }
 871 
 872 bool GenCollectedHeap::no_allocs_since_save_marks() {
 873   return _young_gen-&gt;no_allocs_since_save_marks() &amp;&amp;
 874          _old_gen-&gt;no_allocs_since_save_marks();
 875 }
 876 
 877 bool GenCollectedHeap::supports_inline_contig_alloc() const {
 878   return _young_gen-&gt;supports_inline_contig_alloc();
 879 }
 880 
 881 HeapWord* volatile* GenCollectedHeap::top_addr() const {
 882   return _young_gen-&gt;top_addr();
 883 }
 884 
 885 HeapWord** GenCollectedHeap::end_addr() const {
 886   return _young_gen-&gt;end_addr();
 887 }
 888 
 889 // public collection interfaces
 890 
 891 void GenCollectedHeap::collect(GCCause::Cause cause) {
 892   if ((cause == GCCause::_wb_young_gc) ||
 893       (cause == GCCause::_gc_locker)) {
 894     // Young collection for WhiteBox or GCLocker.
 895     collect(cause, YoungGen);
 896   } else {
 897 #ifdef ASSERT
 898   if (cause == GCCause::_scavenge_alot) {
 899     // Young collection only.
 900     collect(cause, YoungGen);
 901   } else {
 902     // Stop-the-world full collection.
 903     collect(cause, OldGen);
 904   }
 905 #else
 906     // Stop-the-world full collection.
 907     collect(cause, OldGen);
 908 #endif
 909   }
 910 }
 911 
 912 void GenCollectedHeap::collect(GCCause::Cause cause, GenerationType max_generation) {
 913   // The caller doesn't have the Heap_lock
 914   assert(!Heap_lock-&gt;owned_by_self(), "this thread should not own the Heap_lock");
 915   MutexLocker ml(Heap_lock);
 916   collect_locked(cause, max_generation);
 917 }
 918 
 919 void GenCollectedHeap::collect_locked(GCCause::Cause cause) {
 920   // The caller has the Heap_lock
 921   assert(Heap_lock-&gt;owned_by_self(), "this thread should own the Heap_lock");
 922   collect_locked(cause, OldGen);
 923 }
 924 
 925 // this is the private collection interface
 926 // The Heap_lock is expected to be held on entry.
 927 
 928 void GenCollectedHeap::collect_locked(GCCause::Cause cause, GenerationType max_generation) {
 929   // Read the GC count while holding the Heap_lock
 930   unsigned int gc_count_before      = total_collections();
 931   unsigned int full_gc_count_before = total_full_collections();
 932 
 933   if (GCLocker::should_discard(cause, gc_count_before)) {
 934     return;
 935   }
 936 
 937   {
 938     MutexUnlocker mu(Heap_lock);  // give up heap lock, execute gets it back
 939     VM_GenCollectFull op(gc_count_before, full_gc_count_before,
 940                          cause, max_generation);
 941     VMThread::execute(&amp;op);
 942   }
 943 }
 944 
 945 void GenCollectedHeap::do_full_collection(bool clear_all_soft_refs) {
 946    do_full_collection(clear_all_soft_refs, OldGen);
 947 }
 948 
 949 void GenCollectedHeap::do_full_collection(bool clear_all_soft_refs,
 950                                           GenerationType last_generation) {
 951   do_collection(true,                   // full
 952                 clear_all_soft_refs,    // clear_all_soft_refs
 953                 0,                      // size
 954                 false,                  // is_tlab
 955                 last_generation);       // last_generation
 956   // Hack XXX FIX ME !!!
 957   // A scavenge may not have been attempted, or may have
 958   // been attempted and failed, because the old gen was too full
 959   if (gc_cause() == GCCause::_gc_locker &amp;&amp; incremental_collection_failed()) {
 960     log_debug(gc, jni)("GC locker: Trying a full collection because scavenge failed");
 961     // This time allow the old gen to be collected as well
 962     do_collection(true,                // full
 963                   clear_all_soft_refs, // clear_all_soft_refs
 964                   0,                   // size
 965                   false,               // is_tlab
 966                   OldGen);             // last_generation
 967   }
 968 }
 969 
 970 bool GenCollectedHeap::is_in_young(oop p) {
 971   bool result = cast_from_oop&lt;HeapWord*&gt;(p) &lt; _old_gen-&gt;reserved().start();
 972   assert(result == _young_gen-&gt;is_in_reserved(p),
 973          "incorrect test - result=%d, p=" INTPTR_FORMAT, result, p2i((void*)p));
 974   return result;
 975 }
 976 
 977 // Returns "TRUE" iff "p" points into the committed areas of the heap.
 978 bool GenCollectedHeap::is_in(const void* p) const {
 979   return _young_gen-&gt;is_in(p) || _old_gen-&gt;is_in(p);
 980 }
 981 
 982 #ifdef ASSERT
 983 // Don't implement this by using is_in_young().  This method is used
 984 // in some cases to check that is_in_young() is correct.
 985 bool GenCollectedHeap::is_in_partial_collection(const void* p) {
 986   assert(is_in_reserved(p) || p == NULL,
 987     "Does not work if address is non-null and outside of the heap");
 988   return p &lt; _young_gen-&gt;reserved().end() &amp;&amp; p != NULL;
 989 }
 990 #endif
 991 
 992 void GenCollectedHeap::oop_iterate(OopIterateClosure* cl) {
 993   _young_gen-&gt;oop_iterate(cl);
 994   _old_gen-&gt;oop_iterate(cl);
 995 }
 996 
 997 void GenCollectedHeap::object_iterate(ObjectClosure* cl) {
 998   _young_gen-&gt;object_iterate(cl);
 999   _old_gen-&gt;object_iterate(cl);
1000 }
1001 
1002 Space* GenCollectedHeap::space_containing(const void* addr) const {
1003   Space* res = _young_gen-&gt;space_containing(addr);
1004   if (res != NULL) {
1005     return res;
1006   }
1007   res = _old_gen-&gt;space_containing(addr);
1008   assert(res != NULL, "Could not find containing space");
1009   return res;
1010 }
1011 
1012 HeapWord* GenCollectedHeap::block_start(const void* addr) const {
1013   assert(is_in_reserved(addr), "block_start of address outside of heap");
1014   if (_young_gen-&gt;is_in_reserved(addr)) {
1015     assert(_young_gen-&gt;is_in(addr), "addr should be in allocated part of generation");
1016     return _young_gen-&gt;block_start(addr);
1017   }
1018 
1019   assert(_old_gen-&gt;is_in_reserved(addr), "Some generation should contain the address");
1020   assert(_old_gen-&gt;is_in(addr), "addr should be in allocated part of generation");
1021   return _old_gen-&gt;block_start(addr);
1022 }
1023 
1024 bool GenCollectedHeap::block_is_obj(const HeapWord* addr) const {
1025   assert(is_in_reserved(addr), "block_is_obj of address outside of heap");
1026   assert(block_start(addr) == addr, "addr must be a block start");
1027   if (_young_gen-&gt;is_in_reserved(addr)) {
1028     return _young_gen-&gt;block_is_obj(addr);
1029   }
1030 
1031   assert(_old_gen-&gt;is_in_reserved(addr), "Some generation should contain the address");
1032   return _old_gen-&gt;block_is_obj(addr);
1033 }
1034 
1035 bool GenCollectedHeap::supports_tlab_allocation() const {
1036   assert(!_old_gen-&gt;supports_tlab_allocation(), "Old gen supports TLAB allocation?!");
1037   return _young_gen-&gt;supports_tlab_allocation();
1038 }
1039 
1040 size_t GenCollectedHeap::tlab_capacity(Thread* thr) const {
1041   assert(!_old_gen-&gt;supports_tlab_allocation(), "Old gen supports TLAB allocation?!");
1042   if (_young_gen-&gt;supports_tlab_allocation()) {
1043     return _young_gen-&gt;tlab_capacity();
1044   }
1045   return 0;
1046 }
1047 
1048 size_t GenCollectedHeap::tlab_used(Thread* thr) const {
1049   assert(!_old_gen-&gt;supports_tlab_allocation(), "Old gen supports TLAB allocation?!");
1050   if (_young_gen-&gt;supports_tlab_allocation()) {
1051     return _young_gen-&gt;tlab_used();
1052   }
1053   return 0;
1054 }
1055 
1056 size_t GenCollectedHeap::unsafe_max_tlab_alloc(Thread* thr) const {
1057   assert(!_old_gen-&gt;supports_tlab_allocation(), "Old gen supports TLAB allocation?!");
1058   if (_young_gen-&gt;supports_tlab_allocation()) {
1059     return _young_gen-&gt;unsafe_max_tlab_alloc();
1060   }
1061   return 0;
1062 }
1063 
1064 HeapWord* GenCollectedHeap::allocate_new_tlab(size_t min_size,
1065                                               size_t requested_size,
1066                                               size_t* actual_size) {
1067   bool gc_overhead_limit_was_exceeded;
1068   HeapWord* result = mem_allocate_work(requested_size /* size */,
1069                                        true /* is_tlab */,
1070                                        &amp;gc_overhead_limit_was_exceeded);
1071   if (result != NULL) {
1072     *actual_size = requested_size;
1073   }
1074 
1075   return result;
1076 }
1077 
1078 // Requires "*prev_ptr" to be non-NULL.  Deletes and a block of minimal size
1079 // from the list headed by "*prev_ptr".
1080 static ScratchBlock *removeSmallestScratch(ScratchBlock **prev_ptr) {
1081   bool first = true;
1082   size_t min_size = 0;   // "first" makes this conceptually infinite.
1083   ScratchBlock **smallest_ptr, *smallest;
1084   ScratchBlock  *cur = *prev_ptr;
1085   while (cur) {
1086     assert(*prev_ptr == cur, "just checking");
1087     if (first || cur-&gt;num_words &lt; min_size) {
1088       smallest_ptr = prev_ptr;
1089       smallest     = cur;
1090       min_size     = smallest-&gt;num_words;
1091       first        = false;
1092     }
1093     prev_ptr = &amp;cur-&gt;next;
1094     cur     =  cur-&gt;next;
1095   }
1096   smallest      = *smallest_ptr;
1097   *smallest_ptr = smallest-&gt;next;
1098   return smallest;
1099 }
1100 
1101 // Sort the scratch block list headed by res into decreasing size order,
1102 // and set "res" to the result.
1103 static void sort_scratch_list(ScratchBlock*&amp; list) {
1104   ScratchBlock* sorted = NULL;
1105   ScratchBlock* unsorted = list;
1106   while (unsorted) {
1107     ScratchBlock *smallest = removeSmallestScratch(&amp;unsorted);
1108     smallest-&gt;next  = sorted;
1109     sorted          = smallest;
1110   }
1111   list = sorted;
1112 }
1113 
1114 ScratchBlock* GenCollectedHeap::gather_scratch(Generation* requestor,
1115                                                size_t max_alloc_words) {
1116   ScratchBlock* res = NULL;
1117   _young_gen-&gt;contribute_scratch(res, requestor, max_alloc_words);
1118   _old_gen-&gt;contribute_scratch(res, requestor, max_alloc_words);
1119   sort_scratch_list(res);
1120   return res;
1121 }
1122 
1123 void GenCollectedHeap::release_scratch() {
1124   _young_gen-&gt;reset_scratch();
1125   _old_gen-&gt;reset_scratch();
1126 }
1127 
1128 class GenPrepareForVerifyClosure: public GenCollectedHeap::GenClosure {
1129   void do_generation(Generation* gen) {
1130     gen-&gt;prepare_for_verify();
1131   }
1132 };
1133 
1134 void GenCollectedHeap::prepare_for_verify() {
1135   ensure_parsability(false);        // no need to retire TLABs
1136   GenPrepareForVerifyClosure blk;
1137   generation_iterate(&amp;blk, false);
1138 }
1139 
1140 void GenCollectedHeap::generation_iterate(GenClosure* cl,
1141                                           bool old_to_young) {
1142   if (old_to_young) {
1143     cl-&gt;do_generation(_old_gen);
1144     cl-&gt;do_generation(_young_gen);
1145   } else {
1146     cl-&gt;do_generation(_young_gen);
1147     cl-&gt;do_generation(_old_gen);
1148   }
1149 }
1150 
1151 bool GenCollectedHeap::is_maximal_no_gc() const {
1152   return _young_gen-&gt;is_maximal_no_gc() &amp;&amp; _old_gen-&gt;is_maximal_no_gc();
1153 }
1154 
1155 void GenCollectedHeap::save_marks() {
1156   _young_gen-&gt;save_marks();
1157   _old_gen-&gt;save_marks();
1158 }
1159 
1160 GenCollectedHeap* GenCollectedHeap::heap() {
1161   // SerialHeap is the only subtype of GenCollectedHeap.
1162   return named_heap&lt;GenCollectedHeap&gt;(CollectedHeap::Serial);
1163 }
1164 
1165 #if INCLUDE_SERIALGC
1166 void GenCollectedHeap::prepare_for_compaction() {
1167   // Start by compacting into same gen.
1168   CompactPoint cp(_old_gen);
1169   _old_gen-&gt;prepare_for_compaction(&amp;cp);
1170   _young_gen-&gt;prepare_for_compaction(&amp;cp);
1171 }
1172 #endif // INCLUDE_SERIALGC
1173 
1174 void GenCollectedHeap::verify(VerifyOption option /* ignored */) {
1175   log_debug(gc, verify)("%s", _old_gen-&gt;name());
1176   _old_gen-&gt;verify();
1177 
1178   log_debug(gc, verify)("%s", _old_gen-&gt;name());
1179   _young_gen-&gt;verify();
1180 
1181   log_debug(gc, verify)("RemSet");
1182   rem_set()-&gt;verify();
1183 }
1184 
1185 void GenCollectedHeap::print_on(outputStream* st) const {
1186   if (_young_gen != NULL) {
1187     _young_gen-&gt;print_on(st);
1188   }
1189   if (_old_gen != NULL) {
1190     _old_gen-&gt;print_on(st);
1191   }
1192   MetaspaceUtils::print_on(st);
1193 }
1194 
1195 void GenCollectedHeap::gc_threads_do(ThreadClosure* tc) const {
1196 }
1197 
1198 bool GenCollectedHeap::print_location(outputStream* st, void* addr) const {
1199   return BlockLocationPrinter&lt;GenCollectedHeap&gt;::print_location(st, addr);
1200 }
1201 
1202 void GenCollectedHeap::print_tracing_info() const {
1203   if (log_is_enabled(Debug, gc, heap, exit)) {
1204     LogStreamHandle(Debug, gc, heap, exit) lsh;
1205     _young_gen-&gt;print_summary_info_on(&amp;lsh);
1206     _old_gen-&gt;print_summary_info_on(&amp;lsh);
1207   }
1208 }
1209 
1210 void GenCollectedHeap::print_heap_change(const PreGenGCValues&amp; pre_gc_values) const {
1211   const DefNewGeneration* const def_new_gen = (DefNewGeneration*) young_gen();
1212 
1213   log_info(gc, heap)(HEAP_CHANGE_FORMAT" "
1214                      HEAP_CHANGE_FORMAT" "
1215                      HEAP_CHANGE_FORMAT,
1216                      HEAP_CHANGE_FORMAT_ARGS(def_new_gen-&gt;short_name(),
1217                                              pre_gc_values.young_gen_used(),
1218                                              pre_gc_values.young_gen_capacity(),
1219                                              def_new_gen-&gt;used(),
1220                                              def_new_gen-&gt;capacity()),
1221                      HEAP_CHANGE_FORMAT_ARGS("Eden",
1222                                              pre_gc_values.eden_used(),
1223                                              pre_gc_values.eden_capacity(),
1224                                              def_new_gen-&gt;eden()-&gt;used(),
1225                                              def_new_gen-&gt;eden()-&gt;capacity()),
1226                      HEAP_CHANGE_FORMAT_ARGS("From",
1227                                              pre_gc_values.from_used(),
1228                                              pre_gc_values.from_capacity(),
1229                                              def_new_gen-&gt;from()-&gt;used(),
1230                                              def_new_gen-&gt;from()-&gt;capacity()));
1231   log_info(gc, heap)(HEAP_CHANGE_FORMAT,
1232                      HEAP_CHANGE_FORMAT_ARGS(old_gen()-&gt;short_name(),
1233                                              pre_gc_values.old_gen_used(),
1234                                              pre_gc_values.old_gen_capacity(),
1235                                              old_gen()-&gt;used(),
1236                                              old_gen()-&gt;capacity()));
1237   MetaspaceUtils::print_metaspace_change(pre_gc_values.metaspace_sizes());
1238 }
1239 
1240 class GenGCPrologueClosure: public GenCollectedHeap::GenClosure {
1241  private:
1242   bool _full;
1243  public:
1244   void do_generation(Generation* gen) {
1245     gen-&gt;gc_prologue(_full);
1246   }
1247   GenGCPrologueClosure(bool full) : _full(full) {};
1248 };
1249 
1250 void GenCollectedHeap::gc_prologue(bool full) {
1251   assert(InlineCacheBuffer::is_empty(), "should have cleaned up ICBuffer");
1252 
1253   // Fill TLAB's and such
1254   ensure_parsability(true);   // retire TLABs
1255 
1256   // Walk generations
1257   GenGCPrologueClosure blk(full);
1258   generation_iterate(&amp;blk, false);  // not old-to-young.
1259 };
1260 
1261 class GenGCEpilogueClosure: public GenCollectedHeap::GenClosure {
1262  private:
1263   bool _full;
1264  public:
1265   void do_generation(Generation* gen) {
1266     gen-&gt;gc_epilogue(_full);
1267   }
1268   GenGCEpilogueClosure(bool full) : _full(full) {};
1269 };
1270 
1271 void GenCollectedHeap::gc_epilogue(bool full) {
1272 #if COMPILER2_OR_JVMCI
1273   assert(DerivedPointerTable::is_empty(), "derived pointer present");
1274   size_t actual_gap = pointer_delta((HeapWord*) (max_uintx-3), *(end_addr()));
1275   guarantee(is_client_compilation_mode_vm() || actual_gap &gt; (size_t)FastAllocateSizeLimit, "inline allocation wraps");
1276 #endif // COMPILER2_OR_JVMCI
1277 
1278   resize_all_tlabs();
1279 
1280   GenGCEpilogueClosure blk(full);
1281   generation_iterate(&amp;blk, false);  // not old-to-young.
1282 
1283   if (!CleanChunkPoolAsync) {
1284     Chunk::clean_chunk_pool();
1285   }
1286 
1287   MetaspaceCounters::update_performance_counters();
1288   CompressedClassSpaceCounters::update_performance_counters();
1289 };
1290 
1291 #ifndef PRODUCT
1292 class GenGCSaveTopsBeforeGCClosure: public GenCollectedHeap::GenClosure {
1293  private:
1294  public:
1295   void do_generation(Generation* gen) {
1296     gen-&gt;record_spaces_top();
1297   }
1298 };
1299 
1300 void GenCollectedHeap::record_gen_tops_before_GC() {
1301   if (ZapUnusedHeapArea) {
1302     GenGCSaveTopsBeforeGCClosure blk;
1303     generation_iterate(&amp;blk, false);  // not old-to-young.
1304   }
1305 }
1306 #endif  // not PRODUCT
1307 
1308 class GenEnsureParsabilityClosure: public GenCollectedHeap::GenClosure {
1309  public:
1310   void do_generation(Generation* gen) {
1311     gen-&gt;ensure_parsability();
1312   }
1313 };
1314 
1315 void GenCollectedHeap::ensure_parsability(bool retire_tlabs) {
1316   CollectedHeap::ensure_parsability(retire_tlabs);
1317   GenEnsureParsabilityClosure ep_cl;
1318   generation_iterate(&amp;ep_cl, false);
1319 }
1320 
1321 oop GenCollectedHeap::handle_failed_promotion(Generation* old_gen,
1322                                               oop obj,
1323                                               size_t obj_size) {
1324   guarantee(old_gen == _old_gen, "We only get here with an old generation");
1325   assert(obj_size == (size_t)obj-&gt;size(), "bad obj_size passed in");
1326   HeapWord* result = NULL;
1327 
1328   result = old_gen-&gt;expand_and_allocate(obj_size, false);
1329 
1330   if (result != NULL) {
1331     Copy::aligned_disjoint_words(cast_from_oop&lt;HeapWord*&gt;(obj), result, obj_size);
1332   }
1333   return oop(result);
1334 }
<a name="3" id="anc3"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="3" type="hidden" /></form></body></html>
