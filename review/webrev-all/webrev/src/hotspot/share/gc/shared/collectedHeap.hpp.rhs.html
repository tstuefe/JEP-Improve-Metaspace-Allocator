<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre>rev <a href="https://bugs.openjdk.java.net/browse/JDK-60818">60818</a> : imported patch jep387-all.patch</pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 2001, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #ifndef SHARE_GC_SHARED_COLLECTEDHEAP_HPP
  26 #define SHARE_GC_SHARED_COLLECTEDHEAP_HPP
  27 
  28 #include "gc/shared/gcCause.hpp"
  29 #include "gc/shared/gcWhen.hpp"
  30 #include "gc/shared/verifyOption.hpp"
  31 #include "memory/allocation.hpp"
  32 #include "memory/heapInspection.hpp"
<a name="1" id="anc1"></a><span class="new">  33 #include "memory/metaspace.hpp"</span>
  34 #include "memory/universe.hpp"
  35 #include "runtime/handles.hpp"
  36 #include "runtime/perfData.hpp"
  37 #include "runtime/safepoint.hpp"
  38 #include "services/memoryUsage.hpp"
  39 #include "utilities/debug.hpp"
  40 #include "utilities/events.hpp"
  41 #include "utilities/formatBuffer.hpp"
  42 #include "utilities/growableArray.hpp"
  43 
  44 // A "CollectedHeap" is an implementation of a java heap for HotSpot.  This
  45 // is an abstract class: there may be many different kinds of heaps.  This
  46 // class defines the functions that a heap must implement, and contains
  47 // infrastructure common to all heaps.
  48 
  49 class AbstractGangTask;
  50 class AdaptiveSizePolicy;
  51 class BarrierSet;
  52 class GCHeapSummary;
  53 class GCTimer;
  54 class GCTracer;
  55 class GCMemoryManager;
  56 class MemoryPool;
  57 class MetaspaceSummary;
  58 class ReservedHeapSpace;
  59 class SoftRefPolicy;
  60 class Thread;
  61 class ThreadClosure;
  62 class VirtualSpaceSummary;
  63 class WorkGang;
  64 class nmethod;
  65 
  66 class GCMessage : public FormatBuffer&lt;1024&gt; {
  67  public:
  68   bool is_before;
  69 
  70  public:
  71   GCMessage() {}
  72 };
  73 
  74 class CollectedHeap;
  75 
  76 class GCHeapLog : public EventLogBase&lt;GCMessage&gt; {
  77  private:
  78   void log_heap(CollectedHeap* heap, bool before);
  79 
  80  public:
  81   GCHeapLog() : EventLogBase&lt;GCMessage&gt;("GC Heap History", "gc") {}
  82 
  83   void log_heap_before(CollectedHeap* heap) {
  84     log_heap(heap, true);
  85   }
  86   void log_heap_after(CollectedHeap* heap) {
  87     log_heap(heap, false);
  88   }
  89 };
  90 
  91 class ParallelObjectIterator : public CHeapObj&lt;mtGC&gt; {
  92 public:
  93   virtual void object_iterate(ObjectClosure* cl, uint worker_id) = 0;
  94 };
  95 
  96 //
  97 // CollectedHeap
  98 //   GenCollectedHeap
  99 //     SerialHeap
 100 //   G1CollectedHeap
 101 //   ParallelScavengeHeap
 102 //   ShenandoahHeap
 103 //   ZCollectedHeap
 104 //
 105 class CollectedHeap : public CHeapObj&lt;mtInternal&gt; {
 106   friend class VMStructs;
 107   friend class JVMCIVMStructs;
 108   friend class IsGCActiveMark; // Block structured external access to _is_gc_active
 109   friend class MemAllocator;
 110 
 111  private:
 112   GCHeapLog* _gc_heap_log;
 113 
 114  protected:
 115   // Not used by all GCs
 116   MemRegion _reserved;
 117 
 118   bool _is_gc_active;
 119 
 120   // Used for filler objects (static, but initialized in ctor).
 121   static size_t _filler_array_max_size;
 122 
 123   // Last time the whole heap has been examined in support of RMI
 124   // MaxObjectInspectionAge.
 125   // This timestamp must be monotonically non-decreasing to avoid
 126   // time-warp warnings.
 127   jlong _last_whole_heap_examined_time_ns;
 128 
 129   unsigned int _total_collections;          // ... started
 130   unsigned int _total_full_collections;     // ... started
 131   NOT_PRODUCT(volatile size_t _promotion_failure_alot_count;)
 132   NOT_PRODUCT(volatile size_t _promotion_failure_alot_gc_number;)
 133 
 134   // Reason for current garbage collection.  Should be set to
 135   // a value reflecting no collection between collections.
 136   GCCause::Cause _gc_cause;
 137   GCCause::Cause _gc_lastcause;
 138   PerfStringVariable* _perf_gc_cause;
 139   PerfStringVariable* _perf_gc_lastcause;
 140 
 141   // Constructor
 142   CollectedHeap();
 143 
 144   // Create a new tlab. All TLAB allocations must go through this.
 145   // To allow more flexible TLAB allocations min_size specifies
 146   // the minimum size needed, while requested_size is the requested
 147   // size based on ergonomics. The actually allocated size will be
 148   // returned in actual_size.
 149   virtual HeapWord* allocate_new_tlab(size_t min_size,
 150                                       size_t requested_size,
 151                                       size_t* actual_size);
 152 
 153   // Reinitialize tlabs before resuming mutators.
 154   virtual void resize_all_tlabs();
 155 
 156   // Raw memory allocation facilities
 157   // The obj and array allocate methods are covers for these methods.
 158   // mem_allocate() should never be
 159   // called to allocate TLABs, only individual objects.
 160   virtual HeapWord* mem_allocate(size_t size,
 161                                  bool* gc_overhead_limit_was_exceeded) = 0;
 162 
 163   // Filler object utilities.
 164   static inline size_t filler_array_hdr_size();
 165   static inline size_t filler_array_min_size();
 166 
 167   DEBUG_ONLY(static void fill_args_check(HeapWord* start, size_t words);)
 168   DEBUG_ONLY(static void zap_filler_array(HeapWord* start, size_t words, bool zap = true);)
 169 
 170   // Fill with a single array; caller must ensure filler_array_min_size() &lt;=
 171   // words &lt;= filler_array_max_size().
 172   static inline void fill_with_array(HeapWord* start, size_t words, bool zap = true);
 173 
 174   // Fill with a single object (either an int array or a java.lang.Object).
 175   static inline void fill_with_object_impl(HeapWord* start, size_t words, bool zap = true);
 176 
 177   virtual void trace_heap(GCWhen::Type when, const GCTracer* tracer);
 178 
 179   // Verification functions
 180   virtual void check_for_non_bad_heap_word_value(HeapWord* addr, size_t size)
 181     PRODUCT_RETURN;
 182   debug_only(static void check_for_valid_allocation_state();)
 183 
 184  public:
 185   enum Name {
 186     None,
 187     Serial,
 188     Parallel,
 189     G1,
 190     Epsilon,
 191     Z,
 192     Shenandoah
 193   };
 194 
 195  protected:
 196   // Get a pointer to the derived heap object.  Used to implement
 197   // derived class heap() functions rather than being called directly.
 198   template&lt;typename T&gt;
 199   static T* named_heap(Name kind) {
 200     CollectedHeap* heap = Universe::heap();
 201     assert(heap != NULL, "Uninitialized heap");
 202     assert(kind == heap-&gt;kind(), "Heap kind %u should be %u",
 203            static_cast&lt;uint&gt;(heap-&gt;kind()), static_cast&lt;uint&gt;(kind));
 204     return static_cast&lt;T*&gt;(heap);
 205   }
 206 
 207  public:
 208 
 209   static inline size_t filler_array_max_size() {
 210     return _filler_array_max_size;
 211   }
 212 
 213   virtual Name kind() const = 0;
 214 
 215   virtual const char* name() const = 0;
 216 
 217   /**
 218    * Returns JNI error code JNI_ENOMEM if memory could not be allocated,
 219    * and JNI_OK on success.
 220    */
 221   virtual jint initialize() = 0;
 222 
 223   // In many heaps, there will be a need to perform some initialization activities
 224   // after the Universe is fully formed, but before general heap allocation is allowed.
 225   // This is the correct place to place such initialization methods.
 226   virtual void post_initialize();
 227 
 228   // Stop any onging concurrent work and prepare for exit.
 229   virtual void stop() {}
 230 
 231   // Stop and resume concurrent GC threads interfering with safepoint operations
 232   virtual void safepoint_synchronize_begin() {}
 233   virtual void safepoint_synchronize_end() {}
 234 
 235   void initialize_reserved_region(const ReservedHeapSpace&amp; rs);
 236 
 237   virtual size_t capacity() const = 0;
 238   virtual size_t used() const = 0;
 239 
 240   // Returns unused capacity.
 241   virtual size_t unused() const;
 242 
 243   // Return "true" if the part of the heap that allocates Java
 244   // objects has reached the maximal committed limit that it can
 245   // reach, without a garbage collection.
 246   virtual bool is_maximal_no_gc() const = 0;
 247 
 248   // Support for java.lang.Runtime.maxMemory():  return the maximum amount of
 249   // memory that the vm could make available for storing 'normal' java objects.
 250   // This is based on the reserved address space, but should not include space
 251   // that the vm uses internally for bookkeeping or temporary storage
 252   // (e.g., in the case of the young gen, one of the survivor
 253   // spaces).
 254   virtual size_t max_capacity() const = 0;
 255 
 256   // Returns "TRUE" iff "p" points into the committed areas of the heap.
 257   // This method can be expensive so avoid using it in performance critical
 258   // code.
 259   virtual bool is_in(const void* p) const = 0;
 260 
 261   DEBUG_ONLY(bool is_in_or_null(const void* p) const { return p == NULL || is_in(p); })
 262 
 263   virtual uint32_t hash_oop(oop obj) const;
 264 
 265   void set_gc_cause(GCCause::Cause v) {
 266      if (UsePerfData) {
 267        _gc_lastcause = _gc_cause;
 268        _perf_gc_lastcause-&gt;set_value(GCCause::to_string(_gc_lastcause));
 269        _perf_gc_cause-&gt;set_value(GCCause::to_string(v));
 270      }
 271     _gc_cause = v;
 272   }
 273   GCCause::Cause gc_cause() { return _gc_cause; }
 274 
 275   oop obj_allocate(Klass* klass, int size, TRAPS);
 276   virtual oop array_allocate(Klass* klass, int size, int length, bool do_zero, TRAPS);
 277   oop class_allocate(Klass* klass, int size, TRAPS);
 278 
 279   // Utilities for turning raw memory into filler objects.
 280   //
 281   // min_fill_size() is the smallest region that can be filled.
 282   // fill_with_objects() can fill arbitrary-sized regions of the heap using
 283   // multiple objects.  fill_with_object() is for regions known to be smaller
 284   // than the largest array of integers; it uses a single object to fill the
 285   // region and has slightly less overhead.
 286   static size_t min_fill_size() {
 287     return size_t(align_object_size(oopDesc::header_size()));
 288   }
 289 
 290   static void fill_with_objects(HeapWord* start, size_t words, bool zap = true);
 291 
 292   static void fill_with_object(HeapWord* start, size_t words, bool zap = true);
 293   static void fill_with_object(MemRegion region, bool zap = true) {
 294     fill_with_object(region.start(), region.word_size(), zap);
 295   }
 296   static void fill_with_object(HeapWord* start, HeapWord* end, bool zap = true) {
 297     fill_with_object(start, pointer_delta(end, start), zap);
 298   }
 299 
 300   virtual void fill_with_dummy_object(HeapWord* start, HeapWord* end, bool zap);
 301   virtual size_t min_dummy_object_size() const;
 302   size_t tlab_alloc_reserve() const;
 303 
 304   // Return the address "addr" aligned by "alignment_in_bytes" if such
 305   // an address is below "end".  Return NULL otherwise.
 306   inline static HeapWord* align_allocation_or_fail(HeapWord* addr,
 307                                                    HeapWord* end,
 308                                                    unsigned short alignment_in_bytes);
 309 
 310   // Some heaps may offer a contiguous region for shared non-blocking
 311   // allocation, via inlined code (by exporting the address of the top and
 312   // end fields defining the extent of the contiguous allocation region.)
 313 
 314   // This function returns "true" iff the heap supports this kind of
 315   // allocation.  (Default is "no".)
 316   virtual bool supports_inline_contig_alloc() const {
 317     return false;
 318   }
 319   // These functions return the addresses of the fields that define the
 320   // boundaries of the contiguous allocation area.  (These fields should be
 321   // physically near to one another.)
 322   virtual HeapWord* volatile* top_addr() const {
 323     guarantee(false, "inline contiguous allocation not supported");
 324     return NULL;
 325   }
 326   virtual HeapWord** end_addr() const {
 327     guarantee(false, "inline contiguous allocation not supported");
 328     return NULL;
 329   }
 330 
 331   // Some heaps may be in an unparseable state at certain times between
 332   // collections. This may be necessary for efficient implementation of
 333   // certain allocation-related activities. Calling this function before
 334   // attempting to parse a heap ensures that the heap is in a parsable
 335   // state (provided other concurrent activity does not introduce
 336   // unparsability). It is normally expected, therefore, that this
 337   // method is invoked with the world stopped.
 338   // NOTE: if you override this method, make sure you call
 339   // super::ensure_parsability so that the non-generational
 340   // part of the work gets done. See implementation of
 341   // CollectedHeap::ensure_parsability and, for instance,
 342   // that of GenCollectedHeap::ensure_parsability().
 343   // The argument "retire_tlabs" controls whether existing TLABs
 344   // are merely filled or also retired, thus preventing further
 345   // allocation from them and necessitating allocation of new TLABs.
 346   virtual void ensure_parsability(bool retire_tlabs);
 347 
 348   // Section on thread-local allocation buffers (TLABs)
 349   // If the heap supports thread-local allocation buffers, it should override
 350   // the following methods:
 351   // Returns "true" iff the heap supports thread-local allocation buffers.
 352   // The default is "no".
 353   virtual bool supports_tlab_allocation() const = 0;
 354 
 355   // The amount of space available for thread-local allocation buffers.
 356   virtual size_t tlab_capacity(Thread *thr) const = 0;
 357 
 358   // The amount of used space for thread-local allocation buffers for the given thread.
 359   virtual size_t tlab_used(Thread *thr) const = 0;
 360 
 361   virtual size_t max_tlab_size() const;
 362 
 363   // An estimate of the maximum allocation that could be performed
 364   // for thread-local allocation buffers without triggering any
 365   // collection or expansion activity.
 366   virtual size_t unsafe_max_tlab_alloc(Thread *thr) const {
 367     guarantee(false, "thread-local allocation buffers not supported");
 368     return 0;
 369   }
 370 
 371   // Perform a collection of the heap; intended for use in implementing
 372   // "System.gc".  This probably implies as full a collection as the
 373   // "CollectedHeap" supports.
 374   virtual void collect(GCCause::Cause cause) = 0;
 375 
 376   // Perform a full collection
 377   virtual void do_full_collection(bool clear_all_soft_refs) = 0;
 378 
 379   // This interface assumes that it's being called by the
 380   // vm thread. It collects the heap assuming that the
 381   // heap lock is already held and that we are executing in
 382   // the context of the vm thread.
 383   virtual void collect_as_vm_thread(GCCause::Cause cause);
 384 
 385   virtual MetaWord* satisfy_failed_metadata_allocation(ClassLoaderData* loader_data,
 386                                                        size_t size,
 387                                                        Metaspace::MetadataType mdtype);
 388 
 389   // Returns "true" iff there is a stop-world GC in progress.  (I assume
 390   // that it should answer "false" for the concurrent part of a concurrent
 391   // collector -- dld).
 392   bool is_gc_active() const { return _is_gc_active; }
 393 
 394   // Total number of GC collections (started)
 395   unsigned int total_collections() const { return _total_collections; }
 396   unsigned int total_full_collections() const { return _total_full_collections;}
 397 
 398   // Increment total number of GC collections (started)
 399   void increment_total_collections(bool full = false) {
 400     _total_collections++;
 401     if (full) {
 402       increment_total_full_collections();
 403     }
 404   }
 405 
 406   void increment_total_full_collections() { _total_full_collections++; }
 407 
 408   // Return the SoftRefPolicy for the heap;
 409   virtual SoftRefPolicy* soft_ref_policy() = 0;
 410 
 411   virtual MemoryUsage memory_usage();
 412   virtual GrowableArray&lt;GCMemoryManager*&gt; memory_managers() = 0;
 413   virtual GrowableArray&lt;MemoryPool*&gt; memory_pools() = 0;
 414 
 415   // Iterate over all objects, calling "cl.do_object" on each.
 416   virtual void object_iterate(ObjectClosure* cl) = 0;
 417 
 418   virtual ParallelObjectIterator* parallel_object_iterator(uint thread_num) {
 419     return NULL;
 420   }
 421 
 422   // Keep alive an object that was loaded with AS_NO_KEEPALIVE.
 423   virtual void keep_alive(oop obj) {}
 424 
 425   // Perform any cleanup actions necessary before allowing a verification.
 426   virtual void prepare_for_verify() = 0;
 427 
 428   // Returns the longest time (in ms) that has elapsed since the last
 429   // time that the whole heap has been examined by a garbage collection.
 430   jlong millis_since_last_whole_heap_examined();
 431   // GC should call this when the next whole heap analysis has completed to
 432   // satisfy above requirement.
 433   void record_whole_heap_examined_timestamp();
 434 
 435  private:
 436   // Generate any dumps preceding or following a full gc
 437   void full_gc_dump(GCTimer* timer, bool before);
 438 
 439   virtual void initialize_serviceability() = 0;
 440 
 441  public:
 442   void pre_full_gc_dump(GCTimer* timer);
 443   void post_full_gc_dump(GCTimer* timer);
 444 
 445   virtual VirtualSpaceSummary create_heap_space_summary();
 446   GCHeapSummary create_heap_summary();
 447 
 448   MetaspaceSummary create_metaspace_summary();
 449 
 450   // Print heap information on the given outputStream.
 451   virtual void print_on(outputStream* st) const = 0;
 452   // The default behavior is to call print_on() on tty.
 453   virtual void print() const;
 454 
 455   // Print more detailed heap information on the given
 456   // outputStream. The default behavior is to call print_on(). It is
 457   // up to each subclass to override it and add any additional output
 458   // it needs.
 459   virtual void print_extended_on(outputStream* st) const {
 460     print_on(st);
 461   }
 462 
 463   virtual void print_on_error(outputStream* st) const;
 464 
 465   // Used to print information about locations in the hs_err file.
 466   virtual bool print_location(outputStream* st, void* addr) const = 0;
 467 
 468   // Iterator for all GC threads (other than VM thread)
 469   virtual void gc_threads_do(ThreadClosure* tc) const = 0;
 470 
 471   // Print any relevant tracing info that flags imply.
 472   // Default implementation does nothing.
 473   virtual void print_tracing_info() const = 0;
 474 
 475   void print_heap_before_gc();
 476   void print_heap_after_gc();
 477 
 478   // Registering and unregistering an nmethod (compiled code) with the heap.
 479   virtual void register_nmethod(nmethod* nm) = 0;
 480   virtual void unregister_nmethod(nmethod* nm) = 0;
 481   // Callback for when nmethod is about to be deleted.
 482   virtual void flush_nmethod(nmethod* nm) = 0;
 483   virtual void verify_nmethod(nmethod* nm) = 0;
 484 
 485   void trace_heap_before_gc(const GCTracer* gc_tracer);
 486   void trace_heap_after_gc(const GCTracer* gc_tracer);
 487 
 488   // Heap verification
 489   virtual void verify(VerifyOption option) = 0;
 490 
 491   // Return true if concurrent gc control via WhiteBox is supported by
 492   // this collector.  The default implementation returns false.
 493   virtual bool supports_concurrent_gc_breakpoints() const;
 494 
 495   // Provides a thread pool to SafepointSynchronize to use
 496   // for parallel safepoint cleanup.
 497   // GCs that use a GC worker thread pool may want to share
 498   // it for use during safepoint cleanup. This is only possible
 499   // if the GC can pause and resume concurrent work (e.g. G1
 500   // concurrent marking) for an intermittent non-GC safepoint.
 501   // If this method returns NULL, SafepointSynchronize will
 502   // perform cleanup tasks serially in the VMThread.
 503   virtual WorkGang* safepoint_workers() { return NULL; }
 504 
 505   // Support for object pinning. This is used by JNI Get*Critical()
 506   // and Release*Critical() family of functions. If supported, the GC
 507   // must guarantee that pinned objects never move.
 508   virtual bool supports_object_pinning() const;
 509   virtual oop pin_object(JavaThread* thread, oop obj);
 510   virtual void unpin_object(JavaThread* thread, oop obj);
 511 
 512   // Deduplicate the string, iff the GC supports string deduplication.
 513   virtual void deduplicate_string(oop str);
 514 
 515   virtual bool is_oop(oop object) const;
 516 
 517   // Non product verification and debugging.
 518 #ifndef PRODUCT
 519   // Support for PromotionFailureALot.  Return true if it's time to cause a
 520   // promotion failure.  The no-argument version uses
 521   // this-&gt;_promotion_failure_alot_count as the counter.
 522   bool promotion_should_fail(volatile size_t* count);
 523   bool promotion_should_fail();
 524 
 525   // Reset the PromotionFailureALot counters.  Should be called at the end of a
 526   // GC in which promotion failure occurred.
 527   void reset_promotion_should_fail(volatile size_t* count);
 528   void reset_promotion_should_fail();
 529 #endif  // #ifndef PRODUCT
 530 };
 531 
 532 // Class to set and reset the GC cause for a CollectedHeap.
 533 
 534 class GCCauseSetter : StackObj {
 535   CollectedHeap* _heap;
 536   GCCause::Cause _previous_cause;
 537  public:
 538   GCCauseSetter(CollectedHeap* heap, GCCause::Cause cause) {
 539     _heap = heap;
 540     _previous_cause = _heap-&gt;gc_cause();
 541     _heap-&gt;set_gc_cause(cause);
 542   }
 543 
 544   ~GCCauseSetter() {
 545     _heap-&gt;set_gc_cause(_previous_cause);
 546   }
 547 };
 548 
 549 #endif // SHARE_GC_SHARED_COLLECTEDHEAP_HPP
<a name="2" id="anc2"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="2" type="hidden" /></form></body></html>
