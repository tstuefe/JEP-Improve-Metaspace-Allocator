<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre>rev <a href="https://bugs.openjdk.java.net/browse/JDK-60800">60800</a> : imported patch jep387-all.patch</pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 2001, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "classfile/classLoaderDataGraph.hpp"
  27 #include "classfile/metadataOnStackMark.hpp"
  28 #include "classfile/stringTable.hpp"
  29 #include "code/codeCache.hpp"
  30 #include "code/icBuffer.hpp"
  31 #include "gc/g1/g1Allocator.inline.hpp"
  32 #include "gc/g1/g1Arguments.hpp"
  33 #include "gc/g1/g1BarrierSet.hpp"
  34 #include "gc/g1/g1CardTableEntryClosure.hpp"
  35 #include "gc/g1/g1CollectedHeap.inline.hpp"
  36 #include "gc/g1/g1CollectionSet.hpp"
  37 #include "gc/g1/g1CollectorState.hpp"
  38 #include "gc/g1/g1ConcurrentRefine.hpp"
  39 #include "gc/g1/g1ConcurrentRefineThread.hpp"
  40 #include "gc/g1/g1ConcurrentMarkThread.inline.hpp"
  41 #include "gc/g1/g1DirtyCardQueue.hpp"
  42 #include "gc/g1/g1EvacStats.inline.hpp"
  43 #include "gc/g1/g1FullCollector.hpp"
  44 #include "gc/g1/g1GCParPhaseTimesTracker.hpp"
  45 #include "gc/g1/g1GCPhaseTimes.hpp"
  46 #include "gc/g1/g1HeapSizingPolicy.hpp"
  47 #include "gc/g1/g1HeapTransition.hpp"
  48 #include "gc/g1/g1HeapVerifier.hpp"
  49 #include "gc/g1/g1HotCardCache.hpp"
  50 #include "gc/g1/g1InitLogger.hpp"
  51 #include "gc/g1/g1MemoryPool.hpp"
  52 #include "gc/g1/g1OopClosures.inline.hpp"
  53 #include "gc/g1/g1ParallelCleaning.hpp"
  54 #include "gc/g1/g1ParScanThreadState.inline.hpp"
  55 #include "gc/g1/g1Policy.hpp"
  56 #include "gc/g1/g1RedirtyCardsQueue.hpp"
  57 #include "gc/g1/g1RegionToSpaceMapper.hpp"
  58 #include "gc/g1/g1RemSet.hpp"
  59 #include "gc/g1/g1RootClosures.hpp"
  60 #include "gc/g1/g1RootProcessor.hpp"
  61 #include "gc/g1/g1SATBMarkQueueSet.hpp"
  62 #include "gc/g1/g1StringDedup.hpp"
  63 #include "gc/g1/g1ThreadLocalData.hpp"
  64 #include "gc/g1/g1Trace.hpp"
  65 #include "gc/g1/g1YCTypes.hpp"
  66 #include "gc/g1/g1ServiceThread.hpp"
  67 #include "gc/g1/g1VMOperations.hpp"
  68 #include "gc/g1/heapRegion.inline.hpp"
  69 #include "gc/g1/heapRegionRemSet.hpp"
  70 #include "gc/g1/heapRegionSet.inline.hpp"
  71 #include "gc/shared/concurrentGCBreakpoints.hpp"
  72 #include "gc/shared/gcBehaviours.hpp"
  73 #include "gc/shared/gcHeapSummary.hpp"
  74 #include "gc/shared/gcId.hpp"
  75 #include "gc/shared/gcLocker.hpp"
  76 #include "gc/shared/gcTimer.hpp"
  77 #include "gc/shared/gcTraceTime.inline.hpp"
  78 #include "gc/shared/generationSpec.hpp"
  79 #include "gc/shared/isGCActiveMark.hpp"
  80 #include "gc/shared/locationPrinter.inline.hpp"
  81 #include "gc/shared/oopStorageParState.hpp"
  82 #include "gc/shared/preservedMarks.inline.hpp"
  83 #include "gc/shared/suspendibleThreadSet.hpp"
  84 #include "gc/shared/referenceProcessor.inline.hpp"
  85 #include "gc/shared/taskTerminator.hpp"
  86 #include "gc/shared/taskqueue.inline.hpp"
  87 #include "gc/shared/weakProcessor.inline.hpp"
  88 #include "gc/shared/workerPolicy.hpp"
  89 #include "logging/log.hpp"
  90 #include "memory/allocation.hpp"
  91 #include "memory/iterator.hpp"
  92 #include "memory/heapInspection.hpp"
  93 #include "memory/resourceArea.hpp"
  94 #include "memory/universe.hpp"
  95 #include "oops/access.inline.hpp"
  96 #include "oops/compressedOops.inline.hpp"
  97 #include "oops/oop.inline.hpp"
  98 #include "runtime/atomic.hpp"
  99 #include "runtime/handles.inline.hpp"
 100 #include "runtime/init.hpp"
 101 #include "runtime/java.hpp"
 102 #include "runtime/orderAccess.hpp"
 103 #include "runtime/threadSMR.hpp"
 104 #include "runtime/vmThread.hpp"
 105 #include "utilities/align.hpp"
 106 #include "utilities/autoRestore.hpp"
 107 #include "utilities/bitMap.inline.hpp"
 108 #include "utilities/globalDefinitions.hpp"
 109 #include "utilities/stack.inline.hpp"
 110 
 111 size_t G1CollectedHeap::_humongous_object_threshold_in_words = 0;
 112 
 113 // INVARIANTS/NOTES
 114 //
 115 // All allocation activity covered by the G1CollectedHeap interface is
 116 // serialized by acquiring the HeapLock.  This happens in mem_allocate
 117 // and allocate_new_tlab, which are the "entry" points to the
 118 // allocation code from the rest of the JVM.  (Note that this does not
 119 // apply to TLAB allocation, which is not part of this interface: it
 120 // is done by clients of this interface.)
 121 
 122 class RedirtyLoggedCardTableEntryClosure : public G1CardTableEntryClosure {
 123  private:
 124   size_t _num_dirtied;
 125   G1CollectedHeap* _g1h;
 126   G1CardTable* _g1_ct;
 127 
 128   HeapRegion* region_for_card(CardValue* card_ptr) const {
 129     return _g1h-&gt;heap_region_containing(_g1_ct-&gt;addr_for(card_ptr));
 130   }
 131 
 132   bool will_become_free(HeapRegion* hr) const {
 133     // A region will be freed by free_collection_set if the region is in the
 134     // collection set and has not had an evacuation failure.
 135     return _g1h-&gt;is_in_cset(hr) &amp;&amp; !hr-&gt;evacuation_failed();
 136   }
 137 
 138  public:
 139   RedirtyLoggedCardTableEntryClosure(G1CollectedHeap* g1h) : G1CardTableEntryClosure(),
 140     _num_dirtied(0), _g1h(g1h), _g1_ct(g1h-&gt;card_table()) { }
 141 
 142   void do_card_ptr(CardValue* card_ptr, uint worker_id) {
 143     HeapRegion* hr = region_for_card(card_ptr);
 144 
 145     // Should only dirty cards in regions that won't be freed.
 146     if (!will_become_free(hr)) {
 147       *card_ptr = G1CardTable::dirty_card_val();
 148       _num_dirtied++;
 149     }
 150   }
 151 
 152   size_t num_dirtied()   const { return _num_dirtied; }
 153 };
 154 
 155 
 156 void G1RegionMappingChangedListener::reset_from_card_cache(uint start_idx, size_t num_regions) {
 157   HeapRegionRemSet::invalidate_from_card_cache(start_idx, num_regions);
 158 }
 159 
 160 void G1RegionMappingChangedListener::on_commit(uint start_idx, size_t num_regions, bool zero_filled) {
 161   // The from card cache is not the memory that is actually committed. So we cannot
 162   // take advantage of the zero_filled parameter.
 163   reset_from_card_cache(start_idx, num_regions);
 164 }
 165 
 166 Tickspan G1CollectedHeap::run_task_timed(AbstractGangTask* task) {
 167   Ticks start = Ticks::now();
 168   workers()-&gt;run_task(task);
 169   return Ticks::now() - start;
 170 }
 171 
 172 HeapRegion* G1CollectedHeap::new_heap_region(uint hrs_index,
 173                                              MemRegion mr) {
 174   return new HeapRegion(hrs_index, bot(), mr);
 175 }
 176 
 177 // Private methods.
 178 
 179 HeapRegion* G1CollectedHeap::new_region(size_t word_size,
 180                                         HeapRegionType type,
 181                                         bool do_expand,
 182                                         uint node_index) {
 183   assert(!is_humongous(word_size) || word_size &lt;= HeapRegion::GrainWords,
 184          "the only time we use this to allocate a humongous region is "
 185          "when we are allocating a single humongous region");
 186 
 187   HeapRegion* res = _hrm-&gt;allocate_free_region(type, node_index);
 188 
 189   if (res == NULL &amp;&amp; do_expand &amp;&amp; _expand_heap_after_alloc_failure) {
 190     // Currently, only attempts to allocate GC alloc regions set
 191     // do_expand to true. So, we should only reach here during a
 192     // safepoint. If this assumption changes we might have to
 193     // reconsider the use of _expand_heap_after_alloc_failure.
 194     assert(SafepointSynchronize::is_at_safepoint(), "invariant");
 195 
 196     log_debug(gc, ergo, heap)("Attempt heap expansion (region allocation request failed). Allocation request: " SIZE_FORMAT "B",
 197                               word_size * HeapWordSize);
 198 
 199     assert(word_size * HeapWordSize &lt; HeapRegion::GrainBytes,
 200            "This kind of expansion should never be more than one region. Size: " SIZE_FORMAT,
 201            word_size * HeapWordSize);
 202     if (expand_single_region(node_index)) {
 203       // Given that expand_single_region() succeeded in expanding the heap, and we
 204       // always expand the heap by an amount aligned to the heap
 205       // region size, the free list should in theory not be empty.
 206       // In either case allocate_free_region() will check for NULL.
 207       res = _hrm-&gt;allocate_free_region(type, node_index);
 208     } else {
 209       _expand_heap_after_alloc_failure = false;
 210     }
 211   }
 212   return res;
 213 }
 214 
 215 HeapWord*
 216 G1CollectedHeap::humongous_obj_allocate_initialize_regions(HeapRegion* first_hr,
 217                                                            uint num_regions,
 218                                                            size_t word_size) {
 219   assert(first_hr != NULL, "pre-condition");
 220   assert(is_humongous(word_size), "word_size should be humongous");
 221   assert(num_regions * HeapRegion::GrainWords &gt;= word_size, "pre-condition");
 222 
 223   // Index of last region in the series.
 224   uint first = first_hr-&gt;hrm_index();
 225   uint last = first + num_regions - 1;
 226 
 227   // We need to initialize the region(s) we just discovered. This is
 228   // a bit tricky given that it can happen concurrently with
 229   // refinement threads refining cards on these regions and
 230   // potentially wanting to refine the BOT as they are scanning
 231   // those cards (this can happen shortly after a cleanup; see CR
 232   // 6991377). So we have to set up the region(s) carefully and in
 233   // a specific order.
 234 
 235   // The word size sum of all the regions we will allocate.
 236   size_t word_size_sum = (size_t) num_regions * HeapRegion::GrainWords;
 237   assert(word_size &lt;= word_size_sum, "sanity");
 238 
 239   // The passed in hr will be the "starts humongous" region. The header
 240   // of the new object will be placed at the bottom of this region.
 241   HeapWord* new_obj = first_hr-&gt;bottom();
 242   // This will be the new top of the new object.
 243   HeapWord* obj_top = new_obj + word_size;
 244 
 245   // First, we need to zero the header of the space that we will be
 246   // allocating. When we update top further down, some refinement
 247   // threads might try to scan the region. By zeroing the header we
 248   // ensure that any thread that will try to scan the region will
 249   // come across the zero klass word and bail out.
 250   //
 251   // NOTE: It would not have been correct to have used
 252   // CollectedHeap::fill_with_object() and make the space look like
 253   // an int array. The thread that is doing the allocation will
 254   // later update the object header to a potentially different array
 255   // type and, for a very short period of time, the klass and length
 256   // fields will be inconsistent. This could cause a refinement
 257   // thread to calculate the object size incorrectly.
 258   Copy::fill_to_words(new_obj, oopDesc::header_size(), 0);
 259 
 260   // Next, pad out the unused tail of the last region with filler
 261   // objects, for improved usage accounting.
 262   // How many words we use for filler objects.
 263   size_t word_fill_size = word_size_sum - word_size;
 264 
 265   // How many words memory we "waste" which cannot hold a filler object.
 266   size_t words_not_fillable = 0;
 267 
 268   if (word_fill_size &gt;= min_fill_size()) {
 269     fill_with_objects(obj_top, word_fill_size);
 270   } else if (word_fill_size &gt; 0) {
 271     // We have space to fill, but we cannot fit an object there.
 272     words_not_fillable = word_fill_size;
 273     word_fill_size = 0;
 274   }
 275 
 276   // We will set up the first region as "starts humongous". This
 277   // will also update the BOT covering all the regions to reflect
 278   // that there is a single object that starts at the bottom of the
 279   // first region.
 280   first_hr-&gt;set_starts_humongous(obj_top, word_fill_size);
 281   _policy-&gt;remset_tracker()-&gt;update_at_allocate(first_hr);
 282   // Then, if there are any, we will set up the "continues
 283   // humongous" regions.
 284   HeapRegion* hr = NULL;
 285   for (uint i = first + 1; i &lt;= last; ++i) {
 286     hr = region_at(i);
 287     hr-&gt;set_continues_humongous(first_hr);
 288     _policy-&gt;remset_tracker()-&gt;update_at_allocate(hr);
 289   }
 290 
 291   // Up to this point no concurrent thread would have been able to
 292   // do any scanning on any region in this series. All the top
 293   // fields still point to bottom, so the intersection between
 294   // [bottom,top] and [card_start,card_end] will be empty. Before we
 295   // update the top fields, we'll do a storestore to make sure that
 296   // no thread sees the update to top before the zeroing of the
 297   // object header and the BOT initialization.
 298   OrderAccess::storestore();
 299 
 300   // Now, we will update the top fields of the "continues humongous"
 301   // regions except the last one.
 302   for (uint i = first; i &lt; last; ++i) {
 303     hr = region_at(i);
 304     hr-&gt;set_top(hr-&gt;end());
 305   }
 306 
 307   hr = region_at(last);
 308   // If we cannot fit a filler object, we must set top to the end
 309   // of the humongous object, otherwise we cannot iterate the heap
 310   // and the BOT will not be complete.
 311   hr-&gt;set_top(hr-&gt;end() - words_not_fillable);
 312 
 313   assert(hr-&gt;bottom() &lt; obj_top &amp;&amp; obj_top &lt;= hr-&gt;end(),
 314          "obj_top should be in last region");
 315 
 316   _verifier-&gt;check_bitmaps("Humongous Region Allocation", first_hr);
 317 
 318   assert(words_not_fillable == 0 ||
 319          first_hr-&gt;bottom() + word_size_sum - words_not_fillable == hr-&gt;top(),
 320          "Miscalculation in humongous allocation");
 321 
 322   increase_used((word_size_sum - words_not_fillable) * HeapWordSize);
 323 
 324   for (uint i = first; i &lt;= last; ++i) {
 325     hr = region_at(i);
 326     _humongous_set.add(hr);
 327     _hr_printer.alloc(hr);
 328   }
 329 
 330   return new_obj;
 331 }
 332 
 333 size_t G1CollectedHeap::humongous_obj_size_in_regions(size_t word_size) {
 334   assert(is_humongous(word_size), "Object of size " SIZE_FORMAT " must be humongous here", word_size);
 335   return align_up(word_size, HeapRegion::GrainWords) / HeapRegion::GrainWords;
 336 }
 337 
 338 // If could fit into free regions w/o expansion, try.
 339 // Otherwise, if can expand, do so.
 340 // Otherwise, if using ex regions might help, try with ex given back.
 341 HeapWord* G1CollectedHeap::humongous_obj_allocate(size_t word_size) {
 342   assert_heap_locked_or_at_safepoint(true /* should_be_vm_thread */);
 343 
 344   _verifier-&gt;verify_region_sets_optional();
 345 
 346   uint obj_regions = (uint) humongous_obj_size_in_regions(word_size);
 347 
 348   // Policy: First try to allocate a humongous object in the free list.
 349   HeapRegion* humongous_start = _hrm-&gt;allocate_humongous(obj_regions);
 350   if (humongous_start == NULL) {
 351     // Policy: We could not find enough regions for the humongous object in the
 352     // free list. Look through the heap to find a mix of free and uncommitted regions.
 353     // If so, expand the heap and allocate the humongous object.
 354     humongous_start = _hrm-&gt;expand_and_allocate_humongous(obj_regions);
 355     if (humongous_start != NULL) {
 356       // We managed to find a region by expanding the heap.
 357       log_debug(gc, ergo, heap)("Heap expansion (humongous allocation request). Allocation request: " SIZE_FORMAT "B",
 358                                 word_size * HeapWordSize);
 359       policy()-&gt;record_new_heap_size(num_regions());
 360     } else {
 361       // Policy: Potentially trigger a defragmentation GC.
 362     }
 363   }
 364 
 365   HeapWord* result = NULL;
 366   if (humongous_start != NULL) {
 367     result = humongous_obj_allocate_initialize_regions(humongous_start, obj_regions, word_size);
 368     assert(result != NULL, "it should always return a valid result");
 369 
 370     // A successful humongous object allocation changes the used space
 371     // information of the old generation so we need to recalculate the
 372     // sizes and update the jstat counters here.
 373     g1mm()-&gt;update_sizes();
 374   }
 375 
 376   _verifier-&gt;verify_region_sets_optional();
 377 
 378   return result;
 379 }
 380 
 381 HeapWord* G1CollectedHeap::allocate_new_tlab(size_t min_size,
 382                                              size_t requested_size,
 383                                              size_t* actual_size) {
 384   assert_heap_not_locked_and_not_at_safepoint();
 385   assert(!is_humongous(requested_size), "we do not allow humongous TLABs");
 386 
 387   return attempt_allocation(min_size, requested_size, actual_size);
 388 }
 389 
 390 HeapWord*
 391 G1CollectedHeap::mem_allocate(size_t word_size,
 392                               bool*  gc_overhead_limit_was_exceeded) {
 393   assert_heap_not_locked_and_not_at_safepoint();
 394 
 395   if (is_humongous(word_size)) {
 396     return attempt_allocation_humongous(word_size);
 397   }
 398   size_t dummy = 0;
 399   return attempt_allocation(word_size, word_size, &amp;dummy);
 400 }
 401 
 402 HeapWord* G1CollectedHeap::attempt_allocation_slow(size_t word_size) {
 403   ResourceMark rm; // For retrieving the thread names in log messages.
 404 
 405   // Make sure you read the note in attempt_allocation_humongous().
 406 
 407   assert_heap_not_locked_and_not_at_safepoint();
 408   assert(!is_humongous(word_size), "attempt_allocation_slow() should not "
 409          "be called for humongous allocation requests");
 410 
 411   // We should only get here after the first-level allocation attempt
 412   // (attempt_allocation()) failed to allocate.
 413 
 414   // We will loop until a) we manage to successfully perform the
 415   // allocation or b) we successfully schedule a collection which
 416   // fails to perform the allocation. b) is the only case when we'll
 417   // return NULL.
 418   HeapWord* result = NULL;
 419   for (uint try_count = 1, gclocker_retry_count = 0; /* we'll return */; try_count += 1) {
 420     bool should_try_gc;
 421     uint gc_count_before;
 422 
 423     {
 424       MutexLocker x(Heap_lock);
 425       result = _allocator-&gt;attempt_allocation_locked(word_size);
 426       if (result != NULL) {
 427         return result;
 428       }
 429 
 430       // If the GCLocker is active and we are bound for a GC, try expanding young gen.
 431       // This is different to when only GCLocker::needs_gc() is set: try to avoid
 432       // waiting because the GCLocker is active to not wait too long.
 433       if (GCLocker::is_active_and_needs_gc() &amp;&amp; policy()-&gt;can_expand_young_list()) {
 434         // No need for an ergo message here, can_expand_young_list() does this when
 435         // it returns true.
 436         result = _allocator-&gt;attempt_allocation_force(word_size);
 437         if (result != NULL) {
 438           return result;
 439         }
 440       }
 441       // Only try a GC if the GCLocker does not signal the need for a GC. Wait until
 442       // the GCLocker initiated GC has been performed and then retry. This includes
 443       // the case when the GC Locker is not active but has not been performed.
 444       should_try_gc = !GCLocker::needs_gc();
 445       // Read the GC count while still holding the Heap_lock.
 446       gc_count_before = total_collections();
 447     }
 448 
 449     if (should_try_gc) {
 450       bool succeeded;
 451       result = do_collection_pause(word_size, gc_count_before, &amp;succeeded,
 452                                    GCCause::_g1_inc_collection_pause);
 453       if (result != NULL) {
 454         assert(succeeded, "only way to get back a non-NULL result");
 455         log_trace(gc, alloc)("%s: Successfully scheduled collection returning " PTR_FORMAT,
 456                              Thread::current()-&gt;name(), p2i(result));
 457         return result;
 458       }
 459 
 460       if (succeeded) {
 461         // We successfully scheduled a collection which failed to allocate. No
 462         // point in trying to allocate further. We'll just return NULL.
 463         log_trace(gc, alloc)("%s: Successfully scheduled collection failing to allocate "
 464                              SIZE_FORMAT " words", Thread::current()-&gt;name(), word_size);
 465         return NULL;
 466       }
 467       log_trace(gc, alloc)("%s: Unsuccessfully scheduled collection allocating " SIZE_FORMAT " words",
 468                            Thread::current()-&gt;name(), word_size);
 469     } else {
 470       // Failed to schedule a collection.
 471       if (gclocker_retry_count &gt; GCLockerRetryAllocationCount) {
 472         log_warning(gc, alloc)("%s: Retried waiting for GCLocker too often allocating "
 473                                SIZE_FORMAT " words", Thread::current()-&gt;name(), word_size);
 474         return NULL;
 475       }
 476       log_trace(gc, alloc)("%s: Stall until clear", Thread::current()-&gt;name());
 477       // The GCLocker is either active or the GCLocker initiated
 478       // GC has not yet been performed. Stall until it is and
 479       // then retry the allocation.
 480       GCLocker::stall_until_clear();
 481       gclocker_retry_count += 1;
 482     }
 483 
 484     // We can reach here if we were unsuccessful in scheduling a
 485     // collection (because another thread beat us to it) or if we were
 486     // stalled due to the GC locker. In either can we should retry the
 487     // allocation attempt in case another thread successfully
 488     // performed a collection and reclaimed enough space. We do the
 489     // first attempt (without holding the Heap_lock) here and the
 490     // follow-on attempt will be at the start of the next loop
 491     // iteration (after taking the Heap_lock).
 492     size_t dummy = 0;
 493     result = _allocator-&gt;attempt_allocation(word_size, word_size, &amp;dummy);
 494     if (result != NULL) {
 495       return result;
 496     }
 497 
 498     // Give a warning if we seem to be looping forever.
 499     if ((QueuedAllocationWarningCount &gt; 0) &amp;&amp;
 500         (try_count % QueuedAllocationWarningCount == 0)) {
 501       log_warning(gc, alloc)("%s:  Retried allocation %u times for " SIZE_FORMAT " words",
 502                              Thread::current()-&gt;name(), try_count, word_size);
 503     }
 504   }
 505 
 506   ShouldNotReachHere();
 507   return NULL;
 508 }
 509 
 510 void G1CollectedHeap::begin_archive_alloc_range(bool open) {
 511   assert_at_safepoint_on_vm_thread();
 512   if (_archive_allocator == NULL) {
 513     _archive_allocator = G1ArchiveAllocator::create_allocator(this, open);
 514   }
 515 }
 516 
 517 bool G1CollectedHeap::is_archive_alloc_too_large(size_t word_size) {
 518   // Allocations in archive regions cannot be of a size that would be considered
 519   // humongous even for a minimum-sized region, because G1 region sizes/boundaries
 520   // may be different at archive-restore time.
 521   return word_size &gt;= humongous_threshold_for(HeapRegion::min_region_size_in_words());
 522 }
 523 
 524 HeapWord* G1CollectedHeap::archive_mem_allocate(size_t word_size) {
 525   assert_at_safepoint_on_vm_thread();
 526   assert(_archive_allocator != NULL, "_archive_allocator not initialized");
 527   if (is_archive_alloc_too_large(word_size)) {
 528     return NULL;
 529   }
 530   return _archive_allocator-&gt;archive_mem_allocate(word_size);
 531 }
 532 
 533 void G1CollectedHeap::end_archive_alloc_range(GrowableArray&lt;MemRegion&gt;* ranges,
 534                                               size_t end_alignment_in_bytes) {
 535   assert_at_safepoint_on_vm_thread();
 536   assert(_archive_allocator != NULL, "_archive_allocator not initialized");
 537 
 538   // Call complete_archive to do the real work, filling in the MemRegion
 539   // array with the archive regions.
 540   _archive_allocator-&gt;complete_archive(ranges, end_alignment_in_bytes);
 541   delete _archive_allocator;
 542   _archive_allocator = NULL;
 543 }
 544 
 545 bool G1CollectedHeap::check_archive_addresses(MemRegion* ranges, size_t count) {
 546   assert(ranges != NULL, "MemRegion array NULL");
 547   assert(count != 0, "No MemRegions provided");
 548   MemRegion reserved = _hrm-&gt;reserved();
 549   for (size_t i = 0; i &lt; count; i++) {
 550     if (!reserved.contains(ranges[i].start()) || !reserved.contains(ranges[i].last())) {
 551       return false;
 552     }
 553   }
 554   return true;
 555 }
 556 
 557 bool G1CollectedHeap::alloc_archive_regions(MemRegion* ranges,
 558                                             size_t count,
 559                                             bool open) {
 560   assert(!is_init_completed(), "Expect to be called at JVM init time");
 561   assert(ranges != NULL, "MemRegion array NULL");
 562   assert(count != 0, "No MemRegions provided");
 563   MutexLocker x(Heap_lock);
 564 
 565   MemRegion reserved = _hrm-&gt;reserved();
 566   HeapWord* prev_last_addr = NULL;
 567   HeapRegion* prev_last_region = NULL;
 568 
 569   // Temporarily disable pretouching of heap pages. This interface is used
 570   // when mmap'ing archived heap data in, so pre-touching is wasted.
 571   FlagSetting fs(AlwaysPreTouch, false);
 572 
 573   // Enable archive object checking used by G1MarkSweep. We have to let it know
 574   // about each archive range, so that objects in those ranges aren't marked.
 575   G1ArchiveAllocator::enable_archive_object_check();
 576 
 577   // For each specified MemRegion range, allocate the corresponding G1
 578   // regions and mark them as archive regions. We expect the ranges
 579   // in ascending starting address order, without overlap.
 580   for (size_t i = 0; i &lt; count; i++) {
 581     MemRegion curr_range = ranges[i];
 582     HeapWord* start_address = curr_range.start();
 583     size_t word_size = curr_range.word_size();
 584     HeapWord* last_address = curr_range.last();
 585     size_t commits = 0;
 586 
 587     guarantee(reserved.contains(start_address) &amp;&amp; reserved.contains(last_address),
 588               "MemRegion outside of heap [" PTR_FORMAT ", " PTR_FORMAT "]",
 589               p2i(start_address), p2i(last_address));
 590     guarantee(start_address &gt; prev_last_addr,
 591               "Ranges not in ascending order: " PTR_FORMAT " &lt;= " PTR_FORMAT ,
 592               p2i(start_address), p2i(prev_last_addr));
 593     prev_last_addr = last_address;
 594 
 595     // Check for ranges that start in the same G1 region in which the previous
 596     // range ended, and adjust the start address so we don't try to allocate
 597     // the same region again. If the current range is entirely within that
 598     // region, skip it, just adjusting the recorded top.
 599     HeapRegion* start_region = _hrm-&gt;addr_to_region(start_address);
 600     if ((prev_last_region != NULL) &amp;&amp; (start_region == prev_last_region)) {
 601       start_address = start_region-&gt;end();
 602       if (start_address &gt; last_address) {
 603         increase_used(word_size * HeapWordSize);
 604         start_region-&gt;set_top(last_address + 1);
 605         continue;
 606       }
 607       start_region-&gt;set_top(start_address);
 608       curr_range = MemRegion(start_address, last_address + 1);
 609       start_region = _hrm-&gt;addr_to_region(start_address);
 610     }
 611 
 612     // Perform the actual region allocation, exiting if it fails.
 613     // Then note how much new space we have allocated.
 614     if (!_hrm-&gt;allocate_containing_regions(curr_range, &amp;commits, workers())) {
 615       return false;
 616     }
 617     increase_used(word_size * HeapWordSize);
 618     if (commits != 0) {
 619       log_debug(gc, ergo, heap)("Attempt heap expansion (allocate archive regions). Total size: " SIZE_FORMAT "B",
 620                                 HeapRegion::GrainWords * HeapWordSize * commits);
 621 
 622     }
 623 
 624     // Mark each G1 region touched by the range as archive, add it to
 625     // the old set, and set top.
 626     HeapRegion* curr_region = _hrm-&gt;addr_to_region(start_address);
 627     HeapRegion* last_region = _hrm-&gt;addr_to_region(last_address);
 628     prev_last_region = last_region;
 629 
 630     while (curr_region != NULL) {
 631       assert(curr_region-&gt;is_empty() &amp;&amp; !curr_region-&gt;is_pinned(),
 632              "Region already in use (index %u)", curr_region-&gt;hrm_index());
 633       if (open) {
 634         curr_region-&gt;set_open_archive();
 635       } else {
 636         curr_region-&gt;set_closed_archive();
 637       }
 638       _hr_printer.alloc(curr_region);
 639       _archive_set.add(curr_region);
 640       HeapWord* top;
 641       HeapRegion* next_region;
 642       if (curr_region != last_region) {
 643         top = curr_region-&gt;end();
 644         next_region = _hrm-&gt;next_region_in_heap(curr_region);
 645       } else {
 646         top = last_address + 1;
 647         next_region = NULL;
 648       }
 649       curr_region-&gt;set_top(top);
 650       curr_region = next_region;
 651     }
 652 
 653     // Notify mark-sweep of the archive
 654     G1ArchiveAllocator::set_range_archive(curr_range, open);
 655   }
 656   return true;
 657 }
 658 
 659 void G1CollectedHeap::fill_archive_regions(MemRegion* ranges, size_t count) {
 660   assert(!is_init_completed(), "Expect to be called at JVM init time");
 661   assert(ranges != NULL, "MemRegion array NULL");
 662   assert(count != 0, "No MemRegions provided");
 663   MemRegion reserved = _hrm-&gt;reserved();
 664   HeapWord *prev_last_addr = NULL;
 665   HeapRegion* prev_last_region = NULL;
 666 
 667   // For each MemRegion, create filler objects, if needed, in the G1 regions
 668   // that contain the address range. The address range actually within the
 669   // MemRegion will not be modified. That is assumed to have been initialized
 670   // elsewhere, probably via an mmap of archived heap data.
 671   MutexLocker x(Heap_lock);
 672   for (size_t i = 0; i &lt; count; i++) {
 673     HeapWord* start_address = ranges[i].start();
 674     HeapWord* last_address = ranges[i].last();
 675 
 676     assert(reserved.contains(start_address) &amp;&amp; reserved.contains(last_address),
 677            "MemRegion outside of heap [" PTR_FORMAT ", " PTR_FORMAT "]",
 678            p2i(start_address), p2i(last_address));
 679     assert(start_address &gt; prev_last_addr,
 680            "Ranges not in ascending order: " PTR_FORMAT " &lt;= " PTR_FORMAT ,
 681            p2i(start_address), p2i(prev_last_addr));
 682 
 683     HeapRegion* start_region = _hrm-&gt;addr_to_region(start_address);
 684     HeapRegion* last_region = _hrm-&gt;addr_to_region(last_address);
 685     HeapWord* bottom_address = start_region-&gt;bottom();
 686 
 687     // Check for a range beginning in the same region in which the
 688     // previous one ended.
 689     if (start_region == prev_last_region) {
 690       bottom_address = prev_last_addr + 1;
 691     }
 692 
 693     // Verify that the regions were all marked as archive regions by
 694     // alloc_archive_regions.
 695     HeapRegion* curr_region = start_region;
 696     while (curr_region != NULL) {
 697       guarantee(curr_region-&gt;is_archive(),
 698                 "Expected archive region at index %u", curr_region-&gt;hrm_index());
 699       if (curr_region != last_region) {
 700         curr_region = _hrm-&gt;next_region_in_heap(curr_region);
 701       } else {
 702         curr_region = NULL;
 703       }
 704     }
 705 
 706     prev_last_addr = last_address;
 707     prev_last_region = last_region;
 708 
 709     // Fill the memory below the allocated range with dummy object(s),
 710     // if the region bottom does not match the range start, or if the previous
 711     // range ended within the same G1 region, and there is a gap.
 712     if (start_address != bottom_address) {
 713       size_t fill_size = pointer_delta(start_address, bottom_address);
 714       G1CollectedHeap::fill_with_objects(bottom_address, fill_size);
 715       increase_used(fill_size * HeapWordSize);
 716     }
 717   }
 718 }
 719 
 720 inline HeapWord* G1CollectedHeap::attempt_allocation(size_t min_word_size,
 721                                                      size_t desired_word_size,
 722                                                      size_t* actual_word_size) {
 723   assert_heap_not_locked_and_not_at_safepoint();
 724   assert(!is_humongous(desired_word_size), "attempt_allocation() should not "
 725          "be called for humongous allocation requests");
 726 
 727   HeapWord* result = _allocator-&gt;attempt_allocation(min_word_size, desired_word_size, actual_word_size);
 728 
 729   if (result == NULL) {
 730     *actual_word_size = desired_word_size;
 731     result = attempt_allocation_slow(desired_word_size);
 732   }
 733 
 734   assert_heap_not_locked();
 735   if (result != NULL) {
 736     assert(*actual_word_size != 0, "Actual size must have been set here");
 737     dirty_young_block(result, *actual_word_size);
 738   } else {
 739     *actual_word_size = 0;
 740   }
 741 
 742   return result;
 743 }
 744 
 745 void G1CollectedHeap::dealloc_archive_regions(MemRegion* ranges, size_t count) {
 746   assert(!is_init_completed(), "Expect to be called at JVM init time");
 747   assert(ranges != NULL, "MemRegion array NULL");
 748   assert(count != 0, "No MemRegions provided");
 749   MemRegion reserved = _hrm-&gt;reserved();
 750   HeapWord* prev_last_addr = NULL;
 751   HeapRegion* prev_last_region = NULL;
 752   size_t size_used = 0;
 753   size_t uncommitted_regions = 0;
 754 
 755   // For each Memregion, free the G1 regions that constitute it, and
 756   // notify mark-sweep that the range is no longer to be considered 'archive.'
 757   MutexLocker x(Heap_lock);
 758   for (size_t i = 0; i &lt; count; i++) {
 759     HeapWord* start_address = ranges[i].start();
 760     HeapWord* last_address = ranges[i].last();
 761 
 762     assert(reserved.contains(start_address) &amp;&amp; reserved.contains(last_address),
 763            "MemRegion outside of heap [" PTR_FORMAT ", " PTR_FORMAT "]",
 764            p2i(start_address), p2i(last_address));
 765     assert(start_address &gt; prev_last_addr,
 766            "Ranges not in ascending order: " PTR_FORMAT " &lt;= " PTR_FORMAT ,
 767            p2i(start_address), p2i(prev_last_addr));
 768     size_used += ranges[i].byte_size();
 769     prev_last_addr = last_address;
 770 
 771     HeapRegion* start_region = _hrm-&gt;addr_to_region(start_address);
 772     HeapRegion* last_region = _hrm-&gt;addr_to_region(last_address);
 773 
 774     // Check for ranges that start in the same G1 region in which the previous
 775     // range ended, and adjust the start address so we don't try to free
 776     // the same region again. If the current range is entirely within that
 777     // region, skip it.
 778     if (start_region == prev_last_region) {
 779       start_address = start_region-&gt;end();
 780       if (start_address &gt; last_address) {
 781         continue;
 782       }
 783       start_region = _hrm-&gt;addr_to_region(start_address);
 784     }
 785     prev_last_region = last_region;
 786 
 787     // After verifying that each region was marked as an archive region by
 788     // alloc_archive_regions, set it free and empty and uncommit it.
 789     HeapRegion* curr_region = start_region;
 790     while (curr_region != NULL) {
 791       guarantee(curr_region-&gt;is_archive(),
 792                 "Expected archive region at index %u", curr_region-&gt;hrm_index());
 793       uint curr_index = curr_region-&gt;hrm_index();
 794       _archive_set.remove(curr_region);
 795       curr_region-&gt;set_free();
 796       curr_region-&gt;set_top(curr_region-&gt;bottom());
 797       if (curr_region != last_region) {
 798         curr_region = _hrm-&gt;next_region_in_heap(curr_region);
 799       } else {
 800         curr_region = NULL;
 801       }
 802       _hrm-&gt;shrink_at(curr_index, 1);
 803       uncommitted_regions++;
 804     }
 805 
 806     // Notify mark-sweep that this is no longer an archive range.
 807     G1ArchiveAllocator::clear_range_archive(ranges[i]);
 808   }
 809 
 810   if (uncommitted_regions != 0) {
 811     log_debug(gc, ergo, heap)("Attempt heap shrinking (uncommitted archive regions). Total size: " SIZE_FORMAT "B",
 812                               HeapRegion::GrainWords * HeapWordSize * uncommitted_regions);
 813   }
 814   decrease_used(size_used);
 815 }
 816 
 817 oop G1CollectedHeap::materialize_archived_object(oop obj) {
 818   assert(obj != NULL, "archived obj is NULL");
 819   assert(G1ArchiveAllocator::is_archived_object(obj), "must be archived object");
 820 
 821   // Loading an archived object makes it strongly reachable. If it is
 822   // loaded during concurrent marking, it must be enqueued to the SATB
 823   // queue, shading the previously white object gray.
 824   G1BarrierSet::enqueue(obj);
 825 
 826   return obj;
 827 }
 828 
 829 HeapWord* G1CollectedHeap::attempt_allocation_humongous(size_t word_size) {
 830   ResourceMark rm; // For retrieving the thread names in log messages.
 831 
 832   // The structure of this method has a lot of similarities to
 833   // attempt_allocation_slow(). The reason these two were not merged
 834   // into a single one is that such a method would require several "if
 835   // allocation is not humongous do this, otherwise do that"
 836   // conditional paths which would obscure its flow. In fact, an early
 837   // version of this code did use a unified method which was harder to
 838   // follow and, as a result, it had subtle bugs that were hard to
 839   // track down. So keeping these two methods separate allows each to
 840   // be more readable. It will be good to keep these two in sync as
 841   // much as possible.
 842 
 843   assert_heap_not_locked_and_not_at_safepoint();
 844   assert(is_humongous(word_size), "attempt_allocation_humongous() "
 845          "should only be called for humongous allocations");
 846 
 847   // Humongous objects can exhaust the heap quickly, so we should check if we
 848   // need to start a marking cycle at each humongous object allocation. We do
 849   // the check before we do the actual allocation. The reason for doing it
 850   // before the allocation is that we avoid having to keep track of the newly
 851   // allocated memory while we do a GC.
 852   if (policy()-&gt;need_to_start_conc_mark("concurrent humongous allocation",
 853                                            word_size)) {
 854     collect(GCCause::_g1_humongous_allocation);
 855   }
 856 
 857   // We will loop until a) we manage to successfully perform the
 858   // allocation or b) we successfully schedule a collection which
 859   // fails to perform the allocation. b) is the only case when we'll
 860   // return NULL.
 861   HeapWord* result = NULL;
 862   for (uint try_count = 1, gclocker_retry_count = 0; /* we'll return */; try_count += 1) {
 863     bool should_try_gc;
 864     uint gc_count_before;
 865 
 866 
 867     {
 868       MutexLocker x(Heap_lock);
 869 
 870       // Given that humongous objects are not allocated in young
 871       // regions, we'll first try to do the allocation without doing a
 872       // collection hoping that there's enough space in the heap.
 873       result = humongous_obj_allocate(word_size);
 874       if (result != NULL) {
 875         size_t size_in_regions = humongous_obj_size_in_regions(word_size);
 876         policy()-&gt;old_gen_alloc_tracker()-&gt;
 877           add_allocated_humongous_bytes_since_last_gc(size_in_regions * HeapRegion::GrainBytes);
 878         return result;
 879       }
 880 
 881       // Only try a GC if the GCLocker does not signal the need for a GC. Wait until
 882       // the GCLocker initiated GC has been performed and then retry. This includes
 883       // the case when the GC Locker is not active but has not been performed.
 884       should_try_gc = !GCLocker::needs_gc();
 885       // Read the GC count while still holding the Heap_lock.
 886       gc_count_before = total_collections();
 887     }
 888 
 889     if (should_try_gc) {
 890       bool succeeded;
 891       result = do_collection_pause(word_size, gc_count_before, &amp;succeeded,
 892                                    GCCause::_g1_humongous_allocation);
 893       if (result != NULL) {
 894         assert(succeeded, "only way to get back a non-NULL result");
 895         log_trace(gc, alloc)("%s: Successfully scheduled collection returning " PTR_FORMAT,
 896                              Thread::current()-&gt;name(), p2i(result));
 897         size_t size_in_regions = humongous_obj_size_in_regions(word_size);
 898         policy()-&gt;old_gen_alloc_tracker()-&gt;
 899           record_collection_pause_humongous_allocation(size_in_regions * HeapRegion::GrainBytes);
 900         return result;
 901       }
 902 
 903       if (succeeded) {
 904         // We successfully scheduled a collection which failed to allocate. No
 905         // point in trying to allocate further. We'll just return NULL.
 906         log_trace(gc, alloc)("%s: Successfully scheduled collection failing to allocate "
 907                              SIZE_FORMAT " words", Thread::current()-&gt;name(), word_size);
 908         return NULL;
 909       }
 910       log_trace(gc, alloc)("%s: Unsuccessfully scheduled collection allocating " SIZE_FORMAT "",
 911                            Thread::current()-&gt;name(), word_size);
 912     } else {
 913       // Failed to schedule a collection.
 914       if (gclocker_retry_count &gt; GCLockerRetryAllocationCount) {
 915         log_warning(gc, alloc)("%s: Retried waiting for GCLocker too often allocating "
 916                                SIZE_FORMAT " words", Thread::current()-&gt;name(), word_size);
 917         return NULL;
 918       }
 919       log_trace(gc, alloc)("%s: Stall until clear", Thread::current()-&gt;name());
 920       // The GCLocker is either active or the GCLocker initiated
 921       // GC has not yet been performed. Stall until it is and
 922       // then retry the allocation.
 923       GCLocker::stall_until_clear();
 924       gclocker_retry_count += 1;
 925     }
 926 
 927 
 928     // We can reach here if we were unsuccessful in scheduling a
 929     // collection (because another thread beat us to it) or if we were
 930     // stalled due to the GC locker. In either can we should retry the
 931     // allocation attempt in case another thread successfully
 932     // performed a collection and reclaimed enough space.
 933     // Humongous object allocation always needs a lock, so we wait for the retry
 934     // in the next iteration of the loop, unlike for the regular iteration case.
 935     // Give a warning if we seem to be looping forever.
 936 
 937     if ((QueuedAllocationWarningCount &gt; 0) &amp;&amp;
 938         (try_count % QueuedAllocationWarningCount == 0)) {
 939       log_warning(gc, alloc)("%s: Retried allocation %u times for " SIZE_FORMAT " words",
 940                              Thread::current()-&gt;name(), try_count, word_size);
 941     }
 942   }
 943 
 944   ShouldNotReachHere();
 945   return NULL;
 946 }
 947 
 948 HeapWord* G1CollectedHeap::attempt_allocation_at_safepoint(size_t word_size,
 949                                                            bool expect_null_mutator_alloc_region) {
 950   assert_at_safepoint_on_vm_thread();
 951   assert(!_allocator-&gt;has_mutator_alloc_region() || !expect_null_mutator_alloc_region,
 952          "the current alloc region was unexpectedly found to be non-NULL");
 953 
 954   if (!is_humongous(word_size)) {
 955     return _allocator-&gt;attempt_allocation_locked(word_size);
 956   } else {
 957     HeapWord* result = humongous_obj_allocate(word_size);
 958     if (result != NULL &amp;&amp; policy()-&gt;need_to_start_conc_mark("STW humongous allocation")) {
 959       collector_state()-&gt;set_initiate_conc_mark_if_possible(true);
 960     }
 961     return result;
 962   }
 963 
 964   ShouldNotReachHere();
 965 }
 966 
 967 class PostCompactionPrinterClosure: public HeapRegionClosure {
 968 private:
 969   G1HRPrinter* _hr_printer;
 970 public:
 971   bool do_heap_region(HeapRegion* hr) {
 972     assert(!hr-&gt;is_young(), "not expecting to find young regions");
 973     _hr_printer-&gt;post_compaction(hr);
 974     return false;
 975   }
 976 
 977   PostCompactionPrinterClosure(G1HRPrinter* hr_printer)
 978     : _hr_printer(hr_printer) { }
 979 };
 980 
 981 void G1CollectedHeap::print_hrm_post_compaction() {
 982   if (_hr_printer.is_active()) {
 983     PostCompactionPrinterClosure cl(hr_printer());
 984     heap_region_iterate(&amp;cl);
 985   }
 986 }
 987 
 988 void G1CollectedHeap::abort_concurrent_cycle() {
 989   // If we start the compaction before the CM threads finish
 990   // scanning the root regions we might trip them over as we'll
 991   // be moving objects / updating references. So let's wait until
 992   // they are done. By telling them to abort, they should complete
 993   // early.
 994   _cm-&gt;root_regions()-&gt;abort();
 995   _cm-&gt;root_regions()-&gt;wait_until_scan_finished();
 996 
 997   // Disable discovery and empty the discovered lists
 998   // for the CM ref processor.
 999   _ref_processor_cm-&gt;disable_discovery();
1000   _ref_processor_cm-&gt;abandon_partial_discovery();
1001   _ref_processor_cm-&gt;verify_no_references_recorded();
1002 
1003   // Abandon current iterations of concurrent marking and concurrent
1004   // refinement, if any are in progress.
1005   concurrent_mark()-&gt;concurrent_cycle_abort();
1006 }
1007 
1008 void G1CollectedHeap::prepare_heap_for_full_collection() {
1009   // Make sure we'll choose a new allocation region afterwards.
1010   _allocator-&gt;release_mutator_alloc_regions();
1011   _allocator-&gt;abandon_gc_alloc_regions();
1012 
1013   // We may have added regions to the current incremental collection
1014   // set between the last GC or pause and now. We need to clear the
1015   // incremental collection set and then start rebuilding it afresh
1016   // after this full GC.
1017   abandon_collection_set(collection_set());
1018 
1019   tear_down_region_sets(false /* free_list_only */);
1020 
1021   hrm()-&gt;prepare_for_full_collection_start();
1022 }
1023 
1024 void G1CollectedHeap::verify_before_full_collection(bool explicit_gc) {
1025   assert(!GCCause::is_user_requested_gc(gc_cause()) || explicit_gc, "invariant");
1026   assert_used_and_recalculate_used_equal(this);
1027   _verifier-&gt;verify_region_sets_optional();
1028   _verifier-&gt;verify_before_gc(G1HeapVerifier::G1VerifyFull);
1029   _verifier-&gt;check_bitmaps("Full GC Start");
1030 }
1031 
1032 void G1CollectedHeap::prepare_heap_for_mutators() {
1033   hrm()-&gt;prepare_for_full_collection_end();
1034 
1035   // Delete metaspaces for unloaded class loaders and clean up loader_data graph
1036   ClassLoaderDataGraph::purge(/*at_safepoint*/true);
<a name="1" id="anc1"></a><span class="changed">1037   DEBUG_ONLY(MetaspaceUtils::verify();)</span>
1038 
1039   // Prepare heap for normal collections.
1040   assert(num_free_regions() == 0, "we should not have added any free regions");
1041   rebuild_region_sets(false /* free_list_only */);
1042   abort_refinement();
1043   resize_heap_if_necessary();
1044 
1045   // Rebuild the strong code root lists for each region
1046   rebuild_strong_code_roots();
1047 
1048   // Purge code root memory
1049   purge_code_root_memory();
1050 
1051   // Start a new incremental collection set for the next pause
1052   start_new_collection_set();
1053 
1054   _allocator-&gt;init_mutator_alloc_regions();
1055 
1056   // Post collection state updates.
1057   MetaspaceGC::compute_new_size();
1058 }
1059 
1060 void G1CollectedHeap::abort_refinement() {
1061   if (_hot_card_cache-&gt;use_cache()) {
1062     _hot_card_cache-&gt;reset_hot_cache();
1063   }
1064 
1065   // Discard all remembered set updates and reset refinement statistics.
1066   G1BarrierSet::dirty_card_queue_set().abandon_logs();
1067   assert(G1BarrierSet::dirty_card_queue_set().num_cards() == 0,
1068          "DCQS should be empty");
1069   concurrent_refine()-&gt;get_and_reset_refinement_stats();
1070 }
1071 
1072 void G1CollectedHeap::verify_after_full_collection() {
1073   _hrm-&gt;verify_optional();
1074   _verifier-&gt;verify_region_sets_optional();
1075   _verifier-&gt;verify_after_gc(G1HeapVerifier::G1VerifyFull);
1076   // Clear the previous marking bitmap, if needed for bitmap verification.
1077   // Note we cannot do this when we clear the next marking bitmap in
1078   // G1ConcurrentMark::abort() above since VerifyDuringGC verifies the
1079   // objects marked during a full GC against the previous bitmap.
1080   // But we need to clear it before calling check_bitmaps below since
1081   // the full GC has compacted objects and updated TAMS but not updated
1082   // the prev bitmap.
1083   if (G1VerifyBitmaps) {
1084     GCTraceTime(Debug, gc) tm("Clear Prev Bitmap for Verification");
1085     _cm-&gt;clear_prev_bitmap(workers());
1086   }
1087   // This call implicitly verifies that the next bitmap is clear after Full GC.
1088   _verifier-&gt;check_bitmaps("Full GC End");
1089 
1090   // At this point there should be no regions in the
1091   // entire heap tagged as young.
1092   assert(check_young_list_empty(), "young list should be empty at this point");
1093 
1094   // Note: since we've just done a full GC, concurrent
1095   // marking is no longer active. Therefore we need not
1096   // re-enable reference discovery for the CM ref processor.
1097   // That will be done at the start of the next marking cycle.
1098   // We also know that the STW processor should no longer
1099   // discover any new references.
1100   assert(!_ref_processor_stw-&gt;discovery_enabled(), "Postcondition");
1101   assert(!_ref_processor_cm-&gt;discovery_enabled(), "Postcondition");
1102   _ref_processor_stw-&gt;verify_no_references_recorded();
1103   _ref_processor_cm-&gt;verify_no_references_recorded();
1104 }
1105 
1106 void G1CollectedHeap::print_heap_after_full_collection(G1HeapTransition* heap_transition) {
1107   // Post collection logging.
1108   // We should do this after we potentially resize the heap so
1109   // that all the COMMIT / UNCOMMIT events are generated before
1110   // the compaction events.
1111   print_hrm_post_compaction();
1112   heap_transition-&gt;print();
1113   print_heap_after_gc();
1114   print_heap_regions();
1115 }
1116 
1117 bool G1CollectedHeap::do_full_collection(bool explicit_gc,
1118                                          bool clear_all_soft_refs) {
1119   assert_at_safepoint_on_vm_thread();
1120 
1121   if (GCLocker::check_active_before_gc()) {
1122     // Full GC was not completed.
1123     return false;
1124   }
1125 
1126   const bool do_clear_all_soft_refs = clear_all_soft_refs ||
1127       soft_ref_policy()-&gt;should_clear_all_soft_refs();
1128 
1129   G1FullCollector collector(this, explicit_gc, do_clear_all_soft_refs);
1130   GCTraceTime(Info, gc) tm("Pause Full", NULL, gc_cause(), true);
1131 
1132   collector.prepare_collection();
1133   collector.collect();
1134   collector.complete_collection();
1135 
1136   // Full collection was successfully completed.
1137   return true;
1138 }
1139 
1140 void G1CollectedHeap::do_full_collection(bool clear_all_soft_refs) {
1141   // Currently, there is no facility in the do_full_collection(bool) API to notify
1142   // the caller that the collection did not succeed (e.g., because it was locked
1143   // out by the GC locker). So, right now, we'll ignore the return value.
1144   bool dummy = do_full_collection(true,                /* explicit_gc */
1145                                   clear_all_soft_refs);
1146 }
1147 
1148 void G1CollectedHeap::resize_heap_if_necessary() {
1149   assert_at_safepoint_on_vm_thread();
1150 
1151   bool should_expand;
1152   size_t resize_amount = _heap_sizing_policy-&gt;full_collection_resize_amount(should_expand);
1153 
1154   if (resize_amount == 0) {
1155     return;
1156   } else if (should_expand) {
1157     expand(resize_amount, _workers);
1158   } else {
1159     shrink(resize_amount);
1160   }
1161 }
1162 
1163 HeapWord* G1CollectedHeap::satisfy_failed_allocation_helper(size_t word_size,
1164                                                             bool do_gc,
1165                                                             bool clear_all_soft_refs,
1166                                                             bool expect_null_mutator_alloc_region,
1167                                                             bool* gc_succeeded) {
1168   *gc_succeeded = true;
1169   // Let's attempt the allocation first.
1170   HeapWord* result =
1171     attempt_allocation_at_safepoint(word_size,
1172                                     expect_null_mutator_alloc_region);
1173   if (result != NULL) {
1174     return result;
1175   }
1176 
1177   // In a G1 heap, we're supposed to keep allocation from failing by
1178   // incremental pauses.  Therefore, at least for now, we'll favor
1179   // expansion over collection.  (This might change in the future if we can
1180   // do something smarter than full collection to satisfy a failed alloc.)
1181   result = expand_and_allocate(word_size);
1182   if (result != NULL) {
1183     return result;
1184   }
1185 
1186   if (do_gc) {
1187     // Expansion didn't work, we'll try to do a Full GC.
1188     *gc_succeeded = do_full_collection(false, /* explicit_gc */
1189                                        clear_all_soft_refs);
1190   }
1191 
1192   return NULL;
1193 }
1194 
1195 HeapWord* G1CollectedHeap::satisfy_failed_allocation(size_t word_size,
1196                                                      bool* succeeded) {
1197   assert_at_safepoint_on_vm_thread();
1198 
1199   // Attempts to allocate followed by Full GC.
1200   HeapWord* result =
1201     satisfy_failed_allocation_helper(word_size,
1202                                      true,  /* do_gc */
1203                                      false, /* clear_all_soft_refs */
1204                                      false, /* expect_null_mutator_alloc_region */
1205                                      succeeded);
1206 
1207   if (result != NULL || !*succeeded) {
1208     return result;
1209   }
1210 
1211   // Attempts to allocate followed by Full GC that will collect all soft references.
1212   result = satisfy_failed_allocation_helper(word_size,
1213                                             true, /* do_gc */
1214                                             true, /* clear_all_soft_refs */
1215                                             true, /* expect_null_mutator_alloc_region */
1216                                             succeeded);
1217 
1218   if (result != NULL || !*succeeded) {
1219     return result;
1220   }
1221 
1222   // Attempts to allocate, no GC
1223   result = satisfy_failed_allocation_helper(word_size,
1224                                             false, /* do_gc */
1225                                             false, /* clear_all_soft_refs */
1226                                             true,  /* expect_null_mutator_alloc_region */
1227                                             succeeded);
1228 
1229   if (result != NULL) {
1230     return result;
1231   }
1232 
1233   assert(!soft_ref_policy()-&gt;should_clear_all_soft_refs(),
1234          "Flag should have been handled and cleared prior to this point");
1235 
1236   // What else?  We might try synchronous finalization later.  If the total
1237   // space available is large enough for the allocation, then a more
1238   // complete compaction phase than we've tried so far might be
1239   // appropriate.
1240   return NULL;
1241 }
1242 
1243 // Attempting to expand the heap sufficiently
1244 // to support an allocation of the given "word_size".  If
1245 // successful, perform the allocation and return the address of the
1246 // allocated block, or else "NULL".
1247 
1248 HeapWord* G1CollectedHeap::expand_and_allocate(size_t word_size) {
1249   assert_at_safepoint_on_vm_thread();
1250 
1251   _verifier-&gt;verify_region_sets_optional();
1252 
1253   size_t expand_bytes = MAX2(word_size * HeapWordSize, MinHeapDeltaBytes);
1254   log_debug(gc, ergo, heap)("Attempt heap expansion (allocation request failed). Allocation request: " SIZE_FORMAT "B",
1255                             word_size * HeapWordSize);
1256 
1257 
1258   if (expand(expand_bytes, _workers)) {
1259     _hrm-&gt;verify_optional();
1260     _verifier-&gt;verify_region_sets_optional();
1261     return attempt_allocation_at_safepoint(word_size,
1262                                            false /* expect_null_mutator_alloc_region */);
1263   }
1264   return NULL;
1265 }
1266 
1267 bool G1CollectedHeap::expand(size_t expand_bytes, WorkGang* pretouch_workers, double* expand_time_ms) {
1268   size_t aligned_expand_bytes = ReservedSpace::page_align_size_up(expand_bytes);
1269   aligned_expand_bytes = align_up(aligned_expand_bytes,
1270                                        HeapRegion::GrainBytes);
1271 
1272   log_debug(gc, ergo, heap)("Expand the heap. requested expansion amount: " SIZE_FORMAT "B expansion amount: " SIZE_FORMAT "B",
1273                             expand_bytes, aligned_expand_bytes);
1274 
1275   if (is_maximal_no_gc()) {
1276     log_debug(gc, ergo, heap)("Did not expand the heap (heap already fully expanded)");
1277     return false;
1278   }
1279 
1280   double expand_heap_start_time_sec = os::elapsedTime();
1281   uint regions_to_expand = (uint)(aligned_expand_bytes / HeapRegion::GrainBytes);
1282   assert(regions_to_expand &gt; 0, "Must expand by at least one region");
1283 
1284   uint expanded_by = _hrm-&gt;expand_by(regions_to_expand, pretouch_workers);
1285   if (expand_time_ms != NULL) {
1286     *expand_time_ms = (os::elapsedTime() - expand_heap_start_time_sec) * MILLIUNITS;
1287   }
1288 
1289   if (expanded_by &gt; 0) {
1290     size_t actual_expand_bytes = expanded_by * HeapRegion::GrainBytes;
1291     assert(actual_expand_bytes &lt;= aligned_expand_bytes, "post-condition");
1292     policy()-&gt;record_new_heap_size(num_regions());
1293   } else {
1294     log_debug(gc, ergo, heap)("Did not expand the heap (heap expansion operation failed)");
1295 
1296     // The expansion of the virtual storage space was unsuccessful.
1297     // Let's see if it was because we ran out of swap.
1298     if (G1ExitOnExpansionFailure &amp;&amp;
1299         _hrm-&gt;available() &gt;= regions_to_expand) {
1300       // We had head room...
1301       vm_exit_out_of_memory(aligned_expand_bytes, OOM_MMAP_ERROR, "G1 heap expansion");
1302     }
1303   }
1304   return regions_to_expand &gt; 0;
1305 }
1306 
1307 bool G1CollectedHeap::expand_single_region(uint node_index) {
1308   uint expanded_by = _hrm-&gt;expand_on_preferred_node(node_index);
1309 
1310   if (expanded_by == 0) {
1311     assert(is_maximal_no_gc(), "Should be no regions left, available: %u", _hrm-&gt;available());
1312     log_debug(gc, ergo, heap)("Did not expand the heap (heap already fully expanded)");
1313     return false;
1314   }
1315 
1316   policy()-&gt;record_new_heap_size(num_regions());
1317   return true;
1318 }
1319 
1320 void G1CollectedHeap::shrink_helper(size_t shrink_bytes) {
1321   size_t aligned_shrink_bytes =
1322     ReservedSpace::page_align_size_down(shrink_bytes);
1323   aligned_shrink_bytes = align_down(aligned_shrink_bytes,
1324                                          HeapRegion::GrainBytes);
1325   uint num_regions_to_remove = (uint)(shrink_bytes / HeapRegion::GrainBytes);
1326 
1327   uint num_regions_removed = _hrm-&gt;shrink_by(num_regions_to_remove);
1328   size_t shrunk_bytes = num_regions_removed * HeapRegion::GrainBytes;
1329 
1330   log_debug(gc, ergo, heap)("Shrink the heap. requested shrinking amount: " SIZE_FORMAT "B aligned shrinking amount: " SIZE_FORMAT "B attempted shrinking amount: " SIZE_FORMAT "B",
1331                             shrink_bytes, aligned_shrink_bytes, shrunk_bytes);
1332   if (num_regions_removed &gt; 0) {
1333     policy()-&gt;record_new_heap_size(num_regions());
1334   } else {
1335     log_debug(gc, ergo, heap)("Did not expand the heap (heap shrinking operation failed)");
1336   }
1337 }
1338 
1339 void G1CollectedHeap::shrink(size_t shrink_bytes) {
1340   _verifier-&gt;verify_region_sets_optional();
1341 
1342   // We should only reach here at the end of a Full GC or during Remark which
1343   // means we should not not be holding to any GC alloc regions. The method
1344   // below will make sure of that and do any remaining clean up.
1345   _allocator-&gt;abandon_gc_alloc_regions();
1346 
1347   // Instead of tearing down / rebuilding the free lists here, we
1348   // could instead use the remove_all_pending() method on free_list to
1349   // remove only the ones that we need to remove.
1350   tear_down_region_sets(true /* free_list_only */);
1351   shrink_helper(shrink_bytes);
1352   rebuild_region_sets(true /* free_list_only */);
1353 
1354   _hrm-&gt;verify_optional();
1355   _verifier-&gt;verify_region_sets_optional();
1356 }
1357 
1358 class OldRegionSetChecker : public HeapRegionSetChecker {
1359 public:
1360   void check_mt_safety() {
1361     // Master Old Set MT safety protocol:
1362     // (a) If we're at a safepoint, operations on the master old set
1363     // should be invoked:
1364     // - by the VM thread (which will serialize them), or
1365     // - by the GC workers while holding the FreeList_lock, if we're
1366     //   at a safepoint for an evacuation pause (this lock is taken
1367     //   anyway when an GC alloc region is retired so that a new one
1368     //   is allocated from the free list), or
1369     // - by the GC workers while holding the OldSets_lock, if we're at a
1370     //   safepoint for a cleanup pause.
1371     // (b) If we're not at a safepoint, operations on the master old set
1372     // should be invoked while holding the Heap_lock.
1373 
1374     if (SafepointSynchronize::is_at_safepoint()) {
1375       guarantee(Thread::current()-&gt;is_VM_thread() ||
1376                 FreeList_lock-&gt;owned_by_self() || OldSets_lock-&gt;owned_by_self(),
1377                 "master old set MT safety protocol at a safepoint");
1378     } else {
1379       guarantee(Heap_lock-&gt;owned_by_self(), "master old set MT safety protocol outside a safepoint");
1380     }
1381   }
1382   bool is_correct_type(HeapRegion* hr) { return hr-&gt;is_old(); }
1383   const char* get_description() { return "Old Regions"; }
1384 };
1385 
1386 class ArchiveRegionSetChecker : public HeapRegionSetChecker {
1387 public:
1388   void check_mt_safety() {
1389     guarantee(!Universe::is_fully_initialized() || SafepointSynchronize::is_at_safepoint(),
1390               "May only change archive regions during initialization or safepoint.");
1391   }
1392   bool is_correct_type(HeapRegion* hr) { return hr-&gt;is_archive(); }
1393   const char* get_description() { return "Archive Regions"; }
1394 };
1395 
1396 class HumongousRegionSetChecker : public HeapRegionSetChecker {
1397 public:
1398   void check_mt_safety() {
1399     // Humongous Set MT safety protocol:
1400     // (a) If we're at a safepoint, operations on the master humongous
1401     // set should be invoked by either the VM thread (which will
1402     // serialize them) or by the GC workers while holding the
1403     // OldSets_lock.
1404     // (b) If we're not at a safepoint, operations on the master
1405     // humongous set should be invoked while holding the Heap_lock.
1406 
1407     if (SafepointSynchronize::is_at_safepoint()) {
1408       guarantee(Thread::current()-&gt;is_VM_thread() ||
1409                 OldSets_lock-&gt;owned_by_self(),
1410                 "master humongous set MT safety protocol at a safepoint");
1411     } else {
1412       guarantee(Heap_lock-&gt;owned_by_self(),
1413                 "master humongous set MT safety protocol outside a safepoint");
1414     }
1415   }
1416   bool is_correct_type(HeapRegion* hr) { return hr-&gt;is_humongous(); }
1417   const char* get_description() { return "Humongous Regions"; }
1418 };
1419 
1420 G1CollectedHeap::G1CollectedHeap() :
1421   CollectedHeap(),
1422   _service_thread(NULL),
1423   _workers(NULL),
1424   _card_table(NULL),
1425   _collection_pause_end(Ticks::now()),
1426   _soft_ref_policy(),
1427   _old_set("Old Region Set", new OldRegionSetChecker()),
1428   _archive_set("Archive Region Set", new ArchiveRegionSetChecker()),
1429   _humongous_set("Humongous Region Set", new HumongousRegionSetChecker()),
1430   _bot(NULL),
1431   _listener(),
1432   _numa(G1NUMA::create()),
1433   _hrm(NULL),
1434   _allocator(NULL),
1435   _verifier(NULL),
1436   _summary_bytes_used(0),
1437   _bytes_used_during_gc(0),
1438   _archive_allocator(NULL),
1439   _survivor_evac_stats("Young", YoungPLABSize, PLABWeight),
1440   _old_evac_stats("Old", OldPLABSize, PLABWeight),
1441   _expand_heap_after_alloc_failure(true),
1442   _g1mm(NULL),
1443   _humongous_reclaim_candidates(),
1444   _has_humongous_reclaim_candidates(false),
1445   _hr_printer(),
1446   _collector_state(),
1447   _old_marking_cycles_started(0),
1448   _old_marking_cycles_completed(0),
1449   _eden(),
1450   _survivor(),
1451   _gc_timer_stw(new (ResourceObj::C_HEAP, mtGC) STWGCTimer()),
1452   _gc_tracer_stw(new (ResourceObj::C_HEAP, mtGC) G1NewTracer()),
1453   _policy(G1Policy::create_policy(_gc_timer_stw)),
1454   _heap_sizing_policy(NULL),
1455   _collection_set(this, _policy),
1456   _hot_card_cache(NULL),
1457   _rem_set(NULL),
1458   _cm(NULL),
1459   _cm_thread(NULL),
1460   _cr(NULL),
1461   _task_queues(NULL),
1462   _evacuation_failed(false),
1463   _evacuation_failed_info_array(NULL),
1464   _preserved_marks_set(true /* in_c_heap */),
1465 #ifndef PRODUCT
1466   _evacuation_failure_alot_for_current_gc(false),
1467   _evacuation_failure_alot_gc_number(0),
1468   _evacuation_failure_alot_count(0),
1469 #endif
1470   _ref_processor_stw(NULL),
1471   _is_alive_closure_stw(this),
1472   _is_subject_to_discovery_stw(this),
1473   _ref_processor_cm(NULL),
1474   _is_alive_closure_cm(this),
1475   _is_subject_to_discovery_cm(this),
1476   _region_attr() {
1477 
1478   _verifier = new G1HeapVerifier(this);
1479 
1480   _allocator = new G1Allocator(this);
1481 
1482   _heap_sizing_policy = G1HeapSizingPolicy::create(this, _policy-&gt;analytics());
1483 
1484   _humongous_object_threshold_in_words = humongous_threshold_for(HeapRegion::GrainWords);
1485 
1486   // Override the default _filler_array_max_size so that no humongous filler
1487   // objects are created.
1488   _filler_array_max_size = _humongous_object_threshold_in_words;
1489 
1490   uint n_queues = ParallelGCThreads;
1491   _task_queues = new G1ScannerTasksQueueSet(n_queues);
1492 
1493   _evacuation_failed_info_array = NEW_C_HEAP_ARRAY(EvacuationFailedInfo, n_queues, mtGC);
1494 
1495   for (uint i = 0; i &lt; n_queues; i++) {
1496     G1ScannerTasksQueue* q = new G1ScannerTasksQueue();
1497     q-&gt;initialize();
1498     _task_queues-&gt;register_queue(i, q);
1499     ::new (&amp;_evacuation_failed_info_array[i]) EvacuationFailedInfo();
1500   }
1501 
1502   // Initialize the G1EvacuationFailureALot counters and flags.
1503   NOT_PRODUCT(reset_evacuation_should_fail();)
1504   _gc_tracer_stw-&gt;initialize();
1505 
1506   guarantee(_task_queues != NULL, "task_queues allocation failure.");
1507 }
1508 
1509 static size_t actual_reserved_page_size(ReservedSpace rs) {
1510   size_t page_size = os::vm_page_size();
1511   if (UseLargePages) {
1512     // There are two ways to manage large page memory.
1513     // 1. OS supports committing large page memory.
1514     // 2. OS doesn't support committing large page memory so ReservedSpace manages it.
1515     //    And ReservedSpace calls it 'special'. If we failed to set 'special',
1516     //    we reserved memory without large page.
1517     if (os::can_commit_large_page_memory() || rs.special()) {
1518       // An alignment at ReservedSpace comes from preferred page size or
1519       // heap alignment, and if the alignment came from heap alignment, it could be
1520       // larger than large pages size. So need to cap with the large page size.
1521       page_size = MIN2(rs.alignment(), os::large_page_size());
1522     }
1523   }
1524 
1525   return page_size;
1526 }
1527 
1528 G1RegionToSpaceMapper* G1CollectedHeap::create_aux_memory_mapper(const char* description,
1529                                                                  size_t size,
1530                                                                  size_t translation_factor) {
1531   size_t preferred_page_size = os::page_size_for_region_unaligned(size, 1);
1532   // Allocate a new reserved space, preferring to use large pages.
1533   ReservedSpace rs(size, preferred_page_size);
1534   size_t page_size = actual_reserved_page_size(rs);
1535   G1RegionToSpaceMapper* result  =
1536     G1RegionToSpaceMapper::create_mapper(rs,
1537                                          size,
1538                                          page_size,
1539                                          HeapRegion::GrainBytes,
1540                                          translation_factor,
1541                                          mtGC);
1542 
1543   os::trace_page_sizes_for_requested_size(description,
1544                                           size,
1545                                           preferred_page_size,
1546                                           page_size,
1547                                           rs.base(),
1548                                           rs.size());
1549 
1550   return result;
1551 }
1552 
1553 jint G1CollectedHeap::initialize_concurrent_refinement() {
1554   jint ecode = JNI_OK;
1555   _cr = G1ConcurrentRefine::create(&amp;ecode);
1556   return ecode;
1557 }
1558 
1559 jint G1CollectedHeap::initialize_service_thread() {
1560   _service_thread = new G1ServiceThread();
1561   if (_service_thread-&gt;osthread() == NULL) {
1562     vm_shutdown_during_initialization("Could not create G1ServiceThread");
1563     return JNI_ENOMEM;
1564   }
1565   return JNI_OK;
1566 }
1567 
1568 jint G1CollectedHeap::initialize() {
1569 
1570   // Necessary to satisfy locking discipline assertions.
1571 
1572   MutexLocker x(Heap_lock);
1573 
1574   // While there are no constraints in the GC code that HeapWordSize
1575   // be any particular value, there are multiple other areas in the
1576   // system which believe this to be true (e.g. oop-&gt;object_size in some
1577   // cases incorrectly returns the size in wordSize units rather than
1578   // HeapWordSize).
1579   guarantee(HeapWordSize == wordSize, "HeapWordSize must equal wordSize");
1580 
1581   size_t init_byte_size = InitialHeapSize;
1582   size_t reserved_byte_size = G1Arguments::heap_reserved_size_bytes();
1583 
1584   // Ensure that the sizes are properly aligned.
1585   Universe::check_alignment(init_byte_size, HeapRegion::GrainBytes, "g1 heap");
1586   Universe::check_alignment(reserved_byte_size, HeapRegion::GrainBytes, "g1 heap");
1587   Universe::check_alignment(reserved_byte_size, HeapAlignment, "g1 heap");
1588 
1589   // Reserve the maximum.
1590 
1591   // When compressed oops are enabled, the preferred heap base
1592   // is calculated by subtracting the requested size from the
1593   // 32Gb boundary and using the result as the base address for
1594   // heap reservation. If the requested size is not aligned to
1595   // HeapRegion::GrainBytes (i.e. the alignment that is passed
1596   // into the ReservedHeapSpace constructor) then the actual
1597   // base of the reserved heap may end up differing from the
1598   // address that was requested (i.e. the preferred heap base).
1599   // If this happens then we could end up using a non-optimal
1600   // compressed oops mode.
1601 
1602   ReservedHeapSpace heap_rs = Universe::reserve_heap(reserved_byte_size,
1603                                                      HeapAlignment);
1604 
1605   initialize_reserved_region(heap_rs);
1606 
1607   // Create the barrier set for the entire reserved region.
1608   G1CardTable* ct = new G1CardTable(heap_rs.region());
1609   ct-&gt;initialize();
1610   G1BarrierSet* bs = new G1BarrierSet(ct);
1611   bs-&gt;initialize();
1612   assert(bs-&gt;is_a(BarrierSet::G1BarrierSet), "sanity");
1613   BarrierSet::set_barrier_set(bs);
1614   _card_table = ct;
1615 
1616   {
1617     G1SATBMarkQueueSet&amp; satbqs = bs-&gt;satb_mark_queue_set();
1618     satbqs.set_process_completed_buffers_threshold(G1SATBProcessCompletedThreshold);
1619     satbqs.set_buffer_enqueue_threshold_percentage(G1SATBBufferEnqueueingThresholdPercent);
1620   }
1621 
1622   // Create the hot card cache.
1623   _hot_card_cache = new G1HotCardCache(this);
1624 
1625   // Create space mappers.
1626   size_t page_size = actual_reserved_page_size(heap_rs);
1627   G1RegionToSpaceMapper* heap_storage =
1628     G1RegionToSpaceMapper::create_heap_mapper(heap_rs,
1629                                               heap_rs.size(),
1630                                               page_size,
1631                                               HeapRegion::GrainBytes,
1632                                               1,
1633                                               mtJavaHeap);
1634   if(heap_storage == NULL) {
1635     vm_shutdown_during_initialization("Could not initialize G1 heap");
1636     return JNI_ERR;
1637   }
1638 
1639   os::trace_page_sizes("Heap",
1640                        MinHeapSize,
1641                        reserved_byte_size,
1642                        page_size,
1643                        heap_rs.base(),
1644                        heap_rs.size());
1645   heap_storage-&gt;set_mapping_changed_listener(&amp;_listener);
1646 
1647   // Create storage for the BOT, card table, card counts table (hot card cache) and the bitmaps.
1648   G1RegionToSpaceMapper* bot_storage =
1649     create_aux_memory_mapper("Block Offset Table",
1650                              G1BlockOffsetTable::compute_size(heap_rs.size() / HeapWordSize),
1651                              G1BlockOffsetTable::heap_map_factor());
1652 
1653   G1RegionToSpaceMapper* cardtable_storage =
1654     create_aux_memory_mapper("Card Table",
1655                              G1CardTable::compute_size(heap_rs.size() / HeapWordSize),
1656                              G1CardTable::heap_map_factor());
1657 
1658   G1RegionToSpaceMapper* card_counts_storage =
1659     create_aux_memory_mapper("Card Counts Table",
1660                              G1CardCounts::compute_size(heap_rs.size() / HeapWordSize),
1661                              G1CardCounts::heap_map_factor());
1662 
1663   size_t bitmap_size = G1CMBitMap::compute_size(heap_rs.size());
1664   G1RegionToSpaceMapper* prev_bitmap_storage =
1665     create_aux_memory_mapper("Prev Bitmap", bitmap_size, G1CMBitMap::heap_map_factor());
1666   G1RegionToSpaceMapper* next_bitmap_storage =
1667     create_aux_memory_mapper("Next Bitmap", bitmap_size, G1CMBitMap::heap_map_factor());
1668 
1669   _hrm = HeapRegionManager::create_manager(this);
1670 
1671   _hrm-&gt;initialize(heap_storage, prev_bitmap_storage, next_bitmap_storage, bot_storage, cardtable_storage, card_counts_storage);
1672   _card_table-&gt;initialize(cardtable_storage);
1673 
1674   // Do later initialization work for concurrent refinement.
1675   _hot_card_cache-&gt;initialize(card_counts_storage);
1676 
1677   // 6843694 - ensure that the maximum region index can fit
1678   // in the remembered set structures.
1679   const uint max_region_idx = (1U &lt;&lt; (sizeof(RegionIdx_t)*BitsPerByte-1)) - 1;
1680   guarantee((max_regions() - 1) &lt;= max_region_idx, "too many regions");
1681 
1682   // The G1FromCardCache reserves card with value 0 as "invalid", so the heap must not
1683   // start within the first card.
1684   guarantee(heap_rs.base() &gt;= (char*)G1CardTable::card_size, "Java heap must not start within the first card.");
1685   // Also create a G1 rem set.
1686   _rem_set = new G1RemSet(this, _card_table, _hot_card_cache);
1687   _rem_set-&gt;initialize(max_regions());
1688 
1689   size_t max_cards_per_region = ((size_t)1 &lt;&lt; (sizeof(CardIdx_t)*BitsPerByte-1)) - 1;
1690   guarantee(HeapRegion::CardsPerRegion &gt; 0, "make sure it's initialized");
1691   guarantee(HeapRegion::CardsPerRegion &lt; max_cards_per_region,
1692             "too many cards per region");
1693 
1694   FreeRegionList::set_unrealistically_long_length(max_expandable_regions() + 1);
1695 
1696   _bot = new G1BlockOffsetTable(reserved(), bot_storage);
1697 
1698   {
1699     size_t granularity = HeapRegion::GrainBytes;
1700 
1701     _region_attr.initialize(reserved(), granularity);
1702     _humongous_reclaim_candidates.initialize(reserved(), granularity);
1703   }
1704 
1705   _workers = new WorkGang("GC Thread", ParallelGCThreads,
1706                           true /* are_GC_task_threads */,
1707                           false /* are_ConcurrentGC_threads */);
1708   if (_workers == NULL) {
1709     return JNI_ENOMEM;
1710   }
1711   _workers-&gt;initialize_workers();
1712 
1713   _numa-&gt;set_region_info(HeapRegion::GrainBytes, page_size);
1714 
1715   // Create the G1ConcurrentMark data structure and thread.
1716   // (Must do this late, so that "max_regions" is defined.)
1717   _cm = new G1ConcurrentMark(this, prev_bitmap_storage, next_bitmap_storage);
1718   _cm_thread = _cm-&gt;cm_thread();
1719 
1720   // Now expand into the initial heap size.
1721   if (!expand(init_byte_size, _workers)) {
1722     vm_shutdown_during_initialization("Failed to allocate initial heap.");
1723     return JNI_ENOMEM;
1724   }
1725 
1726   // Perform any initialization actions delegated to the policy.
1727   policy()-&gt;init(this, &amp;_collection_set);
1728 
1729   jint ecode = initialize_concurrent_refinement();
1730   if (ecode != JNI_OK) {
1731     return ecode;
1732   }
1733 
1734   ecode = initialize_service_thread();
1735   if (ecode != JNI_OK) {
1736     return ecode;
1737   }
1738 
1739   {
1740     G1DirtyCardQueueSet&amp; dcqs = G1BarrierSet::dirty_card_queue_set();
1741     dcqs.set_process_cards_threshold(concurrent_refine()-&gt;yellow_zone());
1742     dcqs.set_max_cards(concurrent_refine()-&gt;red_zone());
1743   }
1744 
1745   // Here we allocate the dummy HeapRegion that is required by the
1746   // G1AllocRegion class.
1747   HeapRegion* dummy_region = _hrm-&gt;get_dummy_region();
1748 
1749   // We'll re-use the same region whether the alloc region will
1750   // require BOT updates or not and, if it doesn't, then a non-young
1751   // region will complain that it cannot support allocations without
1752   // BOT updates. So we'll tag the dummy region as eden to avoid that.
1753   dummy_region-&gt;set_eden();
1754   // Make sure it's full.
1755   dummy_region-&gt;set_top(dummy_region-&gt;end());
1756   G1AllocRegion::setup(this, dummy_region);
1757 
1758   _allocator-&gt;init_mutator_alloc_regions();
1759 
1760   // Do create of the monitoring and management support so that
1761   // values in the heap have been properly initialized.
1762   _g1mm = new G1MonitoringSupport(this);
1763 
1764   G1StringDedup::initialize();
1765 
1766   _preserved_marks_set.init(ParallelGCThreads);
1767 
1768   _collection_set.initialize(max_regions());
1769 
1770   G1InitLogger::print();
1771 
1772   return JNI_OK;
1773 }
1774 
1775 void G1CollectedHeap::stop() {
1776   // Stop all concurrent threads. We do this to make sure these threads
1777   // do not continue to execute and access resources (e.g. logging)
1778   // that are destroyed during shutdown.
1779   _cr-&gt;stop();
1780   _service_thread-&gt;stop();
1781   _cm_thread-&gt;stop();
1782   if (G1StringDedup::is_enabled()) {
1783     G1StringDedup::stop();
1784   }
1785 }
1786 
1787 void G1CollectedHeap::safepoint_synchronize_begin() {
1788   SuspendibleThreadSet::synchronize();
1789 }
1790 
1791 void G1CollectedHeap::safepoint_synchronize_end() {
1792   SuspendibleThreadSet::desynchronize();
1793 }
1794 
1795 void G1CollectedHeap::post_initialize() {
1796   CollectedHeap::post_initialize();
1797   ref_processing_init();
1798 }
1799 
1800 void G1CollectedHeap::ref_processing_init() {
1801   // Reference processing in G1 currently works as follows:
1802   //
1803   // * There are two reference processor instances. One is
1804   //   used to record and process discovered references
1805   //   during concurrent marking; the other is used to
1806   //   record and process references during STW pauses
1807   //   (both full and incremental).
1808   // * Both ref processors need to 'span' the entire heap as
1809   //   the regions in the collection set may be dotted around.
1810   //
1811   // * For the concurrent marking ref processor:
1812   //   * Reference discovery is enabled at concurrent start.
1813   //   * Reference discovery is disabled and the discovered
1814   //     references processed etc during remarking.
1815   //   * Reference discovery is MT (see below).
1816   //   * Reference discovery requires a barrier (see below).
1817   //   * Reference processing may or may not be MT
1818   //     (depending on the value of ParallelRefProcEnabled
1819   //     and ParallelGCThreads).
1820   //   * A full GC disables reference discovery by the CM
1821   //     ref processor and abandons any entries on it's
1822   //     discovered lists.
1823   //
1824   // * For the STW processor:
1825   //   * Non MT discovery is enabled at the start of a full GC.
1826   //   * Processing and enqueueing during a full GC is non-MT.
1827   //   * During a full GC, references are processed after marking.
1828   //
1829   //   * Discovery (may or may not be MT) is enabled at the start
1830   //     of an incremental evacuation pause.
1831   //   * References are processed near the end of a STW evacuation pause.
1832   //   * For both types of GC:
1833   //     * Discovery is atomic - i.e. not concurrent.
1834   //     * Reference discovery will not need a barrier.
1835 
1836   bool mt_processing = ParallelRefProcEnabled &amp;&amp; (ParallelGCThreads &gt; 1);
1837 
1838   // Concurrent Mark ref processor
1839   _ref_processor_cm =
1840     new ReferenceProcessor(&amp;_is_subject_to_discovery_cm,
1841                            mt_processing,                                  // mt processing
1842                            ParallelGCThreads,                              // degree of mt processing
1843                            (ParallelGCThreads &gt; 1) || (ConcGCThreads &gt; 1), // mt discovery
1844                            MAX2(ParallelGCThreads, ConcGCThreads),         // degree of mt discovery
1845                            false,                                          // Reference discovery is not atomic
1846                            &amp;_is_alive_closure_cm,                          // is alive closure
1847                            true);                                          // allow changes to number of processing threads
1848 
1849   // STW ref processor
1850   _ref_processor_stw =
1851     new ReferenceProcessor(&amp;_is_subject_to_discovery_stw,
1852                            mt_processing,                        // mt processing
1853                            ParallelGCThreads,                    // degree of mt processing
1854                            (ParallelGCThreads &gt; 1),              // mt discovery
1855                            ParallelGCThreads,                    // degree of mt discovery
1856                            true,                                 // Reference discovery is atomic
1857                            &amp;_is_alive_closure_stw,               // is alive closure
1858                            true);                                // allow changes to number of processing threads
1859 }
1860 
1861 SoftRefPolicy* G1CollectedHeap::soft_ref_policy() {
1862   return &amp;_soft_ref_policy;
1863 }
1864 
1865 size_t G1CollectedHeap::capacity() const {
1866   return _hrm-&gt;length() * HeapRegion::GrainBytes;
1867 }
1868 
1869 size_t G1CollectedHeap::unused_committed_regions_in_bytes() const {
1870   return _hrm-&gt;total_free_bytes();
1871 }
1872 
1873 void G1CollectedHeap::iterate_hcc_closure(G1CardTableEntryClosure* cl, uint worker_id) {
1874   _hot_card_cache-&gt;drain(cl, worker_id);
1875 }
1876 
1877 // Computes the sum of the storage used by the various regions.
1878 size_t G1CollectedHeap::used() const {
1879   size_t result = _summary_bytes_used + _allocator-&gt;used_in_alloc_regions();
1880   if (_archive_allocator != NULL) {
1881     result += _archive_allocator-&gt;used();
1882   }
1883   return result;
1884 }
1885 
1886 size_t G1CollectedHeap::used_unlocked() const {
1887   return _summary_bytes_used;
1888 }
1889 
1890 class SumUsedClosure: public HeapRegionClosure {
1891   size_t _used;
1892 public:
1893   SumUsedClosure() : _used(0) {}
1894   bool do_heap_region(HeapRegion* r) {
1895     _used += r-&gt;used();
1896     return false;
1897   }
1898   size_t result() { return _used; }
1899 };
1900 
1901 size_t G1CollectedHeap::recalculate_used() const {
1902   SumUsedClosure blk;
1903   heap_region_iterate(&amp;blk);
1904   return blk.result();
1905 }
1906 
1907 bool  G1CollectedHeap::is_user_requested_concurrent_full_gc(GCCause::Cause cause) {
1908   switch (cause) {
1909     case GCCause::_java_lang_system_gc:                 return ExplicitGCInvokesConcurrent;
1910     case GCCause::_dcmd_gc_run:                         return ExplicitGCInvokesConcurrent;
1911     case GCCause::_wb_conc_mark:                        return true;
1912     default :                                           return false;
1913   }
1914 }
1915 
1916 bool G1CollectedHeap::should_do_concurrent_full_gc(GCCause::Cause cause) {
1917   switch (cause) {
1918     case GCCause::_g1_humongous_allocation: return true;
1919     case GCCause::_g1_periodic_collection:  return G1PeriodicGCInvokesConcurrent;
1920     case GCCause::_wb_breakpoint:           return true;
1921     default:                                return is_user_requested_concurrent_full_gc(cause);
1922   }
1923 }
1924 
1925 bool G1CollectedHeap::should_upgrade_to_full_gc(GCCause::Cause cause) {
1926   if (policy()-&gt;force_upgrade_to_full()) {
1927     return true;
1928   } else if (should_do_concurrent_full_gc(_gc_cause)) {
1929     return false;
1930   } else if (has_regions_left_for_allocation()) {
1931     return false;
1932   } else {
1933     return true;
1934   }
1935 }
1936 
1937 #ifndef PRODUCT
1938 void G1CollectedHeap::allocate_dummy_regions() {
1939   // Let's fill up most of the region
1940   size_t word_size = HeapRegion::GrainWords - 1024;
1941   // And as a result the region we'll allocate will be humongous.
1942   guarantee(is_humongous(word_size), "sanity");
1943 
1944   // _filler_array_max_size is set to humongous object threshold
1945   // but temporarily change it to use CollectedHeap::fill_with_object().
1946   AutoModifyRestore&lt;size_t&gt; temporarily(_filler_array_max_size, word_size);
1947 
1948   for (uintx i = 0; i &lt; G1DummyRegionsPerGC; ++i) {
1949     // Let's use the existing mechanism for the allocation
1950     HeapWord* dummy_obj = humongous_obj_allocate(word_size);
1951     if (dummy_obj != NULL) {
1952       MemRegion mr(dummy_obj, word_size);
1953       CollectedHeap::fill_with_object(mr);
1954     } else {
1955       // If we can't allocate once, we probably cannot allocate
1956       // again. Let's get out of the loop.
1957       break;
1958     }
1959   }
1960 }
1961 #endif // !PRODUCT
1962 
1963 void G1CollectedHeap::increment_old_marking_cycles_started() {
1964   assert(_old_marking_cycles_started == _old_marking_cycles_completed ||
1965          _old_marking_cycles_started == _old_marking_cycles_completed + 1,
1966          "Wrong marking cycle count (started: %d, completed: %d)",
1967          _old_marking_cycles_started, _old_marking_cycles_completed);
1968 
1969   _old_marking_cycles_started++;
1970 }
1971 
1972 void G1CollectedHeap::increment_old_marking_cycles_completed(bool concurrent,
1973                                                              bool whole_heap_examined) {
1974   MonitorLocker ml(G1OldGCCount_lock, Mutex::_no_safepoint_check_flag);
1975 
1976   // We assume that if concurrent == true, then the caller is a
1977   // concurrent thread that was joined the Suspendible Thread
1978   // Set. If there's ever a cheap way to check this, we should add an
1979   // assert here.
1980 
1981   // Given that this method is called at the end of a Full GC or of a
1982   // concurrent cycle, and those can be nested (i.e., a Full GC can
1983   // interrupt a concurrent cycle), the number of full collections
1984   // completed should be either one (in the case where there was no
1985   // nesting) or two (when a Full GC interrupted a concurrent cycle)
1986   // behind the number of full collections started.
1987 
1988   // This is the case for the inner caller, i.e. a Full GC.
1989   assert(concurrent ||
1990          (_old_marking_cycles_started == _old_marking_cycles_completed + 1) ||
1991          (_old_marking_cycles_started == _old_marking_cycles_completed + 2),
1992          "for inner caller (Full GC): _old_marking_cycles_started = %u "
1993          "is inconsistent with _old_marking_cycles_completed = %u",
1994          _old_marking_cycles_started, _old_marking_cycles_completed);
1995 
1996   // This is the case for the outer caller, i.e. the concurrent cycle.
1997   assert(!concurrent ||
1998          (_old_marking_cycles_started == _old_marking_cycles_completed + 1),
1999          "for outer caller (concurrent cycle): "
2000          "_old_marking_cycles_started = %u "
2001          "is inconsistent with _old_marking_cycles_completed = %u",
2002          _old_marking_cycles_started, _old_marking_cycles_completed);
2003 
2004   _old_marking_cycles_completed += 1;
2005   if (whole_heap_examined) {
2006     // Signal that we have completed a visit to all live objects.
2007     record_whole_heap_examined_timestamp();
2008   }
2009 
2010   // We need to clear the "in_progress" flag in the CM thread before
2011   // we wake up any waiters (especially when ExplicitInvokesConcurrent
2012   // is set) so that if a waiter requests another System.gc() it doesn't
2013   // incorrectly see that a marking cycle is still in progress.
2014   if (concurrent) {
2015     _cm_thread-&gt;set_idle();
2016   }
2017 
2018   // Notify threads waiting in System.gc() (with ExplicitGCInvokesConcurrent)
2019   // for a full GC to finish that their wait is over.
2020   ml.notify_all();
2021 }
2022 
2023 void G1CollectedHeap::collect(GCCause::Cause cause) {
2024   try_collect(cause);
2025 }
2026 
2027 // Return true if (x &lt; y) with allowance for wraparound.
2028 static bool gc_counter_less_than(uint x, uint y) {
2029   return (x - y) &gt; (UINT_MAX/2);
2030 }
2031 
2032 // LOG_COLLECT_CONCURRENTLY(cause, msg, args...)
2033 // Macro so msg printing is format-checked.
2034 #define LOG_COLLECT_CONCURRENTLY(cause, ...)                            \
2035   do {                                                                  \
2036     LogTarget(Trace, gc) LOG_COLLECT_CONCURRENTLY_lt;                   \
2037     if (LOG_COLLECT_CONCURRENTLY_lt.is_enabled()) {                     \
2038       ResourceMark rm; /* For thread name. */                           \
2039       LogStream LOG_COLLECT_CONCURRENTLY_s(&amp;LOG_COLLECT_CONCURRENTLY_lt); \
2040       LOG_COLLECT_CONCURRENTLY_s.print("%s: Try Collect Concurrently (%s): ", \
2041                                        Thread::current()-&gt;name(),       \
2042                                        GCCause::to_string(cause));      \
2043       LOG_COLLECT_CONCURRENTLY_s.print(__VA_ARGS__);                    \
2044     }                                                                   \
2045   } while (0)
2046 
2047 #define LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, result) \
2048   LOG_COLLECT_CONCURRENTLY(cause, "complete %s", BOOL_TO_STR(result))
2049 
2050 bool G1CollectedHeap::try_collect_concurrently(GCCause::Cause cause,
2051                                                uint gc_counter,
2052                                                uint old_marking_started_before) {
2053   assert_heap_not_locked();
2054   assert(should_do_concurrent_full_gc(cause),
2055          "Non-concurrent cause %s", GCCause::to_string(cause));
2056 
2057   for (uint i = 1; true; ++i) {
2058     // Try to schedule concurrent start evacuation pause that will
2059     // start a concurrent cycle.
2060     LOG_COLLECT_CONCURRENTLY(cause, "attempt %u", i);
2061     VM_G1TryInitiateConcMark op(gc_counter,
2062                                 cause,
2063                                 policy()-&gt;max_pause_time_ms());
2064     VMThread::execute(&amp;op);
2065 
2066     // Request is trivially finished.
2067     if (cause == GCCause::_g1_periodic_collection) {
2068       LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, op.gc_succeeded());
2069       return op.gc_succeeded();
2070     }
2071 
2072     // If VMOp skipped initiating concurrent marking cycle because
2073     // we're terminating, then we're done.
2074     if (op.terminating()) {
2075       LOG_COLLECT_CONCURRENTLY(cause, "skipped: terminating");
2076       return false;
2077     }
2078 
2079     // Lock to get consistent set of values.
2080     uint old_marking_started_after;
2081     uint old_marking_completed_after;
2082     {
2083       MutexLocker ml(Heap_lock);
2084       // Update gc_counter for retrying VMOp if needed. Captured here to be
2085       // consistent with the values we use below for termination tests.  If
2086       // a retry is needed after a possible wait, and another collection
2087       // occurs in the meantime, it will cause our retry to be skipped and
2088       // we'll recheck for termination with updated conditions from that
2089       // more recent collection.  That's what we want, rather than having
2090       // our retry possibly perform an unnecessary collection.
2091       gc_counter = total_collections();
2092       old_marking_started_after = _old_marking_cycles_started;
2093       old_marking_completed_after = _old_marking_cycles_completed;
2094     }
2095 
2096     if (cause == GCCause::_wb_breakpoint) {
2097       if (op.gc_succeeded()) {
2098         LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);
2099         return true;
2100       }
2101       // When _wb_breakpoint there can't be another cycle or deferred.
2102       assert(!op.cycle_already_in_progress(), "invariant");
2103       assert(!op.whitebox_attached(), "invariant");
2104       // Concurrent cycle attempt might have been cancelled by some other
2105       // collection, so retry.  Unlike other cases below, we want to retry
2106       // even if cancelled by a STW full collection, because we really want
2107       // to start a concurrent cycle.
2108       if (old_marking_started_before != old_marking_started_after) {
2109         LOG_COLLECT_CONCURRENTLY(cause, "ignoring STW full GC");
2110         old_marking_started_before = old_marking_started_after;
2111       }
2112     } else if (!GCCause::is_user_requested_gc(cause)) {
2113       // For an "automatic" (not user-requested) collection, we just need to
2114       // ensure that progress is made.
2115       //
2116       // Request is finished if any of
2117       // (1) the VMOp successfully performed a GC,
2118       // (2) a concurrent cycle was already in progress,
2119       // (3) whitebox is controlling concurrent cycles,
2120       // (4) a new cycle was started (by this thread or some other), or
2121       // (5) a Full GC was performed.
2122       // Cases (4) and (5) are detected together by a change to
2123       // _old_marking_cycles_started.
2124       //
2125       // Note that (1) does not imply (4).  If we're still in the mixed
2126       // phase of an earlier concurrent collection, the request to make the
2127       // collection a concurrent start won't be honored.  If we don't check for
2128       // both conditions we'll spin doing back-to-back collections.
2129       if (op.gc_succeeded() ||
2130           op.cycle_already_in_progress() ||
2131           op.whitebox_attached() ||
2132           (old_marking_started_before != old_marking_started_after)) {
2133         LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);
2134         return true;
2135       }
2136     } else {                    // User-requested GC.
2137       // For a user-requested collection, we want to ensure that a complete
2138       // full collection has been performed before returning, but without
2139       // waiting for more than needed.
2140 
2141       // For user-requested GCs (unlike non-UR), a successful VMOp implies a
2142       // new cycle was started.  That's good, because it's not clear what we
2143       // should do otherwise.  Trying again just does back to back GCs.
2144       // Can't wait for someone else to start a cycle.  And returning fails
2145       // to meet the goal of ensuring a full collection was performed.
2146       assert(!op.gc_succeeded() ||
2147              (old_marking_started_before != old_marking_started_after),
2148              "invariant: succeeded %s, started before %u, started after %u",
2149              BOOL_TO_STR(op.gc_succeeded()),
2150              old_marking_started_before, old_marking_started_after);
2151 
2152       // Request is finished if a full collection (concurrent or stw)
2153       // was started after this request and has completed, e.g.
2154       // started_before &lt; completed_after.
2155       if (gc_counter_less_than(old_marking_started_before,
2156                                old_marking_completed_after)) {
2157         LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);
2158         return true;
2159       }
2160 
2161       if (old_marking_started_after != old_marking_completed_after) {
2162         // If there is an in-progress cycle (possibly started by us), then
2163         // wait for that cycle to complete, e.g.
2164         // while completed_now &lt; started_after.
2165         LOG_COLLECT_CONCURRENTLY(cause, "wait");
2166         MonitorLocker ml(G1OldGCCount_lock);
2167         while (gc_counter_less_than(_old_marking_cycles_completed,
2168                                     old_marking_started_after)) {
2169           ml.wait();
2170         }
2171         // Request is finished if the collection we just waited for was
2172         // started after this request.
2173         if (old_marking_started_before != old_marking_started_after) {
2174           LOG_COLLECT_CONCURRENTLY(cause, "complete after wait");
2175           return true;
2176         }
2177       }
2178 
2179       // If VMOp was successful then it started a new cycle that the above
2180       // wait &amp;etc should have recognized as finishing this request.  This
2181       // differs from a non-user-request, where gc_succeeded does not imply
2182       // a new cycle was started.
2183       assert(!op.gc_succeeded(), "invariant");
2184 
2185       if (op.cycle_already_in_progress()) {
2186         // If VMOp failed because a cycle was already in progress, it
2187         // is now complete.  But it didn't finish this user-requested
2188         // GC, so try again.
2189         LOG_COLLECT_CONCURRENTLY(cause, "retry after in-progress");
2190         continue;
2191       } else if (op.whitebox_attached()) {
2192         // If WhiteBox wants control, wait for notification of a state
2193         // change in the controller, then try again.  Don't wait for
2194         // release of control, since collections may complete while in
2195         // control.  Note: This won't recognize a STW full collection
2196         // while waiting; we can't wait on multiple monitors.
2197         LOG_COLLECT_CONCURRENTLY(cause, "whitebox control stall");
2198         MonitorLocker ml(ConcurrentGCBreakpoints::monitor());
2199         if (ConcurrentGCBreakpoints::is_controlled()) {
2200           ml.wait();
2201         }
2202         continue;
2203       }
2204     }
2205 
2206     // Collection failed and should be retried.
2207     assert(op.transient_failure(), "invariant");
2208 
2209     if (GCLocker::is_active_and_needs_gc()) {
2210       // If GCLocker is active, wait until clear before retrying.
2211       LOG_COLLECT_CONCURRENTLY(cause, "gc-locker stall");
2212       GCLocker::stall_until_clear();
2213     }
2214 
2215     LOG_COLLECT_CONCURRENTLY(cause, "retry");
2216   }
2217 }
2218 
2219 bool G1CollectedHeap::try_collect(GCCause::Cause cause) {
2220   assert_heap_not_locked();
2221 
2222   // Lock to get consistent set of values.
2223   uint gc_count_before;
2224   uint full_gc_count_before;
2225   uint old_marking_started_before;
2226   {
2227     MutexLocker ml(Heap_lock);
2228     gc_count_before = total_collections();
2229     full_gc_count_before = total_full_collections();
2230     old_marking_started_before = _old_marking_cycles_started;
2231   }
2232 
2233   if (should_do_concurrent_full_gc(cause)) {
2234     return try_collect_concurrently(cause,
2235                                     gc_count_before,
2236                                     old_marking_started_before);
2237   } else if (GCLocker::should_discard(cause, gc_count_before)) {
2238     // Indicate failure to be consistent with VMOp failure due to
2239     // another collection slipping in after our gc_count but before
2240     // our request is processed.
2241     return false;
2242   } else if (cause == GCCause::_gc_locker || cause == GCCause::_wb_young_gc
2243              DEBUG_ONLY(|| cause == GCCause::_scavenge_alot)) {
2244 
2245     // Schedule a standard evacuation pause. We're setting word_size
2246     // to 0 which means that we are not requesting a post-GC allocation.
2247     VM_G1CollectForAllocation op(0,     /* word_size */
2248                                  gc_count_before,
2249                                  cause,
2250                                  policy()-&gt;max_pause_time_ms());
2251     VMThread::execute(&amp;op);
2252     return op.gc_succeeded();
2253   } else {
2254     // Schedule a Full GC.
2255     VM_G1CollectFull op(gc_count_before, full_gc_count_before, cause);
2256     VMThread::execute(&amp;op);
2257     return op.gc_succeeded();
2258   }
2259 }
2260 
2261 bool G1CollectedHeap::is_in(const void* p) const {
2262   if (_hrm-&gt;reserved().contains(p)) {
2263     // Given that we know that p is in the reserved space,
2264     // heap_region_containing() should successfully
2265     // return the containing region.
2266     HeapRegion* hr = heap_region_containing(p);
2267     return hr-&gt;is_in(p);
2268   } else {
2269     return false;
2270   }
2271 }
2272 
2273 #ifdef ASSERT
2274 bool G1CollectedHeap::is_in_exact(const void* p) const {
2275   bool contains = reserved().contains(p);
2276   bool available = _hrm-&gt;is_available(addr_to_region((HeapWord*)p));
2277   if (contains &amp;&amp; available) {
2278     return true;
2279   } else {
2280     return false;
2281   }
2282 }
2283 #endif
2284 
2285 // Iteration functions.
2286 
2287 // Iterates an ObjectClosure over all objects within a HeapRegion.
2288 
2289 class IterateObjectClosureRegionClosure: public HeapRegionClosure {
2290   ObjectClosure* _cl;
2291 public:
2292   IterateObjectClosureRegionClosure(ObjectClosure* cl) : _cl(cl) {}
2293   bool do_heap_region(HeapRegion* r) {
2294     if (!r-&gt;is_continues_humongous()) {
2295       r-&gt;object_iterate(_cl);
2296     }
2297     return false;
2298   }
2299 };
2300 
2301 void G1CollectedHeap::object_iterate(ObjectClosure* cl) {
2302   IterateObjectClosureRegionClosure blk(cl);
2303   heap_region_iterate(&amp;blk);
2304 }
2305 
2306 class G1ParallelObjectIterator : public ParallelObjectIterator {
2307 private:
2308   G1CollectedHeap*  _heap;
2309   HeapRegionClaimer _claimer;
2310 
2311 public:
2312   G1ParallelObjectIterator(uint thread_num) :
2313       _heap(G1CollectedHeap::heap()),
2314       _claimer(thread_num == 0 ? G1CollectedHeap::heap()-&gt;workers()-&gt;active_workers() : thread_num) {}
2315 
2316   virtual void object_iterate(ObjectClosure* cl, uint worker_id) {
2317     _heap-&gt;object_iterate_parallel(cl, worker_id, &amp;_claimer);
2318   }
2319 };
2320 
2321 ParallelObjectIterator* G1CollectedHeap::parallel_object_iterator(uint thread_num) {
2322   return new G1ParallelObjectIterator(thread_num);
2323 }
2324 
2325 void G1CollectedHeap::object_iterate_parallel(ObjectClosure* cl, uint worker_id, HeapRegionClaimer* claimer) {
2326   IterateObjectClosureRegionClosure blk(cl);
2327   heap_region_par_iterate_from_worker_offset(&amp;blk, claimer, worker_id);
2328 }
2329 
2330 void G1CollectedHeap::keep_alive(oop obj) {
2331   G1BarrierSet::enqueue(obj);
2332 }
2333 
2334 void G1CollectedHeap::heap_region_iterate(HeapRegionClosure* cl) const {
2335   _hrm-&gt;iterate(cl);
2336 }
2337 
2338 void G1CollectedHeap::heap_region_par_iterate_from_worker_offset(HeapRegionClosure* cl,
2339                                                                  HeapRegionClaimer *hrclaimer,
2340                                                                  uint worker_id) const {
2341   _hrm-&gt;par_iterate(cl, hrclaimer, hrclaimer-&gt;offset_for_worker(worker_id));
2342 }
2343 
2344 void G1CollectedHeap::heap_region_par_iterate_from_start(HeapRegionClosure* cl,
2345                                                          HeapRegionClaimer *hrclaimer) const {
2346   _hrm-&gt;par_iterate(cl, hrclaimer, 0);
2347 }
2348 
2349 void G1CollectedHeap::collection_set_iterate_all(HeapRegionClosure* cl) {
2350   _collection_set.iterate(cl);
2351 }
2352 
2353 void G1CollectedHeap::collection_set_par_iterate_all(HeapRegionClosure* cl, HeapRegionClaimer* hr_claimer, uint worker_id) {
2354   _collection_set.par_iterate(cl, hr_claimer, worker_id, workers()-&gt;active_workers());
2355 }
2356 
2357 void G1CollectedHeap::collection_set_iterate_increment_from(HeapRegionClosure *cl, HeapRegionClaimer* hr_claimer, uint worker_id) {
2358   _collection_set.iterate_incremental_part_from(cl, hr_claimer, worker_id, workers()-&gt;active_workers());
2359 }
2360 
2361 HeapWord* G1CollectedHeap::block_start(const void* addr) const {
2362   HeapRegion* hr = heap_region_containing(addr);
2363   return hr-&gt;block_start(addr);
2364 }
2365 
2366 bool G1CollectedHeap::block_is_obj(const HeapWord* addr) const {
2367   HeapRegion* hr = heap_region_containing(addr);
2368   return hr-&gt;block_is_obj(addr);
2369 }
2370 
2371 bool G1CollectedHeap::supports_tlab_allocation() const {
2372   return true;
2373 }
2374 
2375 size_t G1CollectedHeap::tlab_capacity(Thread* ignored) const {
2376   return (_policy-&gt;young_list_target_length() - _survivor.length()) * HeapRegion::GrainBytes;
2377 }
2378 
2379 size_t G1CollectedHeap::tlab_used(Thread* ignored) const {
2380   return _eden.length() * HeapRegion::GrainBytes;
2381 }
2382 
2383 // For G1 TLABs should not contain humongous objects, so the maximum TLAB size
2384 // must be equal to the humongous object limit.
2385 size_t G1CollectedHeap::max_tlab_size() const {
2386   return align_down(_humongous_object_threshold_in_words, MinObjAlignment);
2387 }
2388 
2389 size_t G1CollectedHeap::unsafe_max_tlab_alloc(Thread* ignored) const {
2390   return _allocator-&gt;unsafe_max_tlab_alloc();
2391 }
2392 
2393 size_t G1CollectedHeap::max_capacity() const {
2394   return _hrm-&gt;max_expandable_length() * HeapRegion::GrainBytes;
2395 }
2396 
2397 void G1CollectedHeap::deduplicate_string(oop str) {
2398   assert(java_lang_String::is_instance(str), "invariant");
2399 
2400   if (G1StringDedup::is_enabled()) {
2401     G1StringDedup::deduplicate(str);
2402   }
2403 }
2404 
2405 void G1CollectedHeap::prepare_for_verify() {
2406   _verifier-&gt;prepare_for_verify();
2407 }
2408 
2409 void G1CollectedHeap::verify(VerifyOption vo) {
2410   _verifier-&gt;verify(vo);
2411 }
2412 
2413 bool G1CollectedHeap::supports_concurrent_gc_breakpoints() const {
2414   return true;
2415 }
2416 
2417 bool G1CollectedHeap::is_heterogeneous_heap() const {
2418   return G1Arguments::is_heterogeneous_heap();
2419 }
2420 
2421 class PrintRegionClosure: public HeapRegionClosure {
2422   outputStream* _st;
2423 public:
2424   PrintRegionClosure(outputStream* st) : _st(st) {}
2425   bool do_heap_region(HeapRegion* r) {
2426     r-&gt;print_on(_st);
2427     return false;
2428   }
2429 };
2430 
2431 bool G1CollectedHeap::is_obj_dead_cond(const oop obj,
2432                                        const HeapRegion* hr,
2433                                        const VerifyOption vo) const {
2434   switch (vo) {
2435   case VerifyOption_G1UsePrevMarking: return is_obj_dead(obj, hr);
2436   case VerifyOption_G1UseNextMarking: return is_obj_ill(obj, hr);
2437   case VerifyOption_G1UseFullMarking: return is_obj_dead_full(obj, hr);
2438   default:                            ShouldNotReachHere();
2439   }
2440   return false; // keep some compilers happy
2441 }
2442 
2443 bool G1CollectedHeap::is_obj_dead_cond(const oop obj,
2444                                        const VerifyOption vo) const {
2445   switch (vo) {
2446   case VerifyOption_G1UsePrevMarking: return is_obj_dead(obj);
2447   case VerifyOption_G1UseNextMarking: return is_obj_ill(obj);
2448   case VerifyOption_G1UseFullMarking: return is_obj_dead_full(obj);
2449   default:                            ShouldNotReachHere();
2450   }
2451   return false; // keep some compilers happy
2452 }
2453 
2454 void G1CollectedHeap::print_heap_regions() const {
2455   LogTarget(Trace, gc, heap, region) lt;
2456   if (lt.is_enabled()) {
2457     LogStream ls(lt);
2458     print_regions_on(&amp;ls);
2459   }
2460 }
2461 
2462 void G1CollectedHeap::print_on(outputStream* st) const {
2463   st-&gt;print(" %-20s", "garbage-first heap");
2464   if (_hrm != NULL) {
2465     st-&gt;print(" total " SIZE_FORMAT "K, used " SIZE_FORMAT "K",
2466               capacity()/K, used_unlocked()/K);
2467     st-&gt;print(" [" PTR_FORMAT ", " PTR_FORMAT ")",
2468               p2i(_hrm-&gt;reserved().start()),
2469               p2i(_hrm-&gt;reserved().end()));
2470   }
2471   st-&gt;cr();
2472   st-&gt;print("  region size " SIZE_FORMAT "K, ", HeapRegion::GrainBytes / K);
2473   uint young_regions = young_regions_count();
2474   st-&gt;print("%u young (" SIZE_FORMAT "K), ", young_regions,
2475             (size_t) young_regions * HeapRegion::GrainBytes / K);
2476   uint survivor_regions = survivor_regions_count();
2477   st-&gt;print("%u survivors (" SIZE_FORMAT "K)", survivor_regions,
2478             (size_t) survivor_regions * HeapRegion::GrainBytes / K);
2479   st-&gt;cr();
2480   if (_numa-&gt;is_enabled()) {
2481     uint num_nodes = _numa-&gt;num_active_nodes();
2482     st-&gt;print("  remaining free region(s) on each NUMA node: ");
2483     const int* node_ids = _numa-&gt;node_ids();
2484     for (uint node_index = 0; node_index &lt; num_nodes; node_index++) {
2485       uint num_free_regions = (_hrm != NULL ? _hrm-&gt;num_free_regions(node_index) : 0);
2486       st-&gt;print("%d=%u ", node_ids[node_index], num_free_regions);
2487     }
2488     st-&gt;cr();
2489   }
2490   MetaspaceUtils::print_on(st);
2491 }
2492 
2493 void G1CollectedHeap::print_regions_on(outputStream* st) const {
2494   if (_hrm == NULL) {
2495     return;
2496   }
2497 
2498   st-&gt;print_cr("Heap Regions: E=young(eden), S=young(survivor), O=old, "
2499                "HS=humongous(starts), HC=humongous(continues), "
2500                "CS=collection set, F=free, "
2501                "OA=open archive, CA=closed archive, "
2502                "TAMS=top-at-mark-start (previous, next)");
2503   PrintRegionClosure blk(st);
2504   heap_region_iterate(&amp;blk);
2505 }
2506 
2507 void G1CollectedHeap::print_extended_on(outputStream* st) const {
2508   print_on(st);
2509 
2510   // Print the per-region information.
2511   if (_hrm != NULL) {
2512     st-&gt;cr();
2513     print_regions_on(st);
2514   }
2515 }
2516 
2517 void G1CollectedHeap::print_on_error(outputStream* st) const {
2518   this-&gt;CollectedHeap::print_on_error(st);
2519 
2520   if (_cm != NULL) {
2521     st-&gt;cr();
2522     _cm-&gt;print_on_error(st);
2523   }
2524 }
2525 
2526 void G1CollectedHeap::gc_threads_do(ThreadClosure* tc) const {
2527   workers()-&gt;threads_do(tc);
2528   tc-&gt;do_thread(_cm_thread);
2529   _cm-&gt;threads_do(tc);
2530   _cr-&gt;threads_do(tc);
2531   tc-&gt;do_thread(_service_thread);
2532   if (G1StringDedup::is_enabled()) {
2533     G1StringDedup::threads_do(tc);
2534   }
2535 }
2536 
2537 void G1CollectedHeap::print_tracing_info() const {
2538   rem_set()-&gt;print_summary_info();
2539   concurrent_mark()-&gt;print_summary_info();
2540 }
2541 
2542 #ifndef PRODUCT
2543 // Helpful for debugging RSet issues.
2544 
2545 class PrintRSetsClosure : public HeapRegionClosure {
2546 private:
2547   const char* _msg;
2548   size_t _occupied_sum;
2549 
2550 public:
2551   bool do_heap_region(HeapRegion* r) {
2552     HeapRegionRemSet* hrrs = r-&gt;rem_set();
2553     size_t occupied = hrrs-&gt;occupied();
2554     _occupied_sum += occupied;
2555 
2556     tty-&gt;print_cr("Printing RSet for region " HR_FORMAT, HR_FORMAT_PARAMS(r));
2557     if (occupied == 0) {
2558       tty-&gt;print_cr("  RSet is empty");
2559     } else {
2560       hrrs-&gt;print();
2561     }
2562     tty-&gt;print_cr("----------");
2563     return false;
2564   }
2565 
2566   PrintRSetsClosure(const char* msg) : _msg(msg), _occupied_sum(0) {
2567     tty-&gt;cr();
2568     tty-&gt;print_cr("========================================");
2569     tty-&gt;print_cr("%s", msg);
2570     tty-&gt;cr();
2571   }
2572 
2573   ~PrintRSetsClosure() {
2574     tty-&gt;print_cr("Occupied Sum: " SIZE_FORMAT, _occupied_sum);
2575     tty-&gt;print_cr("========================================");
2576     tty-&gt;cr();
2577   }
2578 };
2579 
2580 void G1CollectedHeap::print_cset_rsets() {
2581   PrintRSetsClosure cl("Printing CSet RSets");
2582   collection_set_iterate_all(&amp;cl);
2583 }
2584 
2585 void G1CollectedHeap::print_all_rsets() {
2586   PrintRSetsClosure cl("Printing All RSets");;
2587   heap_region_iterate(&amp;cl);
2588 }
2589 #endif // PRODUCT
2590 
2591 bool G1CollectedHeap::print_location(outputStream* st, void* addr) const {
2592   return BlockLocationPrinter&lt;G1CollectedHeap&gt;::print_location(st, addr);
2593 }
2594 
2595 G1HeapSummary G1CollectedHeap::create_g1_heap_summary() {
2596 
2597   size_t eden_used_bytes = _eden.used_bytes();
2598   size_t survivor_used_bytes = _survivor.used_bytes();
2599   size_t heap_used = Heap_lock-&gt;owned_by_self() ? used() : used_unlocked();
2600 
2601   size_t eden_capacity_bytes =
2602     (policy()-&gt;young_list_target_length() * HeapRegion::GrainBytes) - survivor_used_bytes;
2603 
2604   VirtualSpaceSummary heap_summary = create_heap_space_summary();
2605   return G1HeapSummary(heap_summary, heap_used, eden_used_bytes,
2606                        eden_capacity_bytes, survivor_used_bytes, num_regions());
2607 }
2608 
2609 G1EvacSummary G1CollectedHeap::create_g1_evac_summary(G1EvacStats* stats) {
2610   return G1EvacSummary(stats-&gt;allocated(), stats-&gt;wasted(), stats-&gt;undo_wasted(),
2611                        stats-&gt;unused(), stats-&gt;used(), stats-&gt;region_end_waste(),
2612                        stats-&gt;regions_filled(), stats-&gt;direct_allocated(),
2613                        stats-&gt;failure_used(), stats-&gt;failure_waste());
2614 }
2615 
2616 void G1CollectedHeap::trace_heap(GCWhen::Type when, const GCTracer* gc_tracer) {
2617   const G1HeapSummary&amp; heap_summary = create_g1_heap_summary();
2618   gc_tracer-&gt;report_gc_heap_summary(when, heap_summary);
2619 
2620   const MetaspaceSummary&amp; metaspace_summary = create_metaspace_summary();
2621   gc_tracer-&gt;report_metaspace_summary(when, metaspace_summary);
2622 }
2623 
2624 void G1CollectedHeap::gc_prologue(bool full) {
2625   assert(InlineCacheBuffer::is_empty(), "should have cleaned up ICBuffer");
2626 
2627   // This summary needs to be printed before incrementing total collections.
2628   rem_set()-&gt;print_periodic_summary_info("Before GC RS summary", total_collections());
2629 
2630   // Update common counters.
2631   increment_total_collections(full /* full gc */);
2632   if (full || collector_state()-&gt;in_concurrent_start_gc()) {
2633     increment_old_marking_cycles_started();
2634   }
2635 
2636   // Fill TLAB's and such
2637   {
2638     Ticks start = Ticks::now();
2639     ensure_parsability(true);
2640     Tickspan dt = Ticks::now() - start;
2641     phase_times()-&gt;record_prepare_tlab_time_ms(dt.seconds() * MILLIUNITS);
2642   }
2643 
2644   if (!full) {
2645     // Flush dirty card queues to qset, so later phases don't need to account
2646     // for partially filled per-thread queues and such.  Not needed for full
2647     // collections, which ignore those logs.
2648     Ticks start = Ticks::now();
2649     G1BarrierSet::dirty_card_queue_set().concatenate_logs();
2650     Tickspan dt = Ticks::now() - start;
2651     phase_times()-&gt;record_concatenate_dirty_card_logs_time_ms(dt.seconds() * MILLIUNITS);
2652   }
2653 }
2654 
2655 void G1CollectedHeap::gc_epilogue(bool full) {
2656   // Update common counters.
2657   if (full) {
2658     // Update the number of full collections that have been completed.
2659     increment_old_marking_cycles_completed(false /* concurrent */, true /* liveness_completed */);
2660   }
2661 
2662   // We are at the end of the GC. Total collections has already been increased.
2663   rem_set()-&gt;print_periodic_summary_info("After GC RS summary", total_collections() - 1);
2664 
2665   // FIXME: what is this about?
2666   // I'm ignoring the "fill_newgen()" call if "alloc_event_enabled"
2667   // is set.
2668 #if COMPILER2_OR_JVMCI
2669   assert(DerivedPointerTable::is_empty(), "derived pointer present");
2670 #endif
2671 
2672   double start = os::elapsedTime();
2673   resize_all_tlabs();
2674   phase_times()-&gt;record_resize_tlab_time_ms((os::elapsedTime() - start) * 1000.0);
2675 
2676   MemoryService::track_memory_usage();
2677   // We have just completed a GC. Update the soft reference
2678   // policy with the new heap occupancy
2679   Universe::update_heap_info_at_gc();
2680 
2681   // Print NUMA statistics.
2682   _numa-&gt;print_statistics();
2683 
2684   _collection_pause_end = Ticks::now();
2685 }
2686 
2687 void G1CollectedHeap::verify_numa_regions(const char* desc) {
2688   LogTarget(Trace, gc, heap, verify) lt;
2689 
2690   if (lt.is_enabled()) {
2691     LogStream ls(lt);
2692     // Iterate all heap regions to print matching between preferred numa id and actual numa id.
2693     G1NodeIndexCheckClosure cl(desc, _numa, &amp;ls);
2694     heap_region_iterate(&amp;cl);
2695   }
2696 }
2697 
2698 HeapWord* G1CollectedHeap::do_collection_pause(size_t word_size,
2699                                                uint gc_count_before,
2700                                                bool* succeeded,
2701                                                GCCause::Cause gc_cause) {
2702   assert_heap_not_locked_and_not_at_safepoint();
2703   VM_G1CollectForAllocation op(word_size,
2704                                gc_count_before,
2705                                gc_cause,
2706                                policy()-&gt;max_pause_time_ms());
2707   VMThread::execute(&amp;op);
2708 
2709   HeapWord* result = op.result();
2710   bool ret_succeeded = op.prologue_succeeded() &amp;&amp; op.gc_succeeded();
2711   assert(result == NULL || ret_succeeded,
2712          "the result should be NULL if the VM did not succeed");
2713   *succeeded = ret_succeeded;
2714 
2715   assert_heap_not_locked();
2716   return result;
2717 }
2718 
2719 void G1CollectedHeap::do_concurrent_mark() {
2720   MutexLocker x(CGC_lock, Mutex::_no_safepoint_check_flag);
2721   if (!_cm_thread-&gt;in_progress()) {
2722     _cm_thread-&gt;set_started();
2723     CGC_lock-&gt;notify();
2724   }
2725 }
2726 
2727 bool G1CollectedHeap::is_potential_eager_reclaim_candidate(HeapRegion* r) const {
2728   // We don't nominate objects with many remembered set entries, on
2729   // the assumption that such objects are likely still live.
2730   HeapRegionRemSet* rem_set = r-&gt;rem_set();
2731 
2732   return G1EagerReclaimHumongousObjectsWithStaleRefs ?
2733          rem_set-&gt;occupancy_less_or_equal_than(G1RSetSparseRegionEntries) :
2734          G1EagerReclaimHumongousObjects &amp;&amp; rem_set-&gt;is_empty();
2735 }
2736 
2737 #ifndef PRODUCT
2738 void G1CollectedHeap::verify_region_attr_remset_update() {
2739   class VerifyRegionAttrRemSet : public HeapRegionClosure {
2740   public:
2741     virtual bool do_heap_region(HeapRegion* r) {
2742       G1CollectedHeap* g1h = G1CollectedHeap::heap();
2743       bool const needs_remset_update = g1h-&gt;region_attr(r-&gt;bottom()).needs_remset_update();
2744       assert(r-&gt;rem_set()-&gt;is_tracked() == needs_remset_update,
2745              "Region %u remset tracking status (%s) different to region attribute (%s)",
2746              r-&gt;hrm_index(), BOOL_TO_STR(r-&gt;rem_set()-&gt;is_tracked()), BOOL_TO_STR(needs_remset_update));
2747       return false;
2748     }
2749   } cl;
2750   heap_region_iterate(&amp;cl);
2751 }
2752 #endif
2753 
2754 class VerifyRegionRemSetClosure : public HeapRegionClosure {
2755   public:
2756     bool do_heap_region(HeapRegion* hr) {
2757       if (!hr-&gt;is_archive() &amp;&amp; !hr-&gt;is_continues_humongous()) {
2758         hr-&gt;verify_rem_set();
2759       }
2760       return false;
2761     }
2762 };
2763 
2764 uint G1CollectedHeap::num_task_queues() const {
2765   return _task_queues-&gt;size();
2766 }
2767 
2768 #if TASKQUEUE_STATS
2769 void G1CollectedHeap::print_taskqueue_stats_hdr(outputStream* const st) {
2770   st-&gt;print_raw_cr("GC Task Stats");
2771   st-&gt;print_raw("thr "); TaskQueueStats::print_header(1, st); st-&gt;cr();
2772   st-&gt;print_raw("--- "); TaskQueueStats::print_header(2, st); st-&gt;cr();
2773 }
2774 
2775 void G1CollectedHeap::print_taskqueue_stats() const {
2776   if (!log_is_enabled(Trace, gc, task, stats)) {
2777     return;
2778   }
2779   Log(gc, task, stats) log;
2780   ResourceMark rm;
2781   LogStream ls(log.trace());
2782   outputStream* st = &amp;ls;
2783 
2784   print_taskqueue_stats_hdr(st);
2785 
2786   TaskQueueStats totals;
2787   const uint n = num_task_queues();
2788   for (uint i = 0; i &lt; n; ++i) {
2789     st-&gt;print("%3u ", i); task_queue(i)-&gt;stats.print(st); st-&gt;cr();
2790     totals += task_queue(i)-&gt;stats;
2791   }
2792   st-&gt;print_raw("tot "); totals.print(st); st-&gt;cr();
2793 
2794   DEBUG_ONLY(totals.verify());
2795 }
2796 
2797 void G1CollectedHeap::reset_taskqueue_stats() {
2798   const uint n = num_task_queues();
2799   for (uint i = 0; i &lt; n; ++i) {
2800     task_queue(i)-&gt;stats.reset();
2801   }
2802 }
2803 #endif // TASKQUEUE_STATS
2804 
2805 void G1CollectedHeap::wait_for_root_region_scanning() {
2806   double scan_wait_start = os::elapsedTime();
2807   // We have to wait until the CM threads finish scanning the
2808   // root regions as it's the only way to ensure that all the
2809   // objects on them have been correctly scanned before we start
2810   // moving them during the GC.
2811   bool waited = _cm-&gt;root_regions()-&gt;wait_until_scan_finished();
2812   double wait_time_ms = 0.0;
2813   if (waited) {
2814     double scan_wait_end = os::elapsedTime();
2815     wait_time_ms = (scan_wait_end - scan_wait_start) * 1000.0;
2816   }
2817   phase_times()-&gt;record_root_region_scan_wait_time(wait_time_ms);
2818 }
2819 
2820 class G1PrintCollectionSetClosure : public HeapRegionClosure {
2821 private:
2822   G1HRPrinter* _hr_printer;
2823 public:
2824   G1PrintCollectionSetClosure(G1HRPrinter* hr_printer) : HeapRegionClosure(), _hr_printer(hr_printer) { }
2825 
2826   virtual bool do_heap_region(HeapRegion* r) {
2827     _hr_printer-&gt;cset(r);
2828     return false;
2829   }
2830 };
2831 
2832 void G1CollectedHeap::start_new_collection_set() {
2833   double start = os::elapsedTime();
2834 
2835   collection_set()-&gt;start_incremental_building();
2836 
2837   clear_region_attr();
2838 
2839   guarantee(_eden.length() == 0, "eden should have been cleared");
2840   policy()-&gt;transfer_survivors_to_cset(survivor());
2841 
2842   // We redo the verification but now wrt to the new CSet which
2843   // has just got initialized after the previous CSet was freed.
2844   _cm-&gt;verify_no_collection_set_oops();
2845 
2846   phase_times()-&gt;record_start_new_cset_time_ms((os::elapsedTime() - start) * 1000.0);
2847 }
2848 
2849 void G1CollectedHeap::calculate_collection_set(G1EvacuationInfo&amp; evacuation_info, double target_pause_time_ms) {
2850 
2851   _collection_set.finalize_initial_collection_set(target_pause_time_ms, &amp;_survivor);
2852   evacuation_info.set_collectionset_regions(collection_set()-&gt;region_length() +
2853                                             collection_set()-&gt;optional_region_length());
2854 
2855   _cm-&gt;verify_no_collection_set_oops();
2856 
2857   if (_hr_printer.is_active()) {
2858     G1PrintCollectionSetClosure cl(&amp;_hr_printer);
2859     _collection_set.iterate(&amp;cl);
2860     _collection_set.iterate_optional(&amp;cl);
2861   }
2862 }
2863 
2864 G1HeapVerifier::G1VerifyType G1CollectedHeap::young_collection_verify_type() const {
2865   if (collector_state()-&gt;in_concurrent_start_gc()) {
2866     return G1HeapVerifier::G1VerifyConcurrentStart;
2867   } else if (collector_state()-&gt;in_young_only_phase()) {
2868     return G1HeapVerifier::G1VerifyYoungNormal;
2869   } else {
2870     return G1HeapVerifier::G1VerifyMixed;
2871   }
2872 }
2873 
2874 void G1CollectedHeap::verify_before_young_collection(G1HeapVerifier::G1VerifyType type) {
2875   if (VerifyRememberedSets) {
2876     log_info(gc, verify)("[Verifying RemSets before GC]");
2877     VerifyRegionRemSetClosure v_cl;
2878     heap_region_iterate(&amp;v_cl);
2879   }
2880   _verifier-&gt;verify_before_gc(type);
2881   _verifier-&gt;check_bitmaps("GC Start");
2882   verify_numa_regions("GC Start");
2883 }
2884 
2885 void G1CollectedHeap::verify_after_young_collection(G1HeapVerifier::G1VerifyType type) {
2886   if (VerifyRememberedSets) {
2887     log_info(gc, verify)("[Verifying RemSets after GC]");
2888     VerifyRegionRemSetClosure v_cl;
2889     heap_region_iterate(&amp;v_cl);
2890   }
2891   _verifier-&gt;verify_after_gc(type);
2892   _verifier-&gt;check_bitmaps("GC End");
2893   verify_numa_regions("GC End");
2894 }
2895 
2896 void G1CollectedHeap::expand_heap_after_young_collection(){
2897   size_t expand_bytes = _heap_sizing_policy-&gt;young_collection_expansion_amount();
2898   if (expand_bytes &gt; 0) {
2899     // No need for an ergo logging here,
2900     // expansion_amount() does this when it returns a value &gt; 0.
2901     double expand_ms;
2902     if (!expand(expand_bytes, _workers, &amp;expand_ms)) {
2903       // We failed to expand the heap. Cannot do anything about it.
2904     }
2905     phase_times()-&gt;record_expand_heap_time(expand_ms);
2906   }
2907 }
2908 
2909 const char* G1CollectedHeap::young_gc_name() const {
2910   if (collector_state()-&gt;in_concurrent_start_gc()) {
2911     return "Pause Young (Concurrent Start)";
2912   } else if (collector_state()-&gt;in_young_only_phase()) {
2913     if (collector_state()-&gt;in_young_gc_before_mixed()) {
2914       return "Pause Young (Prepare Mixed)";
2915     } else {
2916       return "Pause Young (Normal)";
2917     }
2918   } else {
2919     return "Pause Young (Mixed)";
2920   }
2921 }
2922 
2923 bool G1CollectedHeap::do_collection_pause_at_safepoint(double target_pause_time_ms) {
2924   assert_at_safepoint_on_vm_thread();
2925   guarantee(!is_gc_active(), "collection is not reentrant");
2926 
2927   if (GCLocker::check_active_before_gc()) {
2928     return false;
2929   }
2930 
2931   do_collection_pause_at_safepoint_helper(target_pause_time_ms);
2932   if (should_upgrade_to_full_gc(gc_cause())) {
2933     log_info(gc, ergo)("Attempting maximally compacting collection");
2934     bool result = do_full_collection(false /* explicit gc */,
2935                                      true /* clear_all_soft_refs */);
2936     // do_full_collection only fails if blocked by GC locker, but
2937     // we've already checked for that above.
2938     assert(result, "invariant");
2939   }
2940   return true;
2941 }
2942 
2943 void G1CollectedHeap::do_collection_pause_at_safepoint_helper(double target_pause_time_ms) {
2944   GCIdMark gc_id_mark;
2945 
2946   SvcGCMarker sgcm(SvcGCMarker::MINOR);
2947   ResourceMark rm;
2948 
2949   policy()-&gt;note_gc_start();
2950 
2951   _gc_timer_stw-&gt;register_gc_start();
2952   _gc_tracer_stw-&gt;report_gc_start(gc_cause(), _gc_timer_stw-&gt;gc_start());
2953 
2954   wait_for_root_region_scanning();
2955 
2956   print_heap_before_gc();
2957   print_heap_regions();
2958   trace_heap_before_gc(_gc_tracer_stw);
2959 
2960   _verifier-&gt;verify_region_sets_optional();
2961   _verifier-&gt;verify_dirty_young_regions();
2962 
2963   // We should not be doing concurrent start unless the concurrent mark thread is running
2964   if (!_cm_thread-&gt;should_terminate()) {
2965     // This call will decide whether this pause is a concurrent start
2966     // pause. If it is, in_concurrent_start_gc() will return true
2967     // for the duration of this pause.
2968     policy()-&gt;decide_on_conc_mark_initiation();
2969   }
2970 
2971   // We do not allow concurrent start to be piggy-backed on a mixed GC.
2972   assert(!collector_state()-&gt;in_concurrent_start_gc() ||
2973          collector_state()-&gt;in_young_only_phase(), "sanity");
2974   // We also do not allow mixed GCs during marking.
2975   assert(!collector_state()-&gt;mark_or_rebuild_in_progress() || collector_state()-&gt;in_young_only_phase(), "sanity");
2976 
2977   // Record whether this pause is a concurrent start. When the current
2978   // thread has completed its logging output and it's safe to signal
2979   // the CM thread, the flag's value in the policy has been reset.
2980   bool should_start_conc_mark = collector_state()-&gt;in_concurrent_start_gc();
2981   if (should_start_conc_mark) {
2982     _cm-&gt;gc_tracer_cm()-&gt;set_gc_cause(gc_cause());
2983   }
2984 
2985   // Inner scope for scope based logging, timers, and stats collection
2986   {
2987     G1EvacuationInfo evacuation_info;
2988 
2989     _gc_tracer_stw-&gt;report_yc_type(collector_state()-&gt;yc_type());
2990 
2991     GCTraceCPUTime tcpu;
2992 
2993     GCTraceTime(Info, gc) tm(young_gc_name(), NULL, gc_cause(), true);
2994 
2995     uint active_workers = WorkerPolicy::calc_active_workers(workers()-&gt;total_workers(),
2996                                                             workers()-&gt;active_workers(),
2997                                                             Threads::number_of_non_daemon_threads());
2998     active_workers = workers()-&gt;update_active_workers(active_workers);
2999     log_info(gc,task)("Using %u workers of %u for evacuation", active_workers, workers()-&gt;total_workers());
3000 
3001     G1MonitoringScope ms(g1mm(),
3002                          false /* full_gc */,
3003                          collector_state()-&gt;yc_type() == Mixed /* all_memory_pools_affected */);
3004 
3005     G1HeapTransition heap_transition(this);
3006 
3007     {
3008       IsGCActiveMark x;
3009 
3010       gc_prologue(false);
3011 
3012       G1HeapVerifier::G1VerifyType verify_type = young_collection_verify_type();
3013       verify_before_young_collection(verify_type);
3014 
3015       {
3016         // The elapsed time induced by the start time below deliberately elides
3017         // the possible verification above.
3018         double sample_start_time_sec = os::elapsedTime();
3019 
3020         // Please see comment in g1CollectedHeap.hpp and
3021         // G1CollectedHeap::ref_processing_init() to see how
3022         // reference processing currently works in G1.
3023         _ref_processor_stw-&gt;enable_discovery();
3024 
3025         // We want to temporarily turn off discovery by the
3026         // CM ref processor, if necessary, and turn it back on
3027         // on again later if we do. Using a scoped
3028         // NoRefDiscovery object will do this.
3029         NoRefDiscovery no_cm_discovery(_ref_processor_cm);
3030 
3031         policy()-&gt;record_collection_pause_start(sample_start_time_sec);
3032 
3033         // Forget the current allocation region (we might even choose it to be part
3034         // of the collection set!).
3035         _allocator-&gt;release_mutator_alloc_regions();
3036 
3037         calculate_collection_set(evacuation_info, target_pause_time_ms);
3038 
3039         G1RedirtyCardsQueueSet rdcqs(G1BarrierSet::dirty_card_queue_set().allocator());
3040         G1ParScanThreadStateSet per_thread_states(this,
3041                                                   &amp;rdcqs,
3042                                                   workers()-&gt;active_workers(),
3043                                                   collection_set()-&gt;young_region_length(),
3044                                                   collection_set()-&gt;optional_region_length());
3045         pre_evacuate_collection_set(evacuation_info, &amp;per_thread_states);
3046 
3047         // Actually do the work...
3048         evacuate_initial_collection_set(&amp;per_thread_states);
3049 
3050         if (_collection_set.optional_region_length() != 0) {
3051           evacuate_optional_collection_set(&amp;per_thread_states);
3052         }
3053         post_evacuate_collection_set(evacuation_info, &amp;rdcqs, &amp;per_thread_states);
3054 
3055         start_new_collection_set();
3056 
3057         _survivor_evac_stats.adjust_desired_plab_sz();
3058         _old_evac_stats.adjust_desired_plab_sz();
3059 
3060         if (should_start_conc_mark) {
3061           // We have to do this before we notify the CM threads that
3062           // they can start working to make sure that all the
3063           // appropriate initialization is done on the CM object.
3064           concurrent_mark()-&gt;post_concurrent_start();
3065           // Note that we don't actually trigger the CM thread at
3066           // this point. We do that later when we're sure that
3067           // the current thread has completed its logging output.
3068         }
3069 
3070         allocate_dummy_regions();
3071 
3072         _allocator-&gt;init_mutator_alloc_regions();
3073 
3074         expand_heap_after_young_collection();
3075 
3076         double sample_end_time_sec = os::elapsedTime();
3077         double pause_time_ms = (sample_end_time_sec - sample_start_time_sec) * MILLIUNITS;
3078         policy()-&gt;record_collection_pause_end(pause_time_ms);
3079       }
3080 
3081       verify_after_young_collection(verify_type);
3082 
3083       gc_epilogue(false);
3084     }
3085 
3086     // Print the remainder of the GC log output.
3087     if (evacuation_failed()) {
3088       log_info(gc)("To-space exhausted");
3089     }
3090 
3091     policy()-&gt;print_phases();
3092     heap_transition.print();
3093 
3094     _hrm-&gt;verify_optional();
3095     _verifier-&gt;verify_region_sets_optional();
3096 
3097     TASKQUEUE_STATS_ONLY(print_taskqueue_stats());
3098     TASKQUEUE_STATS_ONLY(reset_taskqueue_stats());
3099 
3100     print_heap_after_gc();
3101     print_heap_regions();
3102     trace_heap_after_gc(_gc_tracer_stw);
3103 
3104     // We must call G1MonitoringSupport::update_sizes() in the same scoping level
3105     // as an active TraceMemoryManagerStats object (i.e. before the destructor for the
3106     // TraceMemoryManagerStats is called) so that the G1 memory pools are updated
3107     // before any GC notifications are raised.
3108     g1mm()-&gt;update_sizes();
3109 
3110     _gc_tracer_stw-&gt;report_evacuation_info(&amp;evacuation_info);
3111     _gc_tracer_stw-&gt;report_tenuring_threshold(_policy-&gt;tenuring_threshold());
3112     _gc_timer_stw-&gt;register_gc_end();
3113     _gc_tracer_stw-&gt;report_gc_end(_gc_timer_stw-&gt;gc_end(), _gc_timer_stw-&gt;time_partitions());
3114   }
3115   // It should now be safe to tell the concurrent mark thread to start
3116   // without its logging output interfering with the logging output
3117   // that came from the pause.
3118 
3119   if (should_start_conc_mark) {
3120     // CAUTION: after the doConcurrentMark() call below, the concurrent marking
3121     // thread(s) could be running concurrently with us. Make sure that anything
3122     // after this point does not assume that we are the only GC thread running.
3123     // Note: of course, the actual marking work will not start until the safepoint
3124     // itself is released in SuspendibleThreadSet::desynchronize().
3125     do_concurrent_mark();
3126     ConcurrentGCBreakpoints::notify_idle_to_active();
3127   }
3128 }
3129 
3130 void G1CollectedHeap::remove_self_forwarding_pointers(G1RedirtyCardsQueueSet* rdcqs) {
3131   G1ParRemoveSelfForwardPtrsTask rsfp_task(rdcqs);
3132   workers()-&gt;run_task(&amp;rsfp_task);
3133 }
3134 
3135 void G1CollectedHeap::restore_after_evac_failure(G1RedirtyCardsQueueSet* rdcqs) {
3136   double remove_self_forwards_start = os::elapsedTime();
3137 
3138   remove_self_forwarding_pointers(rdcqs);
3139   _preserved_marks_set.restore(workers());
3140 
3141   phase_times()-&gt;record_evac_fail_remove_self_forwards((os::elapsedTime() - remove_self_forwards_start) * 1000.0);
3142 }
3143 
3144 void G1CollectedHeap::preserve_mark_during_evac_failure(uint worker_id, oop obj, markWord m) {
3145   if (!_evacuation_failed) {
3146     _evacuation_failed = true;
3147   }
3148 
3149   _evacuation_failed_info_array[worker_id].register_copy_failure(obj-&gt;size());
3150   _preserved_marks_set.get(worker_id)-&gt;push_if_necessary(obj, m);
3151 }
3152 
3153 bool G1ParEvacuateFollowersClosure::offer_termination() {
3154   EventGCPhaseParallel event;
3155   G1ParScanThreadState* const pss = par_scan_state();
3156   start_term_time();
3157   const bool res = terminator()-&gt;offer_termination();
3158   end_term_time();
3159   event.commit(GCId::current(), pss-&gt;worker_id(), G1GCPhaseTimes::phase_name(G1GCPhaseTimes::Termination));
3160   return res;
3161 }
3162 
3163 void G1ParEvacuateFollowersClosure::do_void() {
3164   EventGCPhaseParallel event;
3165   G1ParScanThreadState* const pss = par_scan_state();
3166   pss-&gt;trim_queue();
3167   event.commit(GCId::current(), pss-&gt;worker_id(), G1GCPhaseTimes::phase_name(_phase));
3168   do {
3169     EventGCPhaseParallel event;
3170     pss-&gt;steal_and_trim_queue(queues());
3171     event.commit(GCId::current(), pss-&gt;worker_id(), G1GCPhaseTimes::phase_name(_phase));
3172   } while (!offer_termination());
3173 }
3174 
3175 void G1CollectedHeap::complete_cleaning(BoolObjectClosure* is_alive,
3176                                         bool class_unloading_occurred) {
3177   uint num_workers = workers()-&gt;active_workers();
3178   G1ParallelCleaningTask unlink_task(is_alive, num_workers, class_unloading_occurred, false);
3179   workers()-&gt;run_task(&amp;unlink_task);
3180 }
3181 
3182 // Clean string dedup data structures.
3183 // Ideally we would prefer to use a StringDedupCleaningTask here, but we want to
3184 // record the durations of the phases. Hence the almost-copy.
3185 class G1StringDedupCleaningTask : public AbstractGangTask {
3186   BoolObjectClosure* _is_alive;
3187   OopClosure* _keep_alive;
3188   G1GCPhaseTimes* _phase_times;
3189 
3190 public:
3191   G1StringDedupCleaningTask(BoolObjectClosure* is_alive,
3192                             OopClosure* keep_alive,
3193                             G1GCPhaseTimes* phase_times) :
3194     AbstractGangTask("Partial Cleaning Task"),
3195     _is_alive(is_alive),
3196     _keep_alive(keep_alive),
3197     _phase_times(phase_times)
3198   {
3199     assert(G1StringDedup::is_enabled(), "String deduplication disabled.");
3200     StringDedup::gc_prologue(true);
3201   }
3202 
3203   ~G1StringDedupCleaningTask() {
3204     StringDedup::gc_epilogue();
3205   }
3206 
3207   void work(uint worker_id) {
3208     StringDedupUnlinkOrOopsDoClosure cl(_is_alive, _keep_alive);
3209     {
3210       G1GCParPhaseTimesTracker x(_phase_times, G1GCPhaseTimes::StringDedupQueueFixup, worker_id);
3211       StringDedupQueue::unlink_or_oops_do(&amp;cl);
3212     }
3213     {
3214       G1GCParPhaseTimesTracker x(_phase_times, G1GCPhaseTimes::StringDedupTableFixup, worker_id);
3215       StringDedupTable::unlink_or_oops_do(&amp;cl, worker_id);
3216     }
3217   }
3218 };
3219 
3220 void G1CollectedHeap::string_dedup_cleaning(BoolObjectClosure* is_alive,
3221                                             OopClosure* keep_alive,
3222                                             G1GCPhaseTimes* phase_times) {
3223   G1StringDedupCleaningTask cl(is_alive, keep_alive, phase_times);
3224   workers()-&gt;run_task(&amp;cl);
3225 }
3226 
3227 class G1RedirtyLoggedCardsTask : public AbstractGangTask {
3228  private:
3229   G1RedirtyCardsQueueSet* _qset;
3230   G1CollectedHeap* _g1h;
3231   BufferNode* volatile _nodes;
3232 
3233   void par_apply(RedirtyLoggedCardTableEntryClosure* cl, uint worker_id) {
3234     size_t buffer_size = _qset-&gt;buffer_size();
3235     BufferNode* next = Atomic::load(&amp;_nodes);
3236     while (next != NULL) {
3237       BufferNode* node = next;
3238       next = Atomic::cmpxchg(&amp;_nodes, node, node-&gt;next());
3239       if (next == node) {
3240         cl-&gt;apply_to_buffer(node, buffer_size, worker_id);
3241         next = node-&gt;next();
3242       }
3243     }
3244   }
3245 
3246  public:
3247   G1RedirtyLoggedCardsTask(G1RedirtyCardsQueueSet* qset, G1CollectedHeap* g1h) :
3248     AbstractGangTask("Redirty Cards"),
3249     _qset(qset), _g1h(g1h), _nodes(qset-&gt;all_completed_buffers()) { }
3250 
3251   virtual void work(uint worker_id) {
3252     G1GCPhaseTimes* p = _g1h-&gt;phase_times();
3253     G1GCParPhaseTimesTracker x(p, G1GCPhaseTimes::RedirtyCards, worker_id);
3254 
3255     RedirtyLoggedCardTableEntryClosure cl(_g1h);
3256     par_apply(&amp;cl, worker_id);
3257 
3258     p-&gt;record_thread_work_item(G1GCPhaseTimes::RedirtyCards, worker_id, cl.num_dirtied());
3259   }
3260 };
3261 
3262 void G1CollectedHeap::redirty_logged_cards(G1RedirtyCardsQueueSet* rdcqs) {
3263   double redirty_logged_cards_start = os::elapsedTime();
3264 
3265   G1RedirtyLoggedCardsTask redirty_task(rdcqs, this);
3266   workers()-&gt;run_task(&amp;redirty_task);
3267 
3268   G1DirtyCardQueueSet&amp; dcq = G1BarrierSet::dirty_card_queue_set();
3269   dcq.merge_bufferlists(rdcqs);
3270 
3271   phase_times()-&gt;record_redirty_logged_cards_time_ms((os::elapsedTime() - redirty_logged_cards_start) * 1000.0);
3272 }
3273 
3274 // Weak Reference Processing support
3275 
3276 bool G1STWIsAliveClosure::do_object_b(oop p) {
3277   // An object is reachable if it is outside the collection set,
3278   // or is inside and copied.
3279   return !_g1h-&gt;is_in_cset(p) || p-&gt;is_forwarded();
3280 }
3281 
3282 bool G1STWSubjectToDiscoveryClosure::do_object_b(oop obj) {
3283   assert(obj != NULL, "must not be NULL");
3284   assert(_g1h-&gt;is_in_reserved(obj), "Trying to discover obj " PTR_FORMAT " not in heap", p2i(obj));
3285   // The areas the CM and STW ref processor manage must be disjoint. The is_in_cset() below
3286   // may falsely indicate that this is not the case here: however the collection set only
3287   // contains old regions when concurrent mark is not running.
3288   return _g1h-&gt;is_in_cset(obj) || _g1h-&gt;heap_region_containing(obj)-&gt;is_survivor();
3289 }
3290 
3291 // Non Copying Keep Alive closure
3292 class G1KeepAliveClosure: public OopClosure {
3293   G1CollectedHeap*_g1h;
3294 public:
3295   G1KeepAliveClosure(G1CollectedHeap* g1h) :_g1h(g1h) {}
3296   void do_oop(narrowOop* p) { guarantee(false, "Not needed"); }
3297   void do_oop(oop* p) {
3298     oop obj = *p;
3299     assert(obj != NULL, "the caller should have filtered out NULL values");
3300 
3301     const G1HeapRegionAttr region_attr =_g1h-&gt;region_attr(obj);
3302     if (!region_attr.is_in_cset_or_humongous()) {
3303       return;
3304     }
3305     if (region_attr.is_in_cset()) {
3306       assert( obj-&gt;is_forwarded(), "invariant" );
3307       *p = obj-&gt;forwardee();
3308     } else {
3309       assert(!obj-&gt;is_forwarded(), "invariant" );
3310       assert(region_attr.is_humongous(),
3311              "Only allowed G1HeapRegionAttr state is IsHumongous, but is %d", region_attr.type());
3312      _g1h-&gt;set_humongous_is_live(obj);
3313     }
3314   }
3315 };
3316 
3317 // Copying Keep Alive closure - can be called from both
3318 // serial and parallel code as long as different worker
3319 // threads utilize different G1ParScanThreadState instances
3320 // and different queues.
3321 
3322 class G1CopyingKeepAliveClosure: public OopClosure {
3323   G1CollectedHeap*         _g1h;
3324   G1ParScanThreadState*    _par_scan_state;
3325 
3326 public:
3327   G1CopyingKeepAliveClosure(G1CollectedHeap* g1h,
3328                             G1ParScanThreadState* pss):
3329     _g1h(g1h),
3330     _par_scan_state(pss)
3331   {}
3332 
3333   virtual void do_oop(narrowOop* p) { do_oop_work(p); }
3334   virtual void do_oop(      oop* p) { do_oop_work(p); }
3335 
3336   template &lt;class T&gt; void do_oop_work(T* p) {
3337     oop obj = RawAccess&lt;&gt;::oop_load(p);
3338 
3339     if (_g1h-&gt;is_in_cset_or_humongous(obj)) {
3340       // If the referent object has been forwarded (either copied
3341       // to a new location or to itself in the event of an
3342       // evacuation failure) then we need to update the reference
3343       // field and, if both reference and referent are in the G1
3344       // heap, update the RSet for the referent.
3345       //
3346       // If the referent has not been forwarded then we have to keep
3347       // it alive by policy. Therefore we have copy the referent.
3348       //
3349       // When the queue is drained (after each phase of reference processing)
3350       // the object and it's followers will be copied, the reference field set
3351       // to point to the new location, and the RSet updated.
3352       _par_scan_state-&gt;push_on_queue(ScannerTask(p));
3353     }
3354   }
3355 };
3356 
3357 // Serial drain queue closure. Called as the 'complete_gc'
3358 // closure for each discovered list in some of the
3359 // reference processing phases.
3360 
3361 class G1STWDrainQueueClosure: public VoidClosure {
3362 protected:
3363   G1CollectedHeap* _g1h;
3364   G1ParScanThreadState* _par_scan_state;
3365 
3366   G1ParScanThreadState*   par_scan_state() { return _par_scan_state; }
3367 
3368 public:
3369   G1STWDrainQueueClosure(G1CollectedHeap* g1h, G1ParScanThreadState* pss) :
3370     _g1h(g1h),
3371     _par_scan_state(pss)
3372   { }
3373 
3374   void do_void() {
3375     G1ParScanThreadState* const pss = par_scan_state();
3376     pss-&gt;trim_queue();
3377   }
3378 };
3379 
3380 // Parallel Reference Processing closures
3381 
3382 // Implementation of AbstractRefProcTaskExecutor for parallel reference
3383 // processing during G1 evacuation pauses.
3384 
3385 class G1STWRefProcTaskExecutor: public AbstractRefProcTaskExecutor {
3386 private:
3387   G1CollectedHeap*          _g1h;
3388   G1ParScanThreadStateSet*  _pss;
3389   G1ScannerTasksQueueSet*   _queues;
3390   WorkGang*                 _workers;
3391 
3392 public:
3393   G1STWRefProcTaskExecutor(G1CollectedHeap* g1h,
3394                            G1ParScanThreadStateSet* per_thread_states,
3395                            WorkGang* workers,
3396                            G1ScannerTasksQueueSet *task_queues) :
3397     _g1h(g1h),
3398     _pss(per_thread_states),
3399     _queues(task_queues),
3400     _workers(workers)
3401   {
3402     g1h-&gt;ref_processor_stw()-&gt;set_active_mt_degree(workers-&gt;active_workers());
3403   }
3404 
3405   // Executes the given task using concurrent marking worker threads.
3406   virtual void execute(ProcessTask&amp; task, uint ergo_workers);
3407 };
3408 
3409 // Gang task for possibly parallel reference processing
3410 
3411 class G1STWRefProcTaskProxy: public AbstractGangTask {
3412   typedef AbstractRefProcTaskExecutor::ProcessTask ProcessTask;
3413   ProcessTask&amp;     _proc_task;
3414   G1CollectedHeap* _g1h;
3415   G1ParScanThreadStateSet* _pss;
3416   G1ScannerTasksQueueSet* _task_queues;
3417   TaskTerminator* _terminator;
3418 
3419 public:
3420   G1STWRefProcTaskProxy(ProcessTask&amp; proc_task,
3421                         G1CollectedHeap* g1h,
3422                         G1ParScanThreadStateSet* per_thread_states,
3423                         G1ScannerTasksQueueSet *task_queues,
3424                         TaskTerminator* terminator) :
3425     AbstractGangTask("Process reference objects in parallel"),
3426     _proc_task(proc_task),
3427     _g1h(g1h),
3428     _pss(per_thread_states),
3429     _task_queues(task_queues),
3430     _terminator(terminator)
3431   {}
3432 
3433   virtual void work(uint worker_id) {
3434     // The reference processing task executed by a single worker.
3435     ResourceMark rm;
3436 
3437     G1STWIsAliveClosure is_alive(_g1h);
3438 
3439     G1ParScanThreadState* pss = _pss-&gt;state_for_worker(worker_id);
3440     pss-&gt;set_ref_discoverer(NULL);
3441 
3442     // Keep alive closure.
3443     G1CopyingKeepAliveClosure keep_alive(_g1h, pss);
3444 
3445     // Complete GC closure
3446     G1ParEvacuateFollowersClosure drain_queue(_g1h, pss, _task_queues, _terminator, G1GCPhaseTimes::ObjCopy);
3447 
3448     // Call the reference processing task's work routine.
3449     _proc_task.work(worker_id, is_alive, keep_alive, drain_queue);
3450 
3451     // Note we cannot assert that the refs array is empty here as not all
3452     // of the processing tasks (specifically phase2 - pp2_work) execute
3453     // the complete_gc closure (which ordinarily would drain the queue) so
3454     // the queue may not be empty.
3455   }
3456 };
3457 
3458 // Driver routine for parallel reference processing.
3459 // Creates an instance of the ref processing gang
3460 // task and has the worker threads execute it.
3461 void G1STWRefProcTaskExecutor::execute(ProcessTask&amp; proc_task, uint ergo_workers) {
3462   assert(_workers != NULL, "Need parallel worker threads.");
3463 
3464   assert(_workers-&gt;active_workers() &gt;= ergo_workers,
3465          "Ergonomically chosen workers (%u) should be less than or equal to active workers (%u)",
3466          ergo_workers, _workers-&gt;active_workers());
3467   TaskTerminator terminator(ergo_workers, _queues);
3468   G1STWRefProcTaskProxy proc_task_proxy(proc_task, _g1h, _pss, _queues, &amp;terminator);
3469 
3470   _workers-&gt;run_task(&amp;proc_task_proxy, ergo_workers);
3471 }
3472 
3473 // End of weak reference support closures
3474 
3475 void G1CollectedHeap::process_discovered_references(G1ParScanThreadStateSet* per_thread_states) {
3476   double ref_proc_start = os::elapsedTime();
3477 
3478   ReferenceProcessor* rp = _ref_processor_stw;
3479   assert(rp-&gt;discovery_enabled(), "should have been enabled");
3480 
3481   // Closure to test whether a referent is alive.
3482   G1STWIsAliveClosure is_alive(this);
3483 
3484   // Even when parallel reference processing is enabled, the processing
3485   // of JNI refs is serial and performed serially by the current thread
3486   // rather than by a worker. The following PSS will be used for processing
3487   // JNI refs.
3488 
3489   // Use only a single queue for this PSS.
3490   G1ParScanThreadState*          pss = per_thread_states-&gt;state_for_worker(0);
3491   pss-&gt;set_ref_discoverer(NULL);
3492   assert(pss-&gt;queue_is_empty(), "pre-condition");
3493 
3494   // Keep alive closure.
3495   G1CopyingKeepAliveClosure keep_alive(this, pss);
3496 
3497   // Serial Complete GC closure
3498   G1STWDrainQueueClosure drain_queue(this, pss);
3499 
3500   // Setup the soft refs policy...
3501   rp-&gt;setup_policy(false);
3502 
3503   ReferenceProcessorPhaseTimes* pt = phase_times()-&gt;ref_phase_times();
3504 
3505   ReferenceProcessorStats stats;
3506   if (!rp-&gt;processing_is_mt()) {
3507     // Serial reference processing...
3508     stats = rp-&gt;process_discovered_references(&amp;is_alive,
3509                                               &amp;keep_alive,
3510                                               &amp;drain_queue,
3511                                               NULL,
3512                                               pt);
3513   } else {
3514     uint no_of_gc_workers = workers()-&gt;active_workers();
3515 
3516     // Parallel reference processing
3517     assert(no_of_gc_workers &lt;= rp-&gt;max_num_queues(),
3518            "Mismatch between the number of GC workers %u and the maximum number of Reference process queues %u",
3519            no_of_gc_workers,  rp-&gt;max_num_queues());
3520 
3521     G1STWRefProcTaskExecutor par_task_executor(this, per_thread_states, workers(), _task_queues);
3522     stats = rp-&gt;process_discovered_references(&amp;is_alive,
3523                                               &amp;keep_alive,
3524                                               &amp;drain_queue,
3525                                               &amp;par_task_executor,
3526                                               pt);
3527   }
3528 
3529   _gc_tracer_stw-&gt;report_gc_reference_stats(stats);
3530 
3531   // We have completed copying any necessary live referent objects.
3532   assert(pss-&gt;queue_is_empty(), "both queue and overflow should be empty");
3533 
3534   make_pending_list_reachable();
3535 
3536   assert(!rp-&gt;discovery_enabled(), "Postcondition");
3537   rp-&gt;verify_no_references_recorded();
3538 
3539   double ref_proc_time = os::elapsedTime() - ref_proc_start;
3540   phase_times()-&gt;record_ref_proc_time(ref_proc_time * 1000.0);
3541 }
3542 
3543 void G1CollectedHeap::make_pending_list_reachable() {
3544   if (collector_state()-&gt;in_concurrent_start_gc()) {
3545     oop pll_head = Universe::reference_pending_list();
3546     if (pll_head != NULL) {
3547       // Any valid worker id is fine here as we are in the VM thread and single-threaded.
3548       _cm-&gt;mark_in_next_bitmap(0 /* worker_id */, pll_head);
3549     }
3550   }
3551 }
3552 
3553 void G1CollectedHeap::merge_per_thread_state_info(G1ParScanThreadStateSet* per_thread_states) {
3554   Ticks start = Ticks::now();
3555   per_thread_states-&gt;flush();
3556   phase_times()-&gt;record_or_add_time_secs(G1GCPhaseTimes::MergePSS, 0 /* worker_id */, (Ticks::now() - start).seconds());
3557 }
3558 
3559 class G1PrepareEvacuationTask : public AbstractGangTask {
3560   class G1PrepareRegionsClosure : public HeapRegionClosure {
3561     G1CollectedHeap* _g1h;
3562     G1PrepareEvacuationTask* _parent_task;
3563     size_t _worker_humongous_total;
3564     size_t _worker_humongous_candidates;
3565 
3566     bool humongous_region_is_candidate(HeapRegion* region) const {
3567       assert(region-&gt;is_starts_humongous(), "Must start a humongous object");
3568 
3569       oop obj = oop(region-&gt;bottom());
3570 
3571       // Dead objects cannot be eager reclaim candidates. Due to class
3572       // unloading it is unsafe to query their classes so we return early.
3573       if (_g1h-&gt;is_obj_dead(obj, region)) {
3574         return false;
3575       }
3576 
3577       // If we do not have a complete remembered set for the region, then we can
3578       // not be sure that we have all references to it.
3579       if (!region-&gt;rem_set()-&gt;is_complete()) {
3580         return false;
3581       }
3582       // Candidate selection must satisfy the following constraints
3583       // while concurrent marking is in progress:
3584       //
3585       // * In order to maintain SATB invariants, an object must not be
3586       // reclaimed if it was allocated before the start of marking and
3587       // has not had its references scanned.  Such an object must have
3588       // its references (including type metadata) scanned to ensure no
3589       // live objects are missed by the marking process.  Objects
3590       // allocated after the start of concurrent marking don't need to
3591       // be scanned.
3592       //
3593       // * An object must not be reclaimed if it is on the concurrent
3594       // mark stack.  Objects allocated after the start of concurrent
3595       // marking are never pushed on the mark stack.
3596       //
3597       // Nominating only objects allocated after the start of concurrent
3598       // marking is sufficient to meet both constraints.  This may miss
3599       // some objects that satisfy the constraints, but the marking data
3600       // structures don't support efficiently performing the needed
3601       // additional tests or scrubbing of the mark stack.
3602       //
3603       // However, we presently only nominate is_typeArray() objects.
3604       // A humongous object containing references induces remembered
3605       // set entries on other regions.  In order to reclaim such an
3606       // object, those remembered sets would need to be cleaned up.
3607       //
3608       // We also treat is_typeArray() objects specially, allowing them
3609       // to be reclaimed even if allocated before the start of
3610       // concurrent mark.  For this we rely on mark stack insertion to
3611       // exclude is_typeArray() objects, preventing reclaiming an object
3612       // that is in the mark stack.  We also rely on the metadata for
3613       // such objects to be built-in and so ensured to be kept live.
3614       // Frequent allocation and drop of large binary blobs is an
3615       // important use case for eager reclaim, and this special handling
3616       // may reduce needed headroom.
3617 
3618       return obj-&gt;is_typeArray() &amp;&amp;
3619              _g1h-&gt;is_potential_eager_reclaim_candidate(region);
3620     }
3621 
3622   public:
3623     G1PrepareRegionsClosure(G1CollectedHeap* g1h, G1PrepareEvacuationTask* parent_task) :
3624       _g1h(g1h),
3625       _parent_task(parent_task),
3626       _worker_humongous_total(0),
3627       _worker_humongous_candidates(0) { }
3628 
3629     ~G1PrepareRegionsClosure() {
3630       _parent_task-&gt;add_humongous_candidates(_worker_humongous_candidates);
3631       _parent_task-&gt;add_humongous_total(_worker_humongous_total);
3632     }
3633 
3634     virtual bool do_heap_region(HeapRegion* hr) {
3635       // First prepare the region for scanning
3636       _g1h-&gt;rem_set()-&gt;prepare_region_for_scan(hr);
3637 
3638       // Now check if region is a humongous candidate
3639       if (!hr-&gt;is_starts_humongous()) {
3640         _g1h-&gt;register_region_with_region_attr(hr);
3641         return false;
3642       }
3643 
3644       uint index = hr-&gt;hrm_index();
3645       if (humongous_region_is_candidate(hr)) {
3646         _g1h-&gt;set_humongous_reclaim_candidate(index, true);
3647         _g1h-&gt;register_humongous_region_with_region_attr(index);
3648         _worker_humongous_candidates++;
3649         // We will later handle the remembered sets of these regions.
3650       } else {
3651         _g1h-&gt;set_humongous_reclaim_candidate(index, false);
3652         _g1h-&gt;register_region_with_region_attr(hr);
3653       }
3654       _worker_humongous_total++;
3655 
3656       return false;
3657     }
3658   };
3659 
3660   G1CollectedHeap* _g1h;
3661   HeapRegionClaimer _claimer;
3662   volatile size_t _humongous_total;
3663   volatile size_t _humongous_candidates;
3664 public:
3665   G1PrepareEvacuationTask(G1CollectedHeap* g1h) :
3666     AbstractGangTask("Prepare Evacuation"),
3667     _g1h(g1h),
3668     _claimer(_g1h-&gt;workers()-&gt;active_workers()),
3669     _humongous_total(0),
3670     _humongous_candidates(0) { }
3671 
3672   ~G1PrepareEvacuationTask() {
3673     _g1h-&gt;set_has_humongous_reclaim_candidate(_humongous_candidates &gt; 0);
3674   }
3675 
3676   void work(uint worker_id) {
3677     G1PrepareRegionsClosure cl(_g1h, this);
3678     _g1h-&gt;heap_region_par_iterate_from_worker_offset(&amp;cl, &amp;_claimer, worker_id);
3679   }
3680 
3681   void add_humongous_candidates(size_t candidates) {
3682     Atomic::add(&amp;_humongous_candidates, candidates);
3683   }
3684 
3685   void add_humongous_total(size_t total) {
3686     Atomic::add(&amp;_humongous_total, total);
3687   }
3688 
3689   size_t humongous_candidates() {
3690     return _humongous_candidates;
3691   }
3692 
3693   size_t humongous_total() {
3694     return _humongous_total;
3695   }
3696 };
3697 
3698 void G1CollectedHeap::pre_evacuate_collection_set(G1EvacuationInfo&amp; evacuation_info, G1ParScanThreadStateSet* per_thread_states) {
3699   _bytes_used_during_gc = 0;
3700 
3701   _expand_heap_after_alloc_failure = true;
3702   _evacuation_failed = false;
3703 
3704   // Disable the hot card cache.
3705   _hot_card_cache-&gt;reset_hot_cache_claimed_index();
3706   _hot_card_cache-&gt;set_use_cache(false);
3707 
3708   // Initialize the GC alloc regions.
3709   _allocator-&gt;init_gc_alloc_regions(evacuation_info);
3710 
3711   {
3712     Ticks start = Ticks::now();
3713     rem_set()-&gt;prepare_for_scan_heap_roots();
3714     phase_times()-&gt;record_prepare_heap_roots_time_ms((Ticks::now() - start).seconds() * 1000.0);
3715   }
3716 
3717   {
3718     G1PrepareEvacuationTask g1_prep_task(this);
3719     Tickspan task_time = run_task_timed(&amp;g1_prep_task);
3720 
3721     phase_times()-&gt;record_register_regions(task_time.seconds() * 1000.0,
3722                                            g1_prep_task.humongous_total(),
3723                                            g1_prep_task.humongous_candidates());
3724   }
3725 
3726   assert(_verifier-&gt;check_region_attr_table(), "Inconsistency in the region attributes table.");
3727   _preserved_marks_set.assert_empty();
3728 
3729 #if COMPILER2_OR_JVMCI
3730   DerivedPointerTable::clear();
3731 #endif
3732 
3733   // Concurrent start needs claim bits to keep track of the marked-through CLDs.
3734   if (collector_state()-&gt;in_concurrent_start_gc()) {
3735     concurrent_mark()-&gt;pre_concurrent_start();
3736 
3737     double start_clear_claimed_marks = os::elapsedTime();
3738 
3739     ClassLoaderDataGraph::clear_claimed_marks();
3740 
3741     double recorded_clear_claimed_marks_time_ms = (os::elapsedTime() - start_clear_claimed_marks) * 1000.0;
3742     phase_times()-&gt;record_clear_claimed_marks_time_ms(recorded_clear_claimed_marks_time_ms);
3743   }
3744 
3745   // Should G1EvacuationFailureALot be in effect for this GC?
3746   NOT_PRODUCT(set_evacuation_failure_alot_for_current_gc();)
3747 }
3748 
3749 class G1EvacuateRegionsBaseTask : public AbstractGangTask {
3750 protected:
3751   G1CollectedHeap* _g1h;
3752   G1ParScanThreadStateSet* _per_thread_states;
3753   G1ScannerTasksQueueSet* _task_queues;
3754   TaskTerminator _terminator;
3755   uint _num_workers;
3756 
3757   void evacuate_live_objects(G1ParScanThreadState* pss,
3758                              uint worker_id,
3759                              G1GCPhaseTimes::GCParPhases objcopy_phase,
3760                              G1GCPhaseTimes::GCParPhases termination_phase) {
3761     G1GCPhaseTimes* p = _g1h-&gt;phase_times();
3762 
3763     Ticks start = Ticks::now();
3764     G1ParEvacuateFollowersClosure cl(_g1h, pss, _task_queues, &amp;_terminator, objcopy_phase);
3765     cl.do_void();
3766 
3767     assert(pss-&gt;queue_is_empty(), "should be empty");
3768 
3769     Tickspan evac_time = (Ticks::now() - start);
3770     p-&gt;record_or_add_time_secs(objcopy_phase, worker_id, evac_time.seconds() - cl.term_time());
3771 
3772     if (termination_phase == G1GCPhaseTimes::Termination) {
3773       p-&gt;record_time_secs(termination_phase, worker_id, cl.term_time());
3774       p-&gt;record_thread_work_item(termination_phase, worker_id, cl.term_attempts());
3775     } else {
3776       p-&gt;record_or_add_time_secs(termination_phase, worker_id, cl.term_time());
3777       p-&gt;record_or_add_thread_work_item(termination_phase, worker_id, cl.term_attempts());
3778     }
3779     assert(pss-&gt;trim_ticks().seconds() == 0.0, "Unexpected partial trimming during evacuation");
3780   }
3781 
3782   virtual void start_work(uint worker_id) { }
3783 
3784   virtual void end_work(uint worker_id) { }
3785 
3786   virtual void scan_roots(G1ParScanThreadState* pss, uint worker_id) = 0;
3787 
3788   virtual void evacuate_live_objects(G1ParScanThreadState* pss, uint worker_id) = 0;
3789 
3790 public:
3791   G1EvacuateRegionsBaseTask(const char* name,
3792                             G1ParScanThreadStateSet* per_thread_states,
3793                             G1ScannerTasksQueueSet* task_queues,
3794                             uint num_workers) :
3795     AbstractGangTask(name),
3796     _g1h(G1CollectedHeap::heap()),
3797     _per_thread_states(per_thread_states),
3798     _task_queues(task_queues),
3799     _terminator(num_workers, _task_queues),
3800     _num_workers(num_workers)
3801   { }
3802 
3803   void work(uint worker_id) {
3804     start_work(worker_id);
3805 
3806     {
3807       ResourceMark rm;
3808 
3809       G1ParScanThreadState* pss = _per_thread_states-&gt;state_for_worker(worker_id);
3810       pss-&gt;set_ref_discoverer(_g1h-&gt;ref_processor_stw());
3811 
3812       scan_roots(pss, worker_id);
3813       evacuate_live_objects(pss, worker_id);
3814     }
3815 
3816     end_work(worker_id);
3817   }
3818 };
3819 
3820 class G1EvacuateRegionsTask : public G1EvacuateRegionsBaseTask {
3821   G1RootProcessor* _root_processor;
3822 
3823   void scan_roots(G1ParScanThreadState* pss, uint worker_id) {
3824     _root_processor-&gt;evacuate_roots(pss, worker_id);
3825     _g1h-&gt;rem_set()-&gt;scan_heap_roots(pss, worker_id, G1GCPhaseTimes::ScanHR, G1GCPhaseTimes::ObjCopy);
3826     _g1h-&gt;rem_set()-&gt;scan_collection_set_regions(pss, worker_id, G1GCPhaseTimes::ScanHR, G1GCPhaseTimes::CodeRoots, G1GCPhaseTimes::ObjCopy);
3827   }
3828 
3829   void evacuate_live_objects(G1ParScanThreadState* pss, uint worker_id) {
3830     G1EvacuateRegionsBaseTask::evacuate_live_objects(pss, worker_id, G1GCPhaseTimes::ObjCopy, G1GCPhaseTimes::Termination);
3831   }
3832 
3833   void start_work(uint worker_id) {
3834     _g1h-&gt;phase_times()-&gt;record_time_secs(G1GCPhaseTimes::GCWorkerStart, worker_id, Ticks::now().seconds());
3835   }
3836 
3837   void end_work(uint worker_id) {
3838     _g1h-&gt;phase_times()-&gt;record_time_secs(G1GCPhaseTimes::GCWorkerEnd, worker_id, Ticks::now().seconds());
3839   }
3840 
3841 public:
3842   G1EvacuateRegionsTask(G1CollectedHeap* g1h,
3843                         G1ParScanThreadStateSet* per_thread_states,
3844                         G1ScannerTasksQueueSet* task_queues,
3845                         G1RootProcessor* root_processor,
3846                         uint num_workers) :
3847     G1EvacuateRegionsBaseTask("G1 Evacuate Regions", per_thread_states, task_queues, num_workers),
3848     _root_processor(root_processor)
3849   { }
3850 };
3851 
3852 void G1CollectedHeap::evacuate_initial_collection_set(G1ParScanThreadStateSet* per_thread_states) {
3853   G1GCPhaseTimes* p = phase_times();
3854 
3855   {
3856     Ticks start = Ticks::now();
3857     rem_set()-&gt;merge_heap_roots(true /* initial_evacuation */);
3858     p-&gt;record_merge_heap_roots_time((Ticks::now() - start).seconds() * 1000.0);
3859   }
3860 
3861   Tickspan task_time;
3862   const uint num_workers = workers()-&gt;active_workers();
3863 
3864   Ticks start_processing = Ticks::now();
3865   {
3866     G1RootProcessor root_processor(this, num_workers);
3867     G1EvacuateRegionsTask g1_par_task(this, per_thread_states, _task_queues, &amp;root_processor, num_workers);
3868     task_time = run_task_timed(&amp;g1_par_task);
3869     // Closing the inner scope will execute the destructor for the G1RootProcessor object.
3870     // To extract its code root fixup time we measure total time of this scope and
3871     // subtract from the time the WorkGang task took.
3872   }
3873   Tickspan total_processing = Ticks::now() - start_processing;
3874 
3875   p-&gt;record_initial_evac_time(task_time.seconds() * 1000.0);
3876   p-&gt;record_or_add_code_root_fixup_time((total_processing - task_time).seconds() * 1000.0);
3877 }
3878 
3879 class G1EvacuateOptionalRegionsTask : public G1EvacuateRegionsBaseTask {
3880 
3881   void scan_roots(G1ParScanThreadState* pss, uint worker_id) {
3882     _g1h-&gt;rem_set()-&gt;scan_heap_roots(pss, worker_id, G1GCPhaseTimes::OptScanHR, G1GCPhaseTimes::OptObjCopy);
3883     _g1h-&gt;rem_set()-&gt;scan_collection_set_regions(pss, worker_id, G1GCPhaseTimes::OptScanHR, G1GCPhaseTimes::OptCodeRoots, G1GCPhaseTimes::OptObjCopy);
3884   }
3885 
3886   void evacuate_live_objects(G1ParScanThreadState* pss, uint worker_id) {
3887     G1EvacuateRegionsBaseTask::evacuate_live_objects(pss, worker_id, G1GCPhaseTimes::OptObjCopy, G1GCPhaseTimes::OptTermination);
3888   }
3889 
3890 public:
3891   G1EvacuateOptionalRegionsTask(G1ParScanThreadStateSet* per_thread_states,
3892                                 G1ScannerTasksQueueSet* queues,
3893                                 uint num_workers) :
3894     G1EvacuateRegionsBaseTask("G1 Evacuate Optional Regions", per_thread_states, queues, num_workers) {
3895   }
3896 };
3897 
3898 void G1CollectedHeap::evacuate_next_optional_regions(G1ParScanThreadStateSet* per_thread_states) {
3899   class G1MarkScope : public MarkScope { };
3900 
3901   Tickspan task_time;
3902 
3903   Ticks start_processing = Ticks::now();
3904   {
3905     G1MarkScope code_mark_scope;
3906     G1EvacuateOptionalRegionsTask task(per_thread_states, _task_queues, workers()-&gt;active_workers());
3907     task_time = run_task_timed(&amp;task);
3908     // See comment in evacuate_collection_set() for the reason of the scope.
3909   }
3910   Tickspan total_processing = Ticks::now() - start_processing;
3911 
3912   G1GCPhaseTimes* p = phase_times();
3913   p-&gt;record_or_add_code_root_fixup_time((total_processing - task_time).seconds() * 1000.0);
3914 }
3915 
3916 void G1CollectedHeap::evacuate_optional_collection_set(G1ParScanThreadStateSet* per_thread_states) {
3917   const double gc_start_time_ms = phase_times()-&gt;cur_collection_start_sec() * 1000.0;
3918 
3919   while (!evacuation_failed() &amp;&amp; _collection_set.optional_region_length() &gt; 0) {
3920 
3921     double time_used_ms = os::elapsedTime() * 1000.0 - gc_start_time_ms;
3922     double time_left_ms = MaxGCPauseMillis - time_used_ms;
3923 
3924     if (time_left_ms &lt; 0 ||
3925         !_collection_set.finalize_optional_for_evacuation(time_left_ms * policy()-&gt;optional_evacuation_fraction())) {
3926       log_trace(gc, ergo, cset)("Skipping evacuation of %u optional regions, no more regions can be evacuated in %.3fms",
3927                                 _collection_set.optional_region_length(), time_left_ms);
3928       break;
3929     }
3930 
3931     {
3932       Ticks start = Ticks::now();
3933       rem_set()-&gt;merge_heap_roots(false /* initial_evacuation */);
3934       phase_times()-&gt;record_or_add_optional_merge_heap_roots_time((Ticks::now() - start).seconds() * 1000.0);
3935     }
3936 
3937     {
3938       Ticks start = Ticks::now();
3939       evacuate_next_optional_regions(per_thread_states);
3940       phase_times()-&gt;record_or_add_optional_evac_time((Ticks::now() - start).seconds() * 1000.0);
3941     }
3942   }
3943 
3944   _collection_set.abandon_optional_collection_set(per_thread_states);
3945 }
3946 
3947 void G1CollectedHeap::post_evacuate_collection_set(G1EvacuationInfo&amp; evacuation_info,
3948                                                    G1RedirtyCardsQueueSet* rdcqs,
3949                                                    G1ParScanThreadStateSet* per_thread_states) {
3950   G1GCPhaseTimes* p = phase_times();
3951 
3952   rem_set()-&gt;cleanup_after_scan_heap_roots();
3953 
3954   // Process any discovered reference objects - we have
3955   // to do this _before_ we retire the GC alloc regions
3956   // as we may have to copy some 'reachable' referent
3957   // objects (and their reachable sub-graphs) that were
3958   // not copied during the pause.
3959   process_discovered_references(per_thread_states);
3960 
3961   G1STWIsAliveClosure is_alive(this);
3962   G1KeepAliveClosure keep_alive(this);
3963 
3964   WeakProcessor::weak_oops_do(workers(), &amp;is_alive, &amp;keep_alive, p-&gt;weak_phase_times());
3965 
3966   if (G1StringDedup::is_enabled()) {
3967     double string_dedup_time_ms = os::elapsedTime();
3968 
3969     string_dedup_cleaning(&amp;is_alive, &amp;keep_alive, p);
3970 
3971     double string_cleanup_time_ms = (os::elapsedTime() - string_dedup_time_ms) * 1000.0;
3972     p-&gt;record_string_deduplication_time(string_cleanup_time_ms);
3973   }
3974 
3975   _allocator-&gt;release_gc_alloc_regions(evacuation_info);
3976 
3977   if (evacuation_failed()) {
3978     restore_after_evac_failure(rdcqs);
3979 
3980     // Reset the G1EvacuationFailureALot counters and flags
3981     NOT_PRODUCT(reset_evacuation_should_fail();)
3982 
3983     double recalculate_used_start = os::elapsedTime();
3984     set_used(recalculate_used());
3985     p-&gt;record_evac_fail_recalc_used_time((os::elapsedTime() - recalculate_used_start) * 1000.0);
3986 
3987     if (_archive_allocator != NULL) {
3988       _archive_allocator-&gt;clear_used();
3989     }
3990     for (uint i = 0; i &lt; ParallelGCThreads; i++) {
3991       if (_evacuation_failed_info_array[i].has_failed()) {
3992         _gc_tracer_stw-&gt;report_evacuation_failed(_evacuation_failed_info_array[i]);
3993       }
3994     }
3995   } else {
3996     // The "used" of the the collection set have already been subtracted
3997     // when they were freed.  Add in the bytes used.
3998     increase_used(_bytes_used_during_gc);
3999   }
4000 
4001   _preserved_marks_set.assert_empty();
4002 
4003   merge_per_thread_state_info(per_thread_states);
4004 
4005   // Reset and re-enable the hot card cache.
4006   // Note the counts for the cards in the regions in the
4007   // collection set are reset when the collection set is freed.
4008   _hot_card_cache-&gt;reset_hot_cache();
4009   _hot_card_cache-&gt;set_use_cache(true);
4010 
4011   purge_code_root_memory();
4012 
4013   redirty_logged_cards(rdcqs);
4014 
4015   free_collection_set(&amp;_collection_set, evacuation_info, per_thread_states-&gt;surviving_young_words());
4016 
4017   eagerly_reclaim_humongous_regions();
4018 
4019   record_obj_copy_mem_stats();
4020 
4021   evacuation_info.set_collectionset_used_before(collection_set()-&gt;bytes_used_before());
4022   evacuation_info.set_bytes_used(_bytes_used_during_gc);
4023 
4024 #if COMPILER2_OR_JVMCI
4025   double start = os::elapsedTime();
4026   DerivedPointerTable::update_pointers();
4027   phase_times()-&gt;record_derived_pointer_table_update_time((os::elapsedTime() - start) * 1000.0);
4028 #endif
4029   policy()-&gt;print_age_table();
4030 }
4031 
4032 void G1CollectedHeap::record_obj_copy_mem_stats() {
4033   policy()-&gt;old_gen_alloc_tracker()-&gt;
4034     add_allocated_bytes_since_last_gc(_old_evac_stats.allocated() * HeapWordSize);
4035 
4036   _gc_tracer_stw-&gt;report_evacuation_statistics(create_g1_evac_summary(&amp;_survivor_evac_stats),
4037                                                create_g1_evac_summary(&amp;_old_evac_stats));
4038 }
4039 
4040 void G1CollectedHeap::free_region(HeapRegion* hr, FreeRegionList* free_list) {
4041   assert(!hr-&gt;is_free(), "the region should not be free");
4042   assert(!hr-&gt;is_empty(), "the region should not be empty");
4043   assert(_hrm-&gt;is_available(hr-&gt;hrm_index()), "region should be committed");
4044 
4045   if (G1VerifyBitmaps) {
4046     MemRegion mr(hr-&gt;bottom(), hr-&gt;end());
4047     concurrent_mark()-&gt;clear_range_in_prev_bitmap(mr);
4048   }
4049 
4050   // Clear the card counts for this region.
4051   // Note: we only need to do this if the region is not young
4052   // (since we don't refine cards in young regions).
4053   if (!hr-&gt;is_young()) {
4054     _hot_card_cache-&gt;reset_card_counts(hr);
4055   }
4056 
4057   // Reset region metadata to allow reuse.
4058   hr-&gt;hr_clear(true /* clear_space */);
4059   _policy-&gt;remset_tracker()-&gt;update_at_free(hr);
4060 
4061   if (free_list != NULL) {
4062     free_list-&gt;add_ordered(hr);
4063   }
4064 }
4065 
4066 void G1CollectedHeap::free_humongous_region(HeapRegion* hr,
4067                                             FreeRegionList* free_list) {
4068   assert(hr-&gt;is_humongous(), "this is only for humongous regions");
4069   assert(free_list != NULL, "pre-condition");
4070   hr-&gt;clear_humongous();
4071   free_region(hr, free_list);
4072 }
4073 
4074 void G1CollectedHeap::remove_from_old_sets(const uint old_regions_removed,
4075                                            const uint humongous_regions_removed) {
4076   if (old_regions_removed &gt; 0 || humongous_regions_removed &gt; 0) {
4077     MutexLocker x(OldSets_lock, Mutex::_no_safepoint_check_flag);
4078     _old_set.bulk_remove(old_regions_removed);
4079     _humongous_set.bulk_remove(humongous_regions_removed);
4080   }
4081 
4082 }
4083 
4084 void G1CollectedHeap::prepend_to_freelist(FreeRegionList* list) {
4085   assert(list != NULL, "list can't be null");
4086   if (!list-&gt;is_empty()) {
4087     MutexLocker x(FreeList_lock, Mutex::_no_safepoint_check_flag);
4088     _hrm-&gt;insert_list_into_free_list(list);
4089   }
4090 }
4091 
4092 void G1CollectedHeap::decrement_summary_bytes(size_t bytes) {
4093   decrease_used(bytes);
4094 }
4095 
4096 class G1FreeCollectionSetTask : public AbstractGangTask {
4097   // Helper class to keep statistics for the collection set freeing
4098   class FreeCSetStats {
4099     size_t _before_used_bytes;   // Usage in regions successfully evacutate
4100     size_t _after_used_bytes;    // Usage in regions failing evacuation
4101     size_t _bytes_allocated_in_old_since_last_gc; // Size of young regions turned into old
4102     size_t _failure_used_words;  // Live size in failed regions
4103     size_t _failure_waste_words; // Wasted size in failed regions
4104     size_t _rs_length;           // Remembered set size
4105     uint _regions_freed;         // Number of regions freed
4106   public:
4107     FreeCSetStats() :
4108         _before_used_bytes(0),
4109         _after_used_bytes(0),
4110         _bytes_allocated_in_old_since_last_gc(0),
4111         _failure_used_words(0),
4112         _failure_waste_words(0),
4113         _rs_length(0),
4114         _regions_freed(0) { }
4115 
4116     void merge_stats(FreeCSetStats* other) {
4117       assert(other != NULL, "invariant");
4118       _before_used_bytes += other-&gt;_before_used_bytes;
4119       _after_used_bytes += other-&gt;_after_used_bytes;
4120       _bytes_allocated_in_old_since_last_gc += other-&gt;_bytes_allocated_in_old_since_last_gc;
4121       _failure_used_words += other-&gt;_failure_used_words;
4122       _failure_waste_words += other-&gt;_failure_waste_words;
4123       _rs_length += other-&gt;_rs_length;
4124       _regions_freed += other-&gt;_regions_freed;
4125     }
4126 
4127     void report(G1CollectedHeap* g1h, G1EvacuationInfo* evacuation_info) {
4128       evacuation_info-&gt;set_regions_freed(_regions_freed);
4129       evacuation_info-&gt;increment_collectionset_used_after(_after_used_bytes);
4130 
4131       g1h-&gt;decrement_summary_bytes(_before_used_bytes);
4132       g1h-&gt;alloc_buffer_stats(G1HeapRegionAttr::Old)-&gt;add_failure_used_and_waste(_failure_used_words, _failure_waste_words);
4133 
4134       G1Policy *policy = g1h-&gt;policy();
4135       policy-&gt;old_gen_alloc_tracker()-&gt;add_allocated_bytes_since_last_gc(_bytes_allocated_in_old_since_last_gc);
4136       policy-&gt;record_rs_length(_rs_length);
4137       policy-&gt;cset_regions_freed();
4138     }
4139 
4140     void account_failed_region(HeapRegion* r) {
4141       size_t used_words = r-&gt;marked_bytes() / HeapWordSize;
4142       _failure_used_words += used_words;
4143       _failure_waste_words += HeapRegion::GrainWords - used_words;
4144       _after_used_bytes += r-&gt;used();
4145 
4146       // When moving a young gen region to old gen, we "allocate" that whole
4147       // region there. This is in addition to any already evacuated objects.
4148       // Notify the policy about that. Old gen regions do not cause an
4149       // additional allocation: both the objects still in the region and the
4150       // ones already moved are accounted for elsewhere.
4151       if (r-&gt;is_young()) {
4152         _bytes_allocated_in_old_since_last_gc += HeapRegion::GrainBytes;
4153       }
4154     }
4155 
4156     void account_evacuated_region(HeapRegion* r) {
4157       _before_used_bytes += r-&gt;used();
4158       _regions_freed += 1;
4159     }
4160 
4161     void account_rs_length(HeapRegion* r) {
4162       _rs_length += r-&gt;rem_set()-&gt;occupied();
4163     }
4164   };
4165 
4166   // Closure applied to all regions in the collection set.
4167   class FreeCSetClosure : public HeapRegionClosure {
4168     // Helper to send JFR events for regions.
4169     class JFREventForRegion {
4170       EventGCPhaseParallel _event;
4171     public:
4172       JFREventForRegion(HeapRegion* region, uint worker_id) : _event() {
4173         _event.set_gcId(GCId::current());
4174         _event.set_gcWorkerId(worker_id);
4175         if (region-&gt;is_young()) {
4176           _event.set_name(G1GCPhaseTimes::phase_name(G1GCPhaseTimes::YoungFreeCSet));
4177         } else {
4178           _event.set_name(G1GCPhaseTimes::phase_name(G1GCPhaseTimes::NonYoungFreeCSet));
4179         }
4180       }
4181 
4182       ~JFREventForRegion() {
4183         _event.commit();
4184       }
4185     };
4186 
4187     // Helper to do timing for region work.
4188     class TimerForRegion {
4189       Tickspan&amp; _time;
4190       Ticks     _start_time;
4191     public:
4192       TimerForRegion(Tickspan&amp; time) : _time(time), _start_time(Ticks::now()) { }
4193       ~TimerForRegion() {
4194         _time += Ticks::now() - _start_time;
4195       }
4196     };
4197 
4198     // FreeCSetClosure members
4199     G1CollectedHeap* _g1h;
4200     const size_t*    _surviving_young_words;
4201     uint             _worker_id;
4202     Tickspan         _young_time;
4203     Tickspan         _non_young_time;
4204     FreeCSetStats*   _stats;
4205 
4206     void assert_in_cset(HeapRegion* r) {
4207       assert(r-&gt;young_index_in_cset() != 0 &amp;&amp;
4208              (uint)r-&gt;young_index_in_cset() &lt;= _g1h-&gt;collection_set()-&gt;young_region_length(),
4209              "Young index %u is wrong for region %u of type %s with %u young regions",
4210              r-&gt;young_index_in_cset(), r-&gt;hrm_index(), r-&gt;get_type_str(), _g1h-&gt;collection_set()-&gt;young_region_length());
4211     }
4212 
4213     void handle_evacuated_region(HeapRegion* r) {
4214       assert(!r-&gt;is_empty(), "Region %u is an empty region in the collection set.", r-&gt;hrm_index());
4215       stats()-&gt;account_evacuated_region(r);
4216 
4217       // Free the region and and its remembered set.
4218       _g1h-&gt;free_region(r, NULL);
4219     }
4220 
4221     void handle_failed_region(HeapRegion* r) {
4222       // Do some allocation statistics accounting. Regions that failed evacuation
4223       // are always made old, so there is no need to update anything in the young
4224       // gen statistics, but we need to update old gen statistics.
4225       stats()-&gt;account_failed_region(r);
4226 
4227       // Update the region state due to the failed evacuation.
4228       r-&gt;handle_evacuation_failure();
4229 
4230       // Add region to old set, need to hold lock.
4231       MutexLocker x(OldSets_lock, Mutex::_no_safepoint_check_flag);
4232       _g1h-&gt;old_set_add(r);
4233     }
4234 
4235     Tickspan&amp; timer_for_region(HeapRegion* r) {
4236       return r-&gt;is_young() ? _young_time : _non_young_time;
4237     }
4238 
4239     FreeCSetStats* stats() {
4240       return _stats;
4241     }
4242   public:
4243     FreeCSetClosure(const size_t* surviving_young_words,
4244                     uint worker_id,
4245                     FreeCSetStats* stats) :
4246         HeapRegionClosure(),
4247         _g1h(G1CollectedHeap::heap()),
4248         _surviving_young_words(surviving_young_words),
4249         _worker_id(worker_id),
4250         _young_time(),
4251         _non_young_time(),
4252         _stats(stats) { }
4253 
4254     virtual bool do_heap_region(HeapRegion* r) {
4255       assert(r-&gt;in_collection_set(), "Invariant: %u missing from CSet", r-&gt;hrm_index());
4256       JFREventForRegion event(r, _worker_id);
4257       TimerForRegion timer(timer_for_region(r));
4258 
4259       _g1h-&gt;clear_region_attr(r);
4260       stats()-&gt;account_rs_length(r);
4261 
4262       if (r-&gt;is_young()) {
4263         assert_in_cset(r);
4264         r-&gt;record_surv_words_in_group(_surviving_young_words[r-&gt;young_index_in_cset()]);
4265       }
4266 
4267       if (r-&gt;evacuation_failed()) {
4268         handle_failed_region(r);
4269       } else {
4270         handle_evacuated_region(r);
4271       }
4272       assert(!_g1h-&gt;is_on_master_free_list(r), "sanity");
4273 
4274       return false;
4275     }
4276 
4277     void report_timing(Tickspan parallel_time) {
4278       G1GCPhaseTimes* pt = _g1h-&gt;phase_times();
4279       pt-&gt;record_time_secs(G1GCPhaseTimes::ParFreeCSet, _worker_id, parallel_time.seconds());
4280       if (_young_time.value() &gt; 0) {
4281         pt-&gt;record_time_secs(G1GCPhaseTimes::YoungFreeCSet, _worker_id, _young_time.seconds());
4282       }
4283       if (_non_young_time.value() &gt; 0) {
4284         pt-&gt;record_time_secs(G1GCPhaseTimes::NonYoungFreeCSet, _worker_id, _non_young_time.seconds());
4285       }
4286     }
4287   };
4288 
4289   // G1FreeCollectionSetTask members
4290   G1CollectedHeap*  _g1h;
4291   G1EvacuationInfo* _evacuation_info;
4292   FreeCSetStats*    _worker_stats;
4293   HeapRegionClaimer _claimer;
4294   const size_t*     _surviving_young_words;
4295   uint              _active_workers;
4296 
4297   FreeCSetStats* worker_stats(uint worker) {
4298     return &amp;_worker_stats[worker];
4299   }
4300 
4301   void report_statistics() {
4302     // Merge the accounting
4303     FreeCSetStats total_stats;
4304     for (uint worker = 0; worker &lt; _active_workers; worker++) {
4305       total_stats.merge_stats(worker_stats(worker));
4306     }
4307     total_stats.report(_g1h, _evacuation_info);
4308   }
4309 
4310 public:
4311   G1FreeCollectionSetTask(G1EvacuationInfo* evacuation_info, const size_t* surviving_young_words, uint active_workers) :
4312       AbstractGangTask("G1 Free Collection Set"),
4313       _g1h(G1CollectedHeap::heap()),
4314       _evacuation_info(evacuation_info),
4315       _worker_stats(NEW_C_HEAP_ARRAY(FreeCSetStats, active_workers, mtGC)),
4316       _claimer(active_workers),
4317       _surviving_young_words(surviving_young_words),
4318       _active_workers(active_workers) {
4319     for (uint worker = 0; worker &lt; active_workers; worker++) {
4320       ::new (&amp;_worker_stats[worker]) FreeCSetStats();
4321     }
4322   }
4323 
4324   ~G1FreeCollectionSetTask() {
4325     Ticks serial_time = Ticks::now();
4326     report_statistics();
4327     for (uint worker = 0; worker &lt; _active_workers; worker++) {
4328       _worker_stats[worker].~FreeCSetStats();
4329     }
4330     FREE_C_HEAP_ARRAY(FreeCSetStats, _worker_stats);
4331     _g1h-&gt;phase_times()-&gt;record_serial_free_cset_time_ms((Ticks::now() - serial_time).seconds() * 1000.0);
4332   }
4333 
4334   virtual void work(uint worker_id) {
4335     EventGCPhaseParallel event;
4336     Ticks start = Ticks::now();
4337     FreeCSetClosure cl(_surviving_young_words, worker_id, worker_stats(worker_id));
4338     _g1h-&gt;collection_set_par_iterate_all(&amp;cl, &amp;_claimer, worker_id);
4339 
4340     // Report the total parallel time along with some more detailed metrics.
4341     cl.report_timing(Ticks::now() - start);
4342     event.commit(GCId::current(), worker_id, G1GCPhaseTimes::phase_name(G1GCPhaseTimes::ParFreeCSet));
4343   }
4344 };
4345 
4346 void G1CollectedHeap::free_collection_set(G1CollectionSet* collection_set, G1EvacuationInfo&amp; evacuation_info, const size_t* surviving_young_words) {
4347   _eden.clear();
4348 
4349   // The free collections set is split up in two tasks, the first
4350   // frees the collection set and records what regions are free,
4351   // and the second one rebuilds the free list. This proved to be
4352   // more efficient than adding a sorted list to another.
4353 
4354   Ticks free_cset_start_time = Ticks::now();
4355   {
4356     uint const num_cs_regions = _collection_set.region_length();
4357     uint const num_workers = clamp(num_cs_regions, 1u, workers()-&gt;active_workers());
4358     G1FreeCollectionSetTask cl(&amp;evacuation_info, surviving_young_words, num_workers);
4359 
4360     log_debug(gc, ergo)("Running %s using %u workers for collection set length %u (%u)",
4361                         cl.name(), num_workers, num_cs_regions, num_regions());
4362     workers()-&gt;run_task(&amp;cl, num_workers);
4363   }
4364 
4365   Ticks free_cset_end_time = Ticks::now();
4366   phase_times()-&gt;record_total_free_cset_time_ms((free_cset_end_time - free_cset_start_time).seconds() * 1000.0);
4367 
4368   // Now rebuild the free region list.
4369   hrm()-&gt;rebuild_free_list(workers());
4370   phase_times()-&gt;record_total_rebuild_freelist_time_ms((Ticks::now() - free_cset_end_time).seconds() * 1000.0);
4371 
4372   collection_set-&gt;clear();
4373 }
4374 
4375 class G1FreeHumongousRegionClosure : public HeapRegionClosure {
4376  private:
4377   FreeRegionList* _free_region_list;
4378   HeapRegionSet* _proxy_set;
4379   uint _humongous_objects_reclaimed;
4380   uint _humongous_regions_reclaimed;
4381   size_t _freed_bytes;
4382  public:
4383 
4384   G1FreeHumongousRegionClosure(FreeRegionList* free_region_list) :
4385     _free_region_list(free_region_list), _proxy_set(NULL), _humongous_objects_reclaimed(0), _humongous_regions_reclaimed(0), _freed_bytes(0) {
4386   }
4387 
4388   virtual bool do_heap_region(HeapRegion* r) {
4389     if (!r-&gt;is_starts_humongous()) {
4390       return false;
4391     }
4392 
4393     G1CollectedHeap* g1h = G1CollectedHeap::heap();
4394 
4395     oop obj = (oop)r-&gt;bottom();
4396     G1CMBitMap* next_bitmap = g1h-&gt;concurrent_mark()-&gt;next_mark_bitmap();
4397 
4398     // The following checks whether the humongous object is live are sufficient.
4399     // The main additional check (in addition to having a reference from the roots
4400     // or the young gen) is whether the humongous object has a remembered set entry.
4401     //
4402     // A humongous object cannot be live if there is no remembered set for it
4403     // because:
4404     // - there can be no references from within humongous starts regions referencing
4405     // the object because we never allocate other objects into them.
4406     // (I.e. there are no intra-region references that may be missed by the
4407     // remembered set)
4408     // - as soon there is a remembered set entry to the humongous starts region
4409     // (i.e. it has "escaped" to an old object) this remembered set entry will stay
4410     // until the end of a concurrent mark.
4411     //
4412     // It is not required to check whether the object has been found dead by marking
4413     // or not, in fact it would prevent reclamation within a concurrent cycle, as
4414     // all objects allocated during that time are considered live.
4415     // SATB marking is even more conservative than the remembered set.
4416     // So if at this point in the collection there is no remembered set entry,
4417     // nobody has a reference to it.
4418     // At the start of collection we flush all refinement logs, and remembered sets
4419     // are completely up-to-date wrt to references to the humongous object.
4420     //
4421     // Other implementation considerations:
4422     // - never consider object arrays at this time because they would pose
4423     // considerable effort for cleaning up the the remembered sets. This is
4424     // required because stale remembered sets might reference locations that
4425     // are currently allocated into.
4426     uint region_idx = r-&gt;hrm_index();
4427     if (!g1h-&gt;is_humongous_reclaim_candidate(region_idx) ||
4428         !r-&gt;rem_set()-&gt;is_empty()) {
4429       log_debug(gc, humongous)("Live humongous region %u object size " SIZE_FORMAT " start " PTR_FORMAT "  with remset " SIZE_FORMAT " code roots " SIZE_FORMAT " is marked %d reclaim candidate %d type array %d",
4430                                region_idx,
4431                                (size_t)obj-&gt;size() * HeapWordSize,
4432                                p2i(r-&gt;bottom()),
4433                                r-&gt;rem_set()-&gt;occupied(),
4434                                r-&gt;rem_set()-&gt;strong_code_roots_list_length(),
4435                                next_bitmap-&gt;is_marked(r-&gt;bottom()),
4436                                g1h-&gt;is_humongous_reclaim_candidate(region_idx),
4437                                obj-&gt;is_typeArray()
4438                               );
4439       return false;
4440     }
4441 
4442     guarantee(obj-&gt;is_typeArray(),
4443               "Only eagerly reclaiming type arrays is supported, but the object "
4444               PTR_FORMAT " is not.", p2i(r-&gt;bottom()));
4445 
4446     log_debug(gc, humongous)("Dead humongous region %u object size " SIZE_FORMAT " start " PTR_FORMAT " with remset " SIZE_FORMAT " code roots " SIZE_FORMAT " is marked %d reclaim candidate %d type array %d",
4447                              region_idx,
4448                              (size_t)obj-&gt;size() * HeapWordSize,
4449                              p2i(r-&gt;bottom()),
4450                              r-&gt;rem_set()-&gt;occupied(),
4451                              r-&gt;rem_set()-&gt;strong_code_roots_list_length(),
4452                              next_bitmap-&gt;is_marked(r-&gt;bottom()),
4453                              g1h-&gt;is_humongous_reclaim_candidate(region_idx),
4454                              obj-&gt;is_typeArray()
4455                             );
4456 
4457     G1ConcurrentMark* const cm = g1h-&gt;concurrent_mark();
4458     cm-&gt;humongous_object_eagerly_reclaimed(r);
4459     assert(!cm-&gt;is_marked_in_prev_bitmap(obj) &amp;&amp; !cm-&gt;is_marked_in_next_bitmap(obj),
4460            "Eagerly reclaimed humongous region %u should not be marked at all but is in prev %s next %s",
4461            region_idx,
4462            BOOL_TO_STR(cm-&gt;is_marked_in_prev_bitmap(obj)),
4463            BOOL_TO_STR(cm-&gt;is_marked_in_next_bitmap(obj)));
4464     _humongous_objects_reclaimed++;
4465     do {
4466       HeapRegion* next = g1h-&gt;next_region_in_humongous(r);
4467       _freed_bytes += r-&gt;used();
4468       r-&gt;set_containing_set(NULL);
4469       _humongous_regions_reclaimed++;
4470       g1h-&gt;free_humongous_region(r, _free_region_list);
4471       r = next;
4472     } while (r != NULL);
4473 
4474     return false;
4475   }
4476 
4477   uint humongous_objects_reclaimed() {
4478     return _humongous_objects_reclaimed;
4479   }
4480 
4481   uint humongous_regions_reclaimed() {
4482     return _humongous_regions_reclaimed;
4483   }
4484 
4485   size_t bytes_freed() const {
4486     return _freed_bytes;
4487   }
4488 };
4489 
4490 void G1CollectedHeap::eagerly_reclaim_humongous_regions() {
4491   assert_at_safepoint_on_vm_thread();
4492 
4493   if (!G1EagerReclaimHumongousObjects ||
4494       (!_has_humongous_reclaim_candidates &amp;&amp; !log_is_enabled(Debug, gc, humongous))) {
4495     phase_times()-&gt;record_fast_reclaim_humongous_time_ms(0.0, 0);
4496     return;
4497   }
4498 
4499   double start_time = os::elapsedTime();
4500 
4501   FreeRegionList local_cleanup_list("Local Humongous Cleanup List");
4502 
4503   G1FreeHumongousRegionClosure cl(&amp;local_cleanup_list);
4504   heap_region_iterate(&amp;cl);
4505 
4506   remove_from_old_sets(0, cl.humongous_regions_reclaimed());
4507 
4508   G1HRPrinter* hrp = hr_printer();
4509   if (hrp-&gt;is_active()) {
4510     FreeRegionListIterator iter(&amp;local_cleanup_list);
4511     while (iter.more_available()) {
4512       HeapRegion* hr = iter.get_next();
4513       hrp-&gt;cleanup(hr);
4514     }
4515   }
4516 
4517   prepend_to_freelist(&amp;local_cleanup_list);
4518   decrement_summary_bytes(cl.bytes_freed());
4519 
4520   phase_times()-&gt;record_fast_reclaim_humongous_time_ms((os::elapsedTime() - start_time) * 1000.0,
4521                                                        cl.humongous_objects_reclaimed());
4522 }
4523 
4524 class G1AbandonCollectionSetClosure : public HeapRegionClosure {
4525 public:
4526   virtual bool do_heap_region(HeapRegion* r) {
4527     assert(r-&gt;in_collection_set(), "Region %u must have been in collection set", r-&gt;hrm_index());
4528     G1CollectedHeap::heap()-&gt;clear_region_attr(r);
4529     r-&gt;clear_young_index_in_cset();
4530     return false;
4531   }
4532 };
4533 
4534 void G1CollectedHeap::abandon_collection_set(G1CollectionSet* collection_set) {
4535   G1AbandonCollectionSetClosure cl;
4536   collection_set_iterate_all(&amp;cl);
4537 
4538   collection_set-&gt;clear();
4539   collection_set-&gt;stop_incremental_building();
4540 }
4541 
4542 bool G1CollectedHeap::is_old_gc_alloc_region(HeapRegion* hr) {
4543   return _allocator-&gt;is_retained_old_region(hr);
4544 }
4545 
4546 void G1CollectedHeap::set_region_short_lived_locked(HeapRegion* hr) {
4547   _eden.add(hr);
4548   _policy-&gt;set_region_eden(hr);
4549 }
4550 
4551 #ifdef ASSERT
4552 
4553 class NoYoungRegionsClosure: public HeapRegionClosure {
4554 private:
4555   bool _success;
4556 public:
4557   NoYoungRegionsClosure() : _success(true) { }
4558   bool do_heap_region(HeapRegion* r) {
4559     if (r-&gt;is_young()) {
4560       log_error(gc, verify)("Region [" PTR_FORMAT ", " PTR_FORMAT ") tagged as young",
4561                             p2i(r-&gt;bottom()), p2i(r-&gt;end()));
4562       _success = false;
4563     }
4564     return false;
4565   }
4566   bool success() { return _success; }
4567 };
4568 
4569 bool G1CollectedHeap::check_young_list_empty() {
4570   bool ret = (young_regions_count() == 0);
4571 
4572   NoYoungRegionsClosure closure;
4573   heap_region_iterate(&amp;closure);
4574   ret = ret &amp;&amp; closure.success();
4575 
4576   return ret;
4577 }
4578 
4579 #endif // ASSERT
4580 
4581 class TearDownRegionSetsClosure : public HeapRegionClosure {
4582   HeapRegionSet *_old_set;
4583 
4584 public:
4585   TearDownRegionSetsClosure(HeapRegionSet* old_set) : _old_set(old_set) { }
4586 
4587   bool do_heap_region(HeapRegion* r) {
4588     if (r-&gt;is_old()) {
4589       _old_set-&gt;remove(r);
4590     } else if(r-&gt;is_young()) {
4591       r-&gt;uninstall_surv_rate_group();
4592     } else {
4593       // We ignore free regions, we'll empty the free list afterwards.
4594       // We ignore humongous and archive regions, we're not tearing down these
4595       // sets.
4596       assert(r-&gt;is_archive() || r-&gt;is_free() || r-&gt;is_humongous(),
4597              "it cannot be another type");
4598     }
4599     return false;
4600   }
4601 
4602   ~TearDownRegionSetsClosure() {
4603     assert(_old_set-&gt;is_empty(), "post-condition");
4604   }
4605 };
4606 
4607 void G1CollectedHeap::tear_down_region_sets(bool free_list_only) {
4608   assert_at_safepoint_on_vm_thread();
4609 
4610   if (!free_list_only) {
4611     TearDownRegionSetsClosure cl(&amp;_old_set);
4612     heap_region_iterate(&amp;cl);
4613 
4614     // Note that emptying the _young_list is postponed and instead done as
4615     // the first step when rebuilding the regions sets again. The reason for
4616     // this is that during a full GC string deduplication needs to know if
4617     // a collected region was young or old when the full GC was initiated.
4618   }
4619   _hrm-&gt;remove_all_free_regions();
4620 }
4621 
4622 void G1CollectedHeap::increase_used(size_t bytes) {
4623   _summary_bytes_used += bytes;
4624 }
4625 
4626 void G1CollectedHeap::decrease_used(size_t bytes) {
4627   assert(_summary_bytes_used &gt;= bytes,
4628          "invariant: _summary_bytes_used: " SIZE_FORMAT " should be &gt;= bytes: " SIZE_FORMAT,
4629          _summary_bytes_used, bytes);
4630   _summary_bytes_used -= bytes;
4631 }
4632 
4633 void G1CollectedHeap::set_used(size_t bytes) {
4634   _summary_bytes_used = bytes;
4635 }
4636 
4637 class RebuildRegionSetsClosure : public HeapRegionClosure {
4638 private:
4639   bool _free_list_only;
4640 
4641   HeapRegionSet* _old_set;
4642   HeapRegionManager* _hrm;
4643 
4644   size_t _total_used;
4645 
4646 public:
4647   RebuildRegionSetsClosure(bool free_list_only,
4648                            HeapRegionSet* old_set,
4649                            HeapRegionManager* hrm) :
4650     _free_list_only(free_list_only),
4651     _old_set(old_set), _hrm(hrm), _total_used(0) {
4652     assert(_hrm-&gt;num_free_regions() == 0, "pre-condition");
4653     if (!free_list_only) {
4654       assert(_old_set-&gt;is_empty(), "pre-condition");
4655     }
4656   }
4657 
4658   bool do_heap_region(HeapRegion* r) {
4659     if (r-&gt;is_empty()) {
4660       assert(r-&gt;rem_set()-&gt;is_empty(), "Empty regions should have empty remembered sets.");
4661       // Add free regions to the free list
4662       r-&gt;set_free();
4663       _hrm-&gt;insert_into_free_list(r);
4664     } else if (!_free_list_only) {
4665       assert(r-&gt;rem_set()-&gt;is_empty(), "At this point remembered sets must have been cleared.");
4666 
4667       if (r-&gt;is_archive() || r-&gt;is_humongous()) {
4668         // We ignore archive and humongous regions. We left these sets unchanged.
4669       } else {
4670         assert(r-&gt;is_young() || r-&gt;is_free() || r-&gt;is_old(), "invariant");
4671         // We now move all (non-humongous, non-old, non-archive) regions to old gen, and register them as such.
4672         r-&gt;move_to_old();
4673         _old_set-&gt;add(r);
4674       }
4675       _total_used += r-&gt;used();
4676     }
4677 
4678     return false;
4679   }
4680 
4681   size_t total_used() {
4682     return _total_used;
4683   }
4684 };
4685 
4686 void G1CollectedHeap::rebuild_region_sets(bool free_list_only) {
4687   assert_at_safepoint_on_vm_thread();
4688 
4689   if (!free_list_only) {
4690     _eden.clear();
4691     _survivor.clear();
4692   }
4693 
4694   RebuildRegionSetsClosure cl(free_list_only, &amp;_old_set, _hrm);
4695   heap_region_iterate(&amp;cl);
4696 
4697   if (!free_list_only) {
4698     set_used(cl.total_used());
4699     if (_archive_allocator != NULL) {
4700       _archive_allocator-&gt;clear_used();
4701     }
4702   }
4703   assert_used_and_recalculate_used_equal(this);
4704 }
4705 
4706 // Methods for the mutator alloc region
4707 
4708 HeapRegion* G1CollectedHeap::new_mutator_alloc_region(size_t word_size,
4709                                                       bool force,
4710                                                       uint node_index) {
4711   assert_heap_locked_or_at_safepoint(true /* should_be_vm_thread */);
4712   bool should_allocate = policy()-&gt;should_allocate_mutator_region();
4713   if (force || should_allocate) {
4714     HeapRegion* new_alloc_region = new_region(word_size,
4715                                               HeapRegionType::Eden,
4716                                               false /* do_expand */,
4717                                               node_index);
4718     if (new_alloc_region != NULL) {
4719       set_region_short_lived_locked(new_alloc_region);
4720       _hr_printer.alloc(new_alloc_region, !should_allocate);
4721       _verifier-&gt;check_bitmaps("Mutator Region Allocation", new_alloc_region);
4722       _policy-&gt;remset_tracker()-&gt;update_at_allocate(new_alloc_region);
4723       return new_alloc_region;
4724     }
4725   }
4726   return NULL;
4727 }
4728 
4729 void G1CollectedHeap::retire_mutator_alloc_region(HeapRegion* alloc_region,
4730                                                   size_t allocated_bytes) {
4731   assert_heap_locked_or_at_safepoint(true /* should_be_vm_thread */);
4732   assert(alloc_region-&gt;is_eden(), "all mutator alloc regions should be eden");
4733 
4734   collection_set()-&gt;add_eden_region(alloc_region);
4735   increase_used(allocated_bytes);
4736   _eden.add_used_bytes(allocated_bytes);
4737   _hr_printer.retire(alloc_region);
4738 
4739   // We update the eden sizes here, when the region is retired,
4740   // instead of when it's allocated, since this is the point that its
4741   // used space has been recorded in _summary_bytes_used.
4742   g1mm()-&gt;update_eden_size();
4743 }
4744 
4745 // Methods for the GC alloc regions
4746 
4747 bool G1CollectedHeap::has_more_regions(G1HeapRegionAttr dest) {
4748   if (dest.is_old()) {
4749     return true;
4750   } else {
4751     return survivor_regions_count() &lt; policy()-&gt;max_survivor_regions();
4752   }
4753 }
4754 
4755 HeapRegion* G1CollectedHeap::new_gc_alloc_region(size_t word_size, G1HeapRegionAttr dest, uint node_index) {
4756   assert(FreeList_lock-&gt;owned_by_self(), "pre-condition");
4757 
4758   if (!has_more_regions(dest)) {
4759     return NULL;
4760   }
4761 
4762   HeapRegionType type;
4763   if (dest.is_young()) {
4764     type = HeapRegionType::Survivor;
4765   } else {
4766     type = HeapRegionType::Old;
4767   }
4768 
4769   HeapRegion* new_alloc_region = new_region(word_size,
4770                                             type,
4771                                             true /* do_expand */,
4772                                             node_index);
4773 
4774   if (new_alloc_region != NULL) {
4775     if (type.is_survivor()) {
4776       new_alloc_region-&gt;set_survivor();
4777       _survivor.add(new_alloc_region);
4778       _verifier-&gt;check_bitmaps("Survivor Region Allocation", new_alloc_region);
4779     } else {
4780       new_alloc_region-&gt;set_old();
4781       _verifier-&gt;check_bitmaps("Old Region Allocation", new_alloc_region);
4782     }
4783     _policy-&gt;remset_tracker()-&gt;update_at_allocate(new_alloc_region);
4784     register_region_with_region_attr(new_alloc_region);
4785     _hr_printer.alloc(new_alloc_region);
4786     return new_alloc_region;
4787   }
4788   return NULL;
4789 }
4790 
4791 void G1CollectedHeap::retire_gc_alloc_region(HeapRegion* alloc_region,
4792                                              size_t allocated_bytes,
4793                                              G1HeapRegionAttr dest) {
4794   _bytes_used_during_gc += allocated_bytes;
4795   if (dest.is_old()) {
4796     old_set_add(alloc_region);
4797   } else {
4798     assert(dest.is_young(), "Retiring alloc region should be young (%d)", dest.type());
4799     _survivor.add_used_bytes(allocated_bytes);
4800   }
4801 
4802   bool const during_im = collector_state()-&gt;in_concurrent_start_gc();
4803   if (during_im &amp;&amp; allocated_bytes &gt; 0) {
4804     _cm-&gt;root_regions()-&gt;add(alloc_region-&gt;next_top_at_mark_start(), alloc_region-&gt;top());
4805   }
4806   _hr_printer.retire(alloc_region);
4807 }
4808 
4809 HeapRegion* G1CollectedHeap::alloc_highest_free_region() {
4810   bool expanded = false;
4811   uint index = _hrm-&gt;find_highest_free(&amp;expanded);
4812 
4813   if (index != G1_NO_HRM_INDEX) {
4814     if (expanded) {
4815       log_debug(gc, ergo, heap)("Attempt heap expansion (requested address range outside heap bounds). region size: " SIZE_FORMAT "B",
4816                                 HeapRegion::GrainWords * HeapWordSize);
4817     }
4818     return _hrm-&gt;allocate_free_regions_starting_at(index, 1);
4819   }
4820   return NULL;
4821 }
4822 
4823 // Optimized nmethod scanning
4824 
4825 class RegisterNMethodOopClosure: public OopClosure {
4826   G1CollectedHeap* _g1h;
4827   nmethod* _nm;
4828 
4829   template &lt;class T&gt; void do_oop_work(T* p) {
4830     T heap_oop = RawAccess&lt;&gt;::oop_load(p);
4831     if (!CompressedOops::is_null(heap_oop)) {
4832       oop obj = CompressedOops::decode_not_null(heap_oop);
4833       HeapRegion* hr = _g1h-&gt;heap_region_containing(obj);
4834       assert(!hr-&gt;is_continues_humongous(),
4835              "trying to add code root " PTR_FORMAT " in continuation of humongous region " HR_FORMAT
4836              " starting at " HR_FORMAT,
4837              p2i(_nm), HR_FORMAT_PARAMS(hr), HR_FORMAT_PARAMS(hr-&gt;humongous_start_region()));
4838 
4839       // HeapRegion::add_strong_code_root_locked() avoids adding duplicate entries.
4840       hr-&gt;add_strong_code_root_locked(_nm);
4841     }
4842   }
4843 
4844 public:
4845   RegisterNMethodOopClosure(G1CollectedHeap* g1h, nmethod* nm) :
4846     _g1h(g1h), _nm(nm) {}
4847 
4848   void do_oop(oop* p)       { do_oop_work(p); }
4849   void do_oop(narrowOop* p) { do_oop_work(p); }
4850 };
4851 
4852 class UnregisterNMethodOopClosure: public OopClosure {
4853   G1CollectedHeap* _g1h;
4854   nmethod* _nm;
4855 
4856   template &lt;class T&gt; void do_oop_work(T* p) {
4857     T heap_oop = RawAccess&lt;&gt;::oop_load(p);
4858     if (!CompressedOops::is_null(heap_oop)) {
4859       oop obj = CompressedOops::decode_not_null(heap_oop);
4860       HeapRegion* hr = _g1h-&gt;heap_region_containing(obj);
4861       assert(!hr-&gt;is_continues_humongous(),
4862              "trying to remove code root " PTR_FORMAT " in continuation of humongous region " HR_FORMAT
4863              " starting at " HR_FORMAT,
4864              p2i(_nm), HR_FORMAT_PARAMS(hr), HR_FORMAT_PARAMS(hr-&gt;humongous_start_region()));
4865 
4866       hr-&gt;remove_strong_code_root(_nm);
4867     }
4868   }
4869 
4870 public:
4871   UnregisterNMethodOopClosure(G1CollectedHeap* g1h, nmethod* nm) :
4872     _g1h(g1h), _nm(nm) {}
4873 
4874   void do_oop(oop* p)       { do_oop_work(p); }
4875   void do_oop(narrowOop* p) { do_oop_work(p); }
4876 };
4877 
4878 void G1CollectedHeap::register_nmethod(nmethod* nm) {
4879   guarantee(nm != NULL, "sanity");
4880   RegisterNMethodOopClosure reg_cl(this, nm);
4881   nm-&gt;oops_do(&amp;reg_cl);
4882 }
4883 
4884 void G1CollectedHeap::unregister_nmethod(nmethod* nm) {
4885   guarantee(nm != NULL, "sanity");
4886   UnregisterNMethodOopClosure reg_cl(this, nm);
4887   nm-&gt;oops_do(&amp;reg_cl, true);
4888 }
4889 
4890 void G1CollectedHeap::purge_code_root_memory() {
4891   double purge_start = os::elapsedTime();
4892   G1CodeRootSet::purge();
4893   double purge_time_ms = (os::elapsedTime() - purge_start) * 1000.0;
4894   phase_times()-&gt;record_strong_code_root_purge_time(purge_time_ms);
4895 }
4896 
4897 class RebuildStrongCodeRootClosure: public CodeBlobClosure {
4898   G1CollectedHeap* _g1h;
4899 
4900 public:
4901   RebuildStrongCodeRootClosure(G1CollectedHeap* g1h) :
4902     _g1h(g1h) {}
4903 
4904   void do_code_blob(CodeBlob* cb) {
4905     nmethod* nm = (cb != NULL) ? cb-&gt;as_nmethod_or_null() : NULL;
4906     if (nm == NULL) {
4907       return;
4908     }
4909 
4910     _g1h-&gt;register_nmethod(nm);
4911   }
4912 };
4913 
4914 void G1CollectedHeap::rebuild_strong_code_roots() {
4915   RebuildStrongCodeRootClosure blob_cl(this);
4916   CodeCache::blobs_do(&amp;blob_cl);
4917 }
4918 
4919 void G1CollectedHeap::initialize_serviceability() {
4920   _g1mm-&gt;initialize_serviceability();
4921 }
4922 
4923 MemoryUsage G1CollectedHeap::memory_usage() {
4924   return _g1mm-&gt;memory_usage();
4925 }
4926 
4927 GrowableArray&lt;GCMemoryManager*&gt; G1CollectedHeap::memory_managers() {
4928   return _g1mm-&gt;memory_managers();
4929 }
4930 
4931 GrowableArray&lt;MemoryPool*&gt; G1CollectedHeap::memory_pools() {
4932   return _g1mm-&gt;memory_pools();
4933 }
<a name="2" id="anc2"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="2" type="hidden" /></form></body></html>
