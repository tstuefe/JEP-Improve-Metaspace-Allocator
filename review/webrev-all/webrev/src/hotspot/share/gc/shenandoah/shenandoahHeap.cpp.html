<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>New src/hotspot/share/gc/shenandoah/shenandoahHeap.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 2013, 2020, Red Hat, Inc. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "memory/allocation.hpp"
  27 #include "memory/universe.hpp"
  28 
  29 #include "gc/shared/gcArguments.hpp"
  30 #include "gc/shared/gcTimer.hpp"
  31 #include "gc/shared/gcTraceTime.inline.hpp"
  32 #include "gc/shared/locationPrinter.inline.hpp"
  33 #include "gc/shared/memAllocator.hpp"
  34 #include "gc/shared/plab.hpp"
  35 
  36 #include "gc/shenandoah/shenandoahBarrierSet.hpp"
  37 #include "gc/shenandoah/shenandoahClosures.inline.hpp"
  38 #include "gc/shenandoah/shenandoahCollectionSet.hpp"
  39 #include "gc/shenandoah/shenandoahCollectorPolicy.hpp"
  40 #include "gc/shenandoah/shenandoahConcurrentMark.inline.hpp"
  41 #include "gc/shenandoah/shenandoahConcurrentRoots.hpp"
  42 #include "gc/shenandoah/shenandoahControlThread.hpp"
  43 #include "gc/shenandoah/shenandoahFreeSet.hpp"
  44 #include "gc/shenandoah/shenandoahPhaseTimings.hpp"
  45 #include "gc/shenandoah/shenandoahHeap.inline.hpp"
  46 #include "gc/shenandoah/shenandoahHeapRegion.inline.hpp"
  47 #include "gc/shenandoah/shenandoahHeapRegionSet.hpp"
  48 #include "gc/shenandoah/shenandoahInitLogger.hpp"
  49 #include "gc/shenandoah/shenandoahMarkCompact.hpp"
  50 #include "gc/shenandoah/shenandoahMarkingContext.inline.hpp"
  51 #include "gc/shenandoah/shenandoahMemoryPool.hpp"
  52 #include "gc/shenandoah/shenandoahMetrics.hpp"
  53 #include "gc/shenandoah/shenandoahMonitoringSupport.hpp"
  54 #include "gc/shenandoah/shenandoahOopClosures.inline.hpp"
  55 #include "gc/shenandoah/shenandoahPacer.inline.hpp"
  56 #include "gc/shenandoah/shenandoahPadding.hpp"
  57 #include "gc/shenandoah/shenandoahParallelCleaning.inline.hpp"
  58 #include "gc/shenandoah/shenandoahRootProcessor.inline.hpp"
  59 #include "gc/shenandoah/shenandoahStringDedup.hpp"
  60 #include "gc/shenandoah/shenandoahTaskqueue.hpp"
  61 #include "gc/shenandoah/shenandoahUtils.hpp"
  62 #include "gc/shenandoah/shenandoahVerifier.hpp"
  63 #include "gc/shenandoah/shenandoahCodeRoots.hpp"
  64 #include "gc/shenandoah/shenandoahVMOperations.hpp"
  65 #include "gc/shenandoah/shenandoahWorkGroup.hpp"
  66 #include "gc/shenandoah/shenandoahWorkerPolicy.hpp"
  67 #include "gc/shenandoah/mode/shenandoahIUMode.hpp"
  68 #include "gc/shenandoah/mode/shenandoahPassiveMode.hpp"
  69 #include "gc/shenandoah/mode/shenandoahSATBMode.hpp"
  70 #if INCLUDE_JFR
  71 #include "gc/shenandoah/shenandoahJfrSupport.hpp"
  72 #endif
  73 
  74 #include "memory/metaspace.hpp"
  75 #include "oops/compressedOops.inline.hpp"
  76 #include "runtime/atomic.hpp"
  77 #include "runtime/globals.hpp"
  78 #include "runtime/interfaceSupport.inline.hpp"
  79 #include "runtime/java.hpp"
  80 #include "runtime/orderAccess.hpp"
  81 #include "runtime/safepointMechanism.hpp"
  82 #include "runtime/vmThread.hpp"
  83 #include "services/mallocTracker.hpp"
  84 #include "services/memTracker.hpp"
  85 #include "utilities/powerOfTwo.hpp"
  86 
  87 class ShenandoahPretouchHeapTask : public AbstractGangTask {
  88 private:
  89   ShenandoahRegionIterator _regions;
  90   const size_t _page_size;
  91 public:
  92   ShenandoahPretouchHeapTask(size_t page_size) :
  93     AbstractGangTask("Shenandoah Pretouch Heap"),
  94     _page_size(page_size) {}
  95 
  96   virtual void work(uint worker_id) {
  97     ShenandoahHeapRegion* r = _regions.next();
  98     while (r != NULL) {
  99       if (r-&gt;is_committed()) {
 100         os::pretouch_memory(r-&gt;bottom(), r-&gt;end(), _page_size);
 101       }
 102       r = _regions.next();
 103     }
 104   }
 105 };
 106 
 107 class ShenandoahPretouchBitmapTask : public AbstractGangTask {
 108 private:
 109   ShenandoahRegionIterator _regions;
 110   char* _bitmap_base;
 111   const size_t _bitmap_size;
 112   const size_t _page_size;
 113 public:
 114   ShenandoahPretouchBitmapTask(char* bitmap_base, size_t bitmap_size, size_t page_size) :
 115     AbstractGangTask("Shenandoah Pretouch Bitmap"),
 116     _bitmap_base(bitmap_base),
 117     _bitmap_size(bitmap_size),
 118     _page_size(page_size) {}
 119 
 120   virtual void work(uint worker_id) {
 121     ShenandoahHeapRegion* r = _regions.next();
 122     while (r != NULL) {
 123       size_t start = r-&gt;index()       * ShenandoahHeapRegion::region_size_bytes() / MarkBitMap::heap_map_factor();
 124       size_t end   = (r-&gt;index() + 1) * ShenandoahHeapRegion::region_size_bytes() / MarkBitMap::heap_map_factor();
 125       assert (end &lt;= _bitmap_size, "end is sane: " SIZE_FORMAT " &lt; " SIZE_FORMAT, end, _bitmap_size);
 126 
 127       if (r-&gt;is_committed()) {
 128         os::pretouch_memory(_bitmap_base + start, _bitmap_base + end, _page_size);
 129       }
 130 
 131       r = _regions.next();
 132     }
 133   }
 134 };
 135 
 136 jint ShenandoahHeap::initialize() {
 137   //
 138   // Figure out heap sizing
 139   //
 140 
 141   size_t init_byte_size = InitialHeapSize;
 142   size_t min_byte_size  = MinHeapSize;
 143   size_t max_byte_size  = MaxHeapSize;
 144   size_t heap_alignment = HeapAlignment;
 145 
 146   size_t reg_size_bytes = ShenandoahHeapRegion::region_size_bytes();
 147 
 148   Universe::check_alignment(max_byte_size,  reg_size_bytes, "Shenandoah heap");
 149   Universe::check_alignment(init_byte_size, reg_size_bytes, "Shenandoah heap");
 150 
 151   _num_regions = ShenandoahHeapRegion::region_count();
 152 
 153   // Now we know the number of regions, initialize the heuristics.
 154   initialize_heuristics();
 155 
 156   size_t num_committed_regions = init_byte_size / reg_size_bytes;
 157   num_committed_regions = MIN2(num_committed_regions, _num_regions);
 158   assert(num_committed_regions &lt;= _num_regions, "sanity");
 159   _initial_size = num_committed_regions * reg_size_bytes;
 160 
 161   size_t num_min_regions = min_byte_size / reg_size_bytes;
 162   num_min_regions = MIN2(num_min_regions, _num_regions);
 163   assert(num_min_regions &lt;= _num_regions, "sanity");
 164   _minimum_size = num_min_regions * reg_size_bytes;
 165 
 166   // Default to max heap size.
 167   _soft_max_size = _num_regions * reg_size_bytes;
 168 
 169   _committed = _initial_size;
 170 
 171   size_t heap_page_size   = UseLargePages ? (size_t)os::large_page_size() : (size_t)os::vm_page_size();
 172   size_t bitmap_page_size = UseLargePages ? (size_t)os::large_page_size() : (size_t)os::vm_page_size();
 173   size_t region_page_size = UseLargePages ? (size_t)os::large_page_size() : (size_t)os::vm_page_size();
 174 
 175   //
 176   // Reserve and commit memory for heap
 177   //
 178 
 179   ReservedHeapSpace heap_rs = Universe::reserve_heap(max_byte_size, heap_alignment);
 180   initialize_reserved_region(heap_rs);
 181   _heap_region = MemRegion((HeapWord*)heap_rs.base(), heap_rs.size() / HeapWordSize);
 182   _heap_region_special = heap_rs.special();
 183 
 184   assert((((size_t) base()) &amp; ShenandoahHeapRegion::region_size_bytes_mask()) == 0,
 185          "Misaligned heap: " PTR_FORMAT, p2i(base()));
 186 
 187 #if SHENANDOAH_OPTIMIZED_OBJTASK
 188   // The optimized ObjArrayChunkedTask takes some bits away from the full object bits.
 189   // Fail if we ever attempt to address more than we can.
 190   if ((uintptr_t)heap_rs.end() &gt;= ObjArrayChunkedTask::max_addressable()) {
 191     FormatBuffer&lt;512&gt; buf("Shenandoah reserved [" PTR_FORMAT ", " PTR_FORMAT") for the heap, \n"
 192                           "but max object address is " PTR_FORMAT ". Try to reduce heap size, or try other \n"
 193                           "VM options that allocate heap at lower addresses (HeapBaseMinAddress, AllocateHeapAt, etc).",
 194                 p2i(heap_rs.base()), p2i(heap_rs.end()), ObjArrayChunkedTask::max_addressable());
 195     vm_exit_during_initialization("Fatal Error", buf);
 196   }
 197 #endif
 198 
 199   ReservedSpace sh_rs = heap_rs.first_part(max_byte_size);
 200   if (!_heap_region_special) {
 201     os::commit_memory_or_exit(sh_rs.base(), _initial_size, heap_alignment, false,
 202                               "Cannot commit heap memory");
 203   }
 204 
 205   //
 206   // Reserve and commit memory for bitmap(s)
 207   //
 208 
 209   _bitmap_size = MarkBitMap::compute_size(heap_rs.size());
 210   _bitmap_size = align_up(_bitmap_size, bitmap_page_size);
 211 
 212   size_t bitmap_bytes_per_region = reg_size_bytes / MarkBitMap::heap_map_factor();
 213 
 214   guarantee(bitmap_bytes_per_region != 0,
 215             "Bitmap bytes per region should not be zero");
 216   guarantee(is_power_of_2(bitmap_bytes_per_region),
 217             "Bitmap bytes per region should be power of two: " SIZE_FORMAT, bitmap_bytes_per_region);
 218 
 219   if (bitmap_page_size &gt; bitmap_bytes_per_region) {
 220     _bitmap_regions_per_slice = bitmap_page_size / bitmap_bytes_per_region;
 221     _bitmap_bytes_per_slice = bitmap_page_size;
 222   } else {
 223     _bitmap_regions_per_slice = 1;
 224     _bitmap_bytes_per_slice = bitmap_bytes_per_region;
 225   }
 226 
 227   guarantee(_bitmap_regions_per_slice &gt;= 1,
 228             "Should have at least one region per slice: " SIZE_FORMAT,
 229             _bitmap_regions_per_slice);
 230 
 231   guarantee(((_bitmap_bytes_per_slice) % bitmap_page_size) == 0,
 232             "Bitmap slices should be page-granular: bps = " SIZE_FORMAT ", page size = " SIZE_FORMAT,
 233             _bitmap_bytes_per_slice, bitmap_page_size);
 234 
 235   ReservedSpace bitmap(_bitmap_size, bitmap_page_size);
 236   MemTracker::record_virtual_memory_type(bitmap.base(), mtGC);
 237   _bitmap_region = MemRegion((HeapWord*) bitmap.base(), bitmap.size() / HeapWordSize);
 238   _bitmap_region_special = bitmap.special();
 239 
 240   size_t bitmap_init_commit = _bitmap_bytes_per_slice *
 241                               align_up(num_committed_regions, _bitmap_regions_per_slice) / _bitmap_regions_per_slice;
 242   bitmap_init_commit = MIN2(_bitmap_size, bitmap_init_commit);
 243   if (!_bitmap_region_special) {
 244     os::commit_memory_or_exit((char *) _bitmap_region.start(), bitmap_init_commit, bitmap_page_size, false,
 245                               "Cannot commit bitmap memory");
 246   }
 247 
 248   _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions);
 249 
 250   if (ShenandoahVerify) {
 251     ReservedSpace verify_bitmap(_bitmap_size, bitmap_page_size);
 252     if (!verify_bitmap.special()) {
 253       os::commit_memory_or_exit(verify_bitmap.base(), verify_bitmap.size(), bitmap_page_size, false,
 254                                 "Cannot commit verification bitmap memory");
 255     }
 256     MemTracker::record_virtual_memory_type(verify_bitmap.base(), mtGC);
 257     MemRegion verify_bitmap_region = MemRegion((HeapWord *) verify_bitmap.base(), verify_bitmap.size() / HeapWordSize);
 258     _verification_bit_map.initialize(_heap_region, verify_bitmap_region);
 259     _verifier = new ShenandoahVerifier(this, &amp;_verification_bit_map);
 260   }
 261 
 262   // Reserve aux bitmap for use in object_iterate(). We don't commit it here.
 263   ReservedSpace aux_bitmap(_bitmap_size, bitmap_page_size);
 264   MemTracker::record_virtual_memory_type(aux_bitmap.base(), mtGC);
 265   _aux_bitmap_region = MemRegion((HeapWord*) aux_bitmap.base(), aux_bitmap.size() / HeapWordSize);
 266   _aux_bitmap_region_special = aux_bitmap.special();
 267   _aux_bit_map.initialize(_heap_region, _aux_bitmap_region);
 268 
 269   //
 270   // Create regions and region sets
 271   //
 272   size_t region_align = align_up(sizeof(ShenandoahHeapRegion), SHENANDOAH_CACHE_LINE_SIZE);
 273   size_t region_storage_size = align_up(region_align * _num_regions, region_page_size);
 274   region_storage_size = align_up(region_storage_size, os::vm_allocation_granularity());
 275 
 276   ReservedSpace region_storage(region_storage_size, region_page_size);
 277   MemTracker::record_virtual_memory_type(region_storage.base(), mtGC);
 278   if (!region_storage.special()) {
 279     os::commit_memory_or_exit(region_storage.base(), region_storage_size, region_page_size, false,
 280                               "Cannot commit region memory");
 281   }
 282 
 283   // Try to fit the collection set bitmap at lower addresses. This optimizes code generation for cset checks.
 284   // Go up until a sensible limit (subject to encoding constraints) and try to reserve the space there.
 285   // If not successful, bite a bullet and allocate at whatever address.
 286   {
 287     size_t cset_align = MAX2&lt;size_t&gt;(os::vm_page_size(), os::vm_allocation_granularity());
 288     size_t cset_size = align_up(((size_t) sh_rs.base() + sh_rs.size()) &gt;&gt; ShenandoahHeapRegion::region_size_bytes_shift(), cset_align);
 289 
 290     uintptr_t min = round_up_power_of_2(cset_align);
 291     uintptr_t max = (1u &lt;&lt; 30u);
 292 
 293     for (uintptr_t addr = min; addr &lt;= max; addr &lt;&lt;= 1u) {
 294       char* req_addr = (char*)addr;
 295       assert(is_aligned(req_addr, cset_align), "Should be aligned");
 296       ReservedSpace cset_rs(cset_size, cset_align, false, req_addr);
 297       if (cset_rs.is_reserved()) {
 298         assert(cset_rs.base() == req_addr, "Allocated where requested: " PTR_FORMAT ", " PTR_FORMAT, p2i(cset_rs.base()), addr);
 299         _collection_set = new ShenandoahCollectionSet(this, cset_rs, sh_rs.base());
 300         break;
 301       }
 302     }
 303 
 304     if (_collection_set == NULL) {
 305       ReservedSpace cset_rs(cset_size, cset_align, false);
 306       _collection_set = new ShenandoahCollectionSet(this, cset_rs, sh_rs.base());
 307     }
 308   }
 309 
 310   _regions = NEW_C_HEAP_ARRAY(ShenandoahHeapRegion*, _num_regions, mtGC);
 311   _free_set = new ShenandoahFreeSet(this, _num_regions);
 312 
 313   {
 314     ShenandoahHeapLocker locker(lock());
 315 
 316     for (size_t i = 0; i &lt; _num_regions; i++) {
 317       HeapWord* start = (HeapWord*)sh_rs.base() + ShenandoahHeapRegion::region_size_words() * i;
 318       bool is_committed = i &lt; num_committed_regions;
 319       void* loc = region_storage.base() + i * region_align;
 320 
 321       ShenandoahHeapRegion* r = new (loc) ShenandoahHeapRegion(start, i, is_committed);
 322       assert(is_aligned(r, SHENANDOAH_CACHE_LINE_SIZE), "Sanity");
 323 
 324       _marking_context-&gt;initialize_top_at_mark_start(r);
 325       _regions[i] = r;
 326       assert(!collection_set()-&gt;is_in(i), "New region should not be in collection set");
 327     }
 328 
 329     // Initialize to complete
 330     _marking_context-&gt;mark_complete();
 331 
 332     _free_set-&gt;rebuild();
 333   }
 334 
 335   if (AlwaysPreTouch) {
 336     // For NUMA, it is important to pre-touch the storage under bitmaps with worker threads,
 337     // before initialize() below zeroes it with initializing thread. For any given region,
 338     // we touch the region and the corresponding bitmaps from the same thread.
 339     ShenandoahPushWorkerScope scope(workers(), _max_workers, false);
 340 
 341     _pretouch_heap_page_size = heap_page_size;
 342     _pretouch_bitmap_page_size = bitmap_page_size;
 343 
 344 #ifdef LINUX
 345     // UseTransparentHugePages would madvise that backing memory can be coalesced into huge
 346     // pages. But, the kernel needs to know that every small page is used, in order to coalesce
 347     // them into huge one. Therefore, we need to pretouch with smaller pages.
 348     if (UseTransparentHugePages) {
 349       _pretouch_heap_page_size = (size_t)os::vm_page_size();
 350       _pretouch_bitmap_page_size = (size_t)os::vm_page_size();
 351     }
 352 #endif
 353 
 354     // OS memory managers may want to coalesce back-to-back pages. Make their jobs
 355     // simpler by pre-touching continuous spaces (heap and bitmap) separately.
 356 
 357     ShenandoahPretouchBitmapTask bcl(bitmap.base(), _bitmap_size, _pretouch_bitmap_page_size);
 358     _workers-&gt;run_task(&amp;bcl);
 359 
 360     ShenandoahPretouchHeapTask hcl(_pretouch_heap_page_size);
 361     _workers-&gt;run_task(&amp;hcl);
 362   }
 363 
 364   //
 365   // Initialize the rest of GC subsystems
 366   //
 367 
 368   _liveness_cache = NEW_C_HEAP_ARRAY(ShenandoahLiveData*, _max_workers, mtGC);
 369   for (uint worker = 0; worker &lt; _max_workers; worker++) {
 370     _liveness_cache[worker] = NEW_C_HEAP_ARRAY(ShenandoahLiveData, _num_regions, mtGC);
 371     Copy::fill_to_bytes(_liveness_cache[worker], _num_regions * sizeof(ShenandoahLiveData));
 372   }
 373 
 374   // There should probably be Shenandoah-specific options for these,
 375   // just as there are G1-specific options.
 376   {
 377     ShenandoahSATBMarkQueueSet&amp; satbqs = ShenandoahBarrierSet::satb_mark_queue_set();
 378     satbqs.set_process_completed_buffers_threshold(20); // G1SATBProcessCompletedThreshold
 379     satbqs.set_buffer_enqueue_threshold_percentage(60); // G1SATBBufferEnqueueingThresholdPercent
 380   }
 381 
 382   _monitoring_support = new ShenandoahMonitoringSupport(this);
 383   _phase_timings = new ShenandoahPhaseTimings(max_workers());
 384   ShenandoahStringDedup::initialize();
 385   ShenandoahCodeRoots::initialize();
 386 
 387   if (ShenandoahPacing) {
 388     _pacer = new ShenandoahPacer(this);
 389     _pacer-&gt;setup_for_idle();
 390   } else {
 391     _pacer = NULL;
 392   }
 393 
 394   _control_thread = new ShenandoahControlThread();
 395 
 396   _ref_proc_mt_processing = ParallelRefProcEnabled &amp;&amp; (ParallelGCThreads &gt; 1);
 397   _ref_proc_mt_discovery = _max_workers &gt; 1;
 398 
 399   ShenandoahInitLogger::print();
 400 
 401   return JNI_OK;
 402 }
 403 
 404 void ShenandoahHeap::initialize_heuristics() {
 405   if (ShenandoahGCMode != NULL) {
 406     if (strcmp(ShenandoahGCMode, "satb") == 0) {
 407       _gc_mode = new ShenandoahSATBMode();
 408     } else if (strcmp(ShenandoahGCMode, "iu") == 0) {
 409       _gc_mode = new ShenandoahIUMode();
 410     } else if (strcmp(ShenandoahGCMode, "passive") == 0) {
 411       _gc_mode = new ShenandoahPassiveMode();
 412     } else {
 413       vm_exit_during_initialization("Unknown -XX:ShenandoahGCMode option");
 414     }
 415   } else {
 416     ShouldNotReachHere();
 417   }
 418   _gc_mode-&gt;initialize_flags();
 419   if (_gc_mode-&gt;is_diagnostic() &amp;&amp; !UnlockDiagnosticVMOptions) {
 420     vm_exit_during_initialization(
 421             err_msg("GC mode \"%s\" is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.",
 422                     _gc_mode-&gt;name()));
 423   }
 424   if (_gc_mode-&gt;is_experimental() &amp;&amp; !UnlockExperimentalVMOptions) {
 425     vm_exit_during_initialization(
 426             err_msg("GC mode \"%s\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.",
 427                     _gc_mode-&gt;name()));
 428   }
 429 
 430   _heuristics = _gc_mode-&gt;initialize_heuristics();
 431 
 432   if (_heuristics-&gt;is_diagnostic() &amp;&amp; !UnlockDiagnosticVMOptions) {
 433     vm_exit_during_initialization(
 434             err_msg("Heuristics \"%s\" is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.",
 435                     _heuristics-&gt;name()));
 436   }
 437   if (_heuristics-&gt;is_experimental() &amp;&amp; !UnlockExperimentalVMOptions) {
 438     vm_exit_during_initialization(
 439             err_msg("Heuristics \"%s\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.",
 440                     _heuristics-&gt;name()));
 441   }
 442 }
 443 
 444 #ifdef _MSC_VER
 445 #pragma warning( push )
 446 #pragma warning( disable:4355 ) // 'this' : used in base member initializer list
 447 #endif
 448 
 449 ShenandoahHeap::ShenandoahHeap(ShenandoahCollectorPolicy* policy) :
 450   CollectedHeap(),
 451   _initial_size(0),
 452   _used(0),
 453   _committed(0),
 454   _bytes_allocated_since_gc_start(0),
 455   _max_workers(MAX2(ConcGCThreads, ParallelGCThreads)),
 456   _workers(NULL),
 457   _safepoint_workers(NULL),
 458   _heap_region_special(false),
 459   _num_regions(0),
 460   _regions(NULL),
 461   _update_refs_iterator(this),
 462   _control_thread(NULL),
 463   _shenandoah_policy(policy),
 464   _heuristics(NULL),
 465   _free_set(NULL),
 466   _scm(new ShenandoahConcurrentMark()),
 467   _full_gc(new ShenandoahMarkCompact()),
 468   _pacer(NULL),
 469   _verifier(NULL),
 470   _phase_timings(NULL),
 471   _monitoring_support(NULL),
 472   _memory_pool(NULL),
 473   _stw_memory_manager("Shenandoah Pauses", "end of GC pause"),
 474   _cycle_memory_manager("Shenandoah Cycles", "end of GC cycle"),
 475   _gc_timer(new (ResourceObj::C_HEAP, mtGC) ConcurrentGCTimer()),
 476   _soft_ref_policy(),
 477   _log_min_obj_alignment_in_bytes(LogMinObjAlignmentInBytes),
 478   _ref_processor(NULL),
 479   _marking_context(NULL),
 480   _bitmap_size(0),
 481   _bitmap_regions_per_slice(0),
 482   _bitmap_bytes_per_slice(0),
 483   _bitmap_region_special(false),
 484   _aux_bitmap_region_special(false),
 485   _liveness_cache(NULL),
 486   _collection_set(NULL)
 487 {
 488   BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this));
 489 
 490   _max_workers = MAX2(_max_workers, 1U);
 491   _workers = new ShenandoahWorkGang("Shenandoah GC Threads", _max_workers,
 492                             /* are_GC_task_threads */ true,
 493                             /* are_ConcurrentGC_threads */ true);
 494   if (_workers == NULL) {
 495     vm_exit_during_initialization("Failed necessary allocation.");
 496   } else {
 497     _workers-&gt;initialize_workers();
 498   }
 499 
 500   if (ParallelGCThreads &gt; 1) {
 501     _safepoint_workers = new ShenandoahWorkGang("Safepoint Cleanup Thread",
 502                                                 ParallelGCThreads,
 503                       /* are_GC_task_threads */ false,
 504                  /* are_ConcurrentGC_threads */ false);
 505     _safepoint_workers-&gt;initialize_workers();
 506   }
 507 }
 508 
 509 #ifdef _MSC_VER
 510 #pragma warning( pop )
 511 #endif
 512 
 513 class ShenandoahResetBitmapTask : public AbstractGangTask {
 514 private:
 515   ShenandoahRegionIterator _regions;
 516 
 517 public:
 518   ShenandoahResetBitmapTask() :
 519     AbstractGangTask("Shenandoah Reset Bitmap") {}
 520 
 521   void work(uint worker_id) {
 522     ShenandoahHeapRegion* region = _regions.next();
 523     ShenandoahHeap* heap = ShenandoahHeap::heap();
 524     ShenandoahMarkingContext* const ctx = heap-&gt;marking_context();
 525     while (region != NULL) {
 526       if (heap-&gt;is_bitmap_slice_committed(region)) {
 527         ctx-&gt;clear_bitmap(region);
 528       }
 529       region = _regions.next();
 530     }
 531   }
 532 };
 533 
 534 void ShenandoahHeap::reset_mark_bitmap() {
 535   assert_gc_workers(_workers-&gt;active_workers());
 536   mark_incomplete_marking_context();
 537 
 538   ShenandoahResetBitmapTask task;
 539   _workers-&gt;run_task(&amp;task);
 540 }
 541 
 542 void ShenandoahHeap::print_on(outputStream* st) const {
 543   st-&gt;print_cr("Shenandoah Heap");
 544   st-&gt;print_cr(" " SIZE_FORMAT "%s max, " SIZE_FORMAT "%s soft max, " SIZE_FORMAT "%s committed, " SIZE_FORMAT "%s used",
 545                byte_size_in_proper_unit(max_capacity()), proper_unit_for_byte_size(max_capacity()),
 546                byte_size_in_proper_unit(soft_max_capacity()), proper_unit_for_byte_size(soft_max_capacity()),
 547                byte_size_in_proper_unit(committed()),    proper_unit_for_byte_size(committed()),
 548                byte_size_in_proper_unit(used()),         proper_unit_for_byte_size(used()));
 549   st-&gt;print_cr(" " SIZE_FORMAT " x " SIZE_FORMAT"%s regions",
 550                num_regions(),
 551                byte_size_in_proper_unit(ShenandoahHeapRegion::region_size_bytes()),
 552                proper_unit_for_byte_size(ShenandoahHeapRegion::region_size_bytes()));
 553 
 554   st-&gt;print("Status: ");
 555   if (has_forwarded_objects())                 st-&gt;print("has forwarded objects, ");
 556   if (is_concurrent_mark_in_progress())        st-&gt;print("marking, ");
 557   if (is_evacuation_in_progress())             st-&gt;print("evacuating, ");
 558   if (is_update_refs_in_progress())            st-&gt;print("updating refs, ");
 559   if (is_degenerated_gc_in_progress())         st-&gt;print("degenerated gc, ");
 560   if (is_full_gc_in_progress())                st-&gt;print("full gc, ");
 561   if (is_full_gc_move_in_progress())           st-&gt;print("full gc move, ");
 562   if (is_concurrent_weak_root_in_progress())   st-&gt;print("concurrent weak roots, ");
 563   if (is_concurrent_strong_root_in_progress() &amp;&amp;
 564       !is_concurrent_weak_root_in_progress())  st-&gt;print("concurrent strong roots, ");
 565 
 566   if (cancelled_gc()) {
 567     st-&gt;print("cancelled");
 568   } else {
 569     st-&gt;print("not cancelled");
 570   }
 571   st-&gt;cr();
 572 
 573   st-&gt;print_cr("Reserved region:");
 574   st-&gt;print_cr(" - [" PTR_FORMAT ", " PTR_FORMAT ") ",
 575                p2i(reserved_region().start()),
 576                p2i(reserved_region().end()));
 577 
 578   ShenandoahCollectionSet* cset = collection_set();
 579   st-&gt;print_cr("Collection set:");
 580   if (cset != NULL) {
 581     st-&gt;print_cr(" - map (vanilla): " PTR_FORMAT, p2i(cset-&gt;map_address()));
 582     st-&gt;print_cr(" - map (biased):  " PTR_FORMAT, p2i(cset-&gt;biased_map_address()));
 583   } else {
 584     st-&gt;print_cr(" (NULL)");
 585   }
 586 
 587   st-&gt;cr();
 588   MetaspaceUtils::print_on(st);
 589 
 590   if (Verbose) {
 591     print_heap_regions_on(st);
 592   }
 593 }
 594 
 595 class ShenandoahInitWorkerGCLABClosure : public ThreadClosure {
 596 public:
 597   void do_thread(Thread* thread) {
 598     assert(thread != NULL, "Sanity");
 599     assert(thread-&gt;is_Worker_thread(), "Only worker thread expected");
 600     ShenandoahThreadLocalData::initialize_gclab(thread);
 601   }
 602 };
 603 
 604 void ShenandoahHeap::post_initialize() {
 605   CollectedHeap::post_initialize();
 606   MutexLocker ml(Threads_lock);
 607 
 608   ShenandoahInitWorkerGCLABClosure init_gclabs;
 609   _workers-&gt;threads_do(&amp;init_gclabs);
 610 
 611   // gclab can not be initialized early during VM startup, as it can not determinate its max_size.
 612   // Now, we will let WorkGang to initialize gclab when new worker is created.
 613   _workers-&gt;set_initialize_gclab();
 614 
 615   _scm-&gt;initialize(_max_workers);
 616   _full_gc-&gt;initialize(_gc_timer);
 617 
 618   ref_processing_init();
 619 
 620   _heuristics-&gt;initialize();
 621 
 622   JFR_ONLY(ShenandoahJFRSupport::register_jfr_type_serializers());
 623 }
 624 
 625 size_t ShenandoahHeap::used() const {
 626   return Atomic::load_acquire(&amp;_used);
 627 }
 628 
 629 size_t ShenandoahHeap::committed() const {
 630   OrderAccess::acquire();
 631   return _committed;
 632 }
 633 
 634 void ShenandoahHeap::increase_committed(size_t bytes) {
 635   shenandoah_assert_heaplocked_or_safepoint();
 636   _committed += bytes;
 637 }
 638 
 639 void ShenandoahHeap::decrease_committed(size_t bytes) {
 640   shenandoah_assert_heaplocked_or_safepoint();
 641   _committed -= bytes;
 642 }
 643 
 644 void ShenandoahHeap::increase_used(size_t bytes) {
 645   Atomic::add(&amp;_used, bytes);
 646 }
 647 
 648 void ShenandoahHeap::set_used(size_t bytes) {
 649   Atomic::release_store_fence(&amp;_used, bytes);
 650 }
 651 
 652 void ShenandoahHeap::decrease_used(size_t bytes) {
 653   assert(used() &gt;= bytes, "never decrease heap size by more than we've left");
 654   Atomic::sub(&amp;_used, bytes);
 655 }
 656 
 657 void ShenandoahHeap::increase_allocated(size_t bytes) {
 658   Atomic::add(&amp;_bytes_allocated_since_gc_start, bytes);
 659 }
 660 
 661 void ShenandoahHeap::notify_mutator_alloc_words(size_t words, bool waste) {
 662   size_t bytes = words * HeapWordSize;
 663   if (!waste) {
 664     increase_used(bytes);
 665   }
 666   increase_allocated(bytes);
 667   if (ShenandoahPacing) {
 668     control_thread()-&gt;pacing_notify_alloc(words);
 669     if (waste) {
 670       pacer()-&gt;claim_for_alloc(words, true);
 671     }
 672   }
 673 }
 674 
 675 size_t ShenandoahHeap::capacity() const {
 676   return committed();
 677 }
 678 
 679 size_t ShenandoahHeap::max_capacity() const {
 680   return _num_regions * ShenandoahHeapRegion::region_size_bytes();
 681 }
 682 
 683 size_t ShenandoahHeap::soft_max_capacity() const {
 684   size_t v = Atomic::load(&amp;_soft_max_size);
 685   assert(min_capacity() &lt;= v &amp;&amp; v &lt;= max_capacity(),
 686          "Should be in bounds: " SIZE_FORMAT " &lt;= " SIZE_FORMAT " &lt;= " SIZE_FORMAT,
 687          min_capacity(), v, max_capacity());
 688   return v;
 689 }
 690 
 691 void ShenandoahHeap::set_soft_max_capacity(size_t v) {
 692   assert(min_capacity() &lt;= v &amp;&amp; v &lt;= max_capacity(),
 693          "Should be in bounds: " SIZE_FORMAT " &lt;= " SIZE_FORMAT " &lt;= " SIZE_FORMAT,
 694          min_capacity(), v, max_capacity());
 695   Atomic::store(&amp;_soft_max_size, v);
 696 }
 697 
 698 size_t ShenandoahHeap::min_capacity() const {
 699   return _minimum_size;
 700 }
 701 
 702 size_t ShenandoahHeap::initial_capacity() const {
 703   return _initial_size;
 704 }
 705 
 706 bool ShenandoahHeap::is_in(const void* p) const {
 707   HeapWord* heap_base = (HeapWord*) base();
 708   HeapWord* last_region_end = heap_base + ShenandoahHeapRegion::region_size_words() * num_regions();
 709   return p &gt;= heap_base &amp;&amp; p &lt; last_region_end;
 710 }
 711 
 712 void ShenandoahHeap::op_uncommit(double shrink_before, size_t shrink_until) {
 713   assert (ShenandoahUncommit, "should be enabled");
 714 
 715   // Application allocates from the beginning of the heap, and GC allocates at
 716   // the end of it. It is more efficient to uncommit from the end, so that applications
 717   // could enjoy the near committed regions. GC allocations are much less frequent,
 718   // and therefore can accept the committing costs.
 719 
 720   size_t count = 0;
 721   for (size_t i = num_regions(); i &gt; 0; i--) { // care about size_t underflow
 722     ShenandoahHeapRegion* r = get_region(i - 1);
 723     if (r-&gt;is_empty_committed() &amp;&amp; (r-&gt;empty_time() &lt; shrink_before)) {
 724       ShenandoahHeapLocker locker(lock());
 725       if (r-&gt;is_empty_committed()) {
 726         if (committed() &lt; shrink_until + ShenandoahHeapRegion::region_size_bytes()) {
 727           break;
 728         }
 729 
 730         r-&gt;make_uncommitted();
 731         count++;
 732       }
 733     }
 734     SpinPause(); // allow allocators to take the lock
 735   }
 736 
 737   if (count &gt; 0) {
 738     control_thread()-&gt;notify_heap_changed();
 739   }
 740 }
 741 
 742 HeapWord* ShenandoahHeap::allocate_from_gclab_slow(Thread* thread, size_t size) {
 743   // New object should fit the GCLAB size
 744   size_t min_size = MAX2(size, PLAB::min_size());
 745 
 746   // Figure out size of new GCLAB, looking back at heuristics. Expand aggressively.
 747   size_t new_size = ShenandoahThreadLocalData::gclab_size(thread) * 2;
 748   new_size = MIN2(new_size, PLAB::max_size());
 749   new_size = MAX2(new_size, PLAB::min_size());
 750 
 751   // Record new heuristic value even if we take any shortcut. This captures
 752   // the case when moderately-sized objects always take a shortcut. At some point,
 753   // heuristics should catch up with them.
 754   ShenandoahThreadLocalData::set_gclab_size(thread, new_size);
 755 
 756   if (new_size &lt; size) {
 757     // New size still does not fit the object. Fall back to shared allocation.
 758     // This avoids retiring perfectly good GCLABs, when we encounter a large object.
 759     return NULL;
 760   }
 761 
 762   // Retire current GCLAB, and allocate a new one.
 763   PLAB* gclab = ShenandoahThreadLocalData::gclab(thread);
 764   gclab-&gt;retire();
 765 
 766   size_t actual_size = 0;
 767   HeapWord* gclab_buf = allocate_new_gclab(min_size, new_size, &amp;actual_size);
 768   if (gclab_buf == NULL) {
 769     return NULL;
 770   }
 771 
 772   assert (size &lt;= actual_size, "allocation should fit");
 773 
 774   if (ZeroTLAB) {
 775     // ..and clear it.
 776     Copy::zero_to_words(gclab_buf, actual_size);
 777   } else {
 778     // ...and zap just allocated object.
 779 #ifdef ASSERT
 780     // Skip mangling the space corresponding to the object header to
 781     // ensure that the returned space is not considered parsable by
 782     // any concurrent GC thread.
 783     size_t hdr_size = oopDesc::header_size();
 784     Copy::fill_to_words(gclab_buf + hdr_size, actual_size - hdr_size, badHeapWordVal);
 785 #endif // ASSERT
 786   }
 787   gclab-&gt;set_buf(gclab_buf, actual_size);
 788   return gclab-&gt;allocate(size);
 789 }
 790 
 791 HeapWord* ShenandoahHeap::allocate_new_tlab(size_t min_size,
 792                                             size_t requested_size,
 793                                             size_t* actual_size) {
 794   ShenandoahAllocRequest req = ShenandoahAllocRequest::for_tlab(min_size, requested_size);
 795   HeapWord* res = allocate_memory(req);
 796   if (res != NULL) {
 797     *actual_size = req.actual_size();
 798   } else {
 799     *actual_size = 0;
 800   }
 801   return res;
 802 }
 803 
 804 HeapWord* ShenandoahHeap::allocate_new_gclab(size_t min_size,
 805                                              size_t word_size,
 806                                              size_t* actual_size) {
 807   ShenandoahAllocRequest req = ShenandoahAllocRequest::for_gclab(min_size, word_size);
 808   HeapWord* res = allocate_memory(req);
 809   if (res != NULL) {
 810     *actual_size = req.actual_size();
 811   } else {
 812     *actual_size = 0;
 813   }
 814   return res;
 815 }
 816 
 817 HeapWord* ShenandoahHeap::allocate_memory(ShenandoahAllocRequest&amp; req) {
 818   intptr_t pacer_epoch = 0;
 819   bool in_new_region = false;
 820   HeapWord* result = NULL;
 821 
 822   if (req.is_mutator_alloc()) {
 823     if (ShenandoahPacing) {
 824       pacer()-&gt;pace_for_alloc(req.size());
 825       pacer_epoch = pacer()-&gt;epoch();
 826     }
 827 
 828     if (!ShenandoahAllocFailureALot || !should_inject_alloc_failure()) {
 829       result = allocate_memory_under_lock(req, in_new_region);
 830     }
 831 
 832     // Allocation failed, block until control thread reacted, then retry allocation.
 833     //
 834     // It might happen that one of the threads requesting allocation would unblock
 835     // way later after GC happened, only to fail the second allocation, because
 836     // other threads have already depleted the free storage. In this case, a better
 837     // strategy is to try again, as long as GC makes progress.
 838     //
 839     // Then, we need to make sure the allocation was retried after at least one
 840     // Full GC, which means we want to try more than ShenandoahFullGCThreshold times.
 841 
 842     size_t tries = 0;
 843 
 844     while (result == NULL &amp;&amp; _progress_last_gc.is_set()) {
 845       tries++;
 846       control_thread()-&gt;handle_alloc_failure(req);
 847       result = allocate_memory_under_lock(req, in_new_region);
 848     }
 849 
 850     while (result == NULL &amp;&amp; tries &lt;= ShenandoahFullGCThreshold) {
 851       tries++;
 852       control_thread()-&gt;handle_alloc_failure(req);
 853       result = allocate_memory_under_lock(req, in_new_region);
 854     }
 855 
 856   } else {
 857     assert(req.is_gc_alloc(), "Can only accept GC allocs here");
 858     result = allocate_memory_under_lock(req, in_new_region);
 859     // Do not call handle_alloc_failure() here, because we cannot block.
 860     // The allocation failure would be handled by the LRB slowpath with handle_alloc_failure_evac().
 861   }
 862 
 863   if (in_new_region) {
 864     control_thread()-&gt;notify_heap_changed();
 865   }
 866 
 867   if (result != NULL) {
 868     size_t requested = req.size();
 869     size_t actual = req.actual_size();
 870 
 871     assert (req.is_lab_alloc() || (requested == actual),
 872             "Only LAB allocations are elastic: %s, requested = " SIZE_FORMAT ", actual = " SIZE_FORMAT,
 873             ShenandoahAllocRequest::alloc_type_to_string(req.type()), requested, actual);
 874 
 875     if (req.is_mutator_alloc()) {
 876       notify_mutator_alloc_words(actual, false);
 877 
 878       // If we requested more than we were granted, give the rest back to pacer.
 879       // This only matters if we are in the same pacing epoch: do not try to unpace
 880       // over the budget for the other phase.
 881       if (ShenandoahPacing &amp;&amp; (pacer_epoch &gt; 0) &amp;&amp; (requested &gt; actual)) {
 882         pacer()-&gt;unpace_for_alloc(pacer_epoch, requested - actual);
 883       }
 884     } else {
 885       increase_used(actual*HeapWordSize);
 886     }
 887   }
 888 
 889   return result;
 890 }
 891 
 892 HeapWord* ShenandoahHeap::allocate_memory_under_lock(ShenandoahAllocRequest&amp; req, bool&amp; in_new_region) {
 893   ShenandoahHeapLocker locker(lock());
 894   return _free_set-&gt;allocate(req, in_new_region);
 895 }
 896 
 897 HeapWord* ShenandoahHeap::mem_allocate(size_t size,
 898                                         bool*  gc_overhead_limit_was_exceeded) {
 899   ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared(size);
 900   return allocate_memory(req);
 901 }
 902 
 903 MetaWord* ShenandoahHeap::satisfy_failed_metadata_allocation(ClassLoaderData* loader_data,
 904                                                              size_t size,
 905                                                              Metaspace::MetadataType mdtype) {
 906   MetaWord* result;
 907 
 908   // Inform metaspace OOM to GC heuristics if class unloading is possible.
 909   if (heuristics()-&gt;can_unload_classes()) {
 910     ShenandoahHeuristics* h = heuristics();
 911     h-&gt;record_metaspace_oom();
 912   }
 913 
 914   // Expand and retry allocation
 915   result = loader_data-&gt;metaspace_non_null()-&gt;expand_and_allocate(size, mdtype);
 916   if (result != NULL) {
 917     return result;
 918   }
 919 
 920   // Start full GC
 921   collect(GCCause::_metadata_GC_clear_soft_refs);
 922 
 923   // Retry allocation
 924   result = loader_data-&gt;metaspace_non_null()-&gt;allocate(size, mdtype);
 925   if (result != NULL) {
 926     return result;
 927   }
 928 
 929   // Expand and retry allocation
 930   result = loader_data-&gt;metaspace_non_null()-&gt;expand_and_allocate(size, mdtype);
 931   if (result != NULL) {
 932     return result;
 933   }
 934 
 935   // Out of memory
 936   return NULL;
 937 }
 938 
 939 class ShenandoahConcurrentEvacuateRegionObjectClosure : public ObjectClosure {
 940 private:
 941   ShenandoahHeap* const _heap;
 942   Thread* const _thread;
 943 public:
 944   ShenandoahConcurrentEvacuateRegionObjectClosure(ShenandoahHeap* heap) :
 945     _heap(heap), _thread(Thread::current()) {}
 946 
 947   void do_object(oop p) {
 948     shenandoah_assert_marked(NULL, p);
 949     if (!p-&gt;is_forwarded()) {
 950       _heap-&gt;evacuate_object(p, _thread);
 951     }
 952   }
 953 };
 954 
 955 class ShenandoahEvacuationTask : public AbstractGangTask {
 956 private:
 957   ShenandoahHeap* const _sh;
 958   ShenandoahCollectionSet* const _cs;
 959   bool _concurrent;
 960 public:
 961   ShenandoahEvacuationTask(ShenandoahHeap* sh,
 962                            ShenandoahCollectionSet* cs,
 963                            bool concurrent) :
 964     AbstractGangTask("Shenandoah Evacuation"),
 965     _sh(sh),
 966     _cs(cs),
 967     _concurrent(concurrent)
 968   {}
 969 
 970   void work(uint worker_id) {
 971     if (_concurrent) {
 972       ShenandoahConcurrentWorkerSession worker_session(worker_id);
 973       ShenandoahSuspendibleThreadSetJoiner stsj(ShenandoahSuspendibleWorkers);
 974       ShenandoahEvacOOMScope oom_evac_scope;
 975       do_work();
 976     } else {
 977       ShenandoahParallelWorkerSession worker_session(worker_id);
 978       ShenandoahEvacOOMScope oom_evac_scope;
 979       do_work();
 980     }
 981   }
 982 
 983 private:
 984   void do_work() {
 985     ShenandoahConcurrentEvacuateRegionObjectClosure cl(_sh);
 986     ShenandoahHeapRegion* r;
 987     while ((r =_cs-&gt;claim_next()) != NULL) {
 988       assert(r-&gt;has_live(), "Region " SIZE_FORMAT " should have been reclaimed early", r-&gt;index());
 989       _sh-&gt;marked_object_iterate(r, &amp;cl);
 990 
 991       if (ShenandoahPacing) {
 992         _sh-&gt;pacer()-&gt;report_evac(r-&gt;used() &gt;&gt; LogHeapWordSize);
 993       }
 994 
 995       if (_sh-&gt;check_cancelled_gc_and_yield(_concurrent)) {
 996         break;
 997       }
 998     }
 999   }
1000 };
1001 
1002 void ShenandoahHeap::trash_cset_regions() {
1003   ShenandoahHeapLocker locker(lock());
1004 
1005   ShenandoahCollectionSet* set = collection_set();
1006   ShenandoahHeapRegion* r;
1007   set-&gt;clear_current_index();
1008   while ((r = set-&gt;next()) != NULL) {
1009     r-&gt;make_trash();
1010   }
1011   collection_set()-&gt;clear();
1012 }
1013 
1014 void ShenandoahHeap::print_heap_regions_on(outputStream* st) const {
1015   st-&gt;print_cr("Heap Regions:");
1016   st-&gt;print_cr("EU=empty-uncommitted, EC=empty-committed, R=regular, H=humongous start, HC=humongous continuation, CS=collection set, T=trash, P=pinned");
1017   st-&gt;print_cr("BTE=bottom/top/end, U=used, T=TLAB allocs, G=GCLAB allocs, S=shared allocs, L=live data");
1018   st-&gt;print_cr("R=root, CP=critical pins, TAMS=top-at-mark-start, UWM=update watermark");
1019   st-&gt;print_cr("SN=alloc sequence number");
1020 
1021   for (size_t i = 0; i &lt; num_regions(); i++) {
1022     get_region(i)-&gt;print_on(st);
1023   }
1024 }
1025 
1026 void ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {
1027   assert(start-&gt;is_humongous_start(), "reclaim regions starting with the first one");
1028 
1029   oop humongous_obj = oop(start-&gt;bottom());
1030   size_t size = humongous_obj-&gt;size();
1031   size_t required_regions = ShenandoahHeapRegion::required_regions(size * HeapWordSize);
1032   size_t index = start-&gt;index() + required_regions - 1;
1033 
1034   assert(!start-&gt;has_live(), "liveness must be zero");
1035 
1036   for(size_t i = 0; i &lt; required_regions; i++) {
1037     // Reclaim from tail. Otherwise, assertion fails when printing region to trace log,
1038     // as it expects that every region belongs to a humongous region starting with a humongous start region.
1039     ShenandoahHeapRegion* region = get_region(index --);
1040 
1041     assert(region-&gt;is_humongous(), "expect correct humongous start or continuation");
1042     assert(!region-&gt;is_cset(), "Humongous region should not be in collection set");
1043 
1044     region-&gt;make_trash_immediate();
1045   }
1046 }
1047 
1048 class ShenandoahCheckCleanGCLABClosure : public ThreadClosure {
1049 public:
1050   ShenandoahCheckCleanGCLABClosure() {}
1051   void do_thread(Thread* thread) {
1052     PLAB* gclab = ShenandoahThreadLocalData::gclab(thread);
1053     assert(gclab != NULL, "GCLAB should be initialized for %s", thread-&gt;name());
1054     assert(gclab-&gt;words_remaining() == 0, "GCLAB should not need retirement");
1055   }
1056 };
1057 
1058 class ShenandoahRetireGCLABClosure : public ThreadClosure {
1059 private:
1060   bool const _resize;
1061 public:
1062   ShenandoahRetireGCLABClosure(bool resize) : _resize(resize) {}
1063   void do_thread(Thread* thread) {
1064     PLAB* gclab = ShenandoahThreadLocalData::gclab(thread);
1065     assert(gclab != NULL, "GCLAB should be initialized for %s", thread-&gt;name());
1066     gclab-&gt;retire();
1067     if (_resize &amp;&amp; ShenandoahThreadLocalData::gclab_size(thread) &gt; 0) {
1068       ShenandoahThreadLocalData::set_gclab_size(thread, 0);
1069     }
1070   }
1071 };
1072 
1073 void ShenandoahHeap::labs_make_parsable() {
1074   assert(UseTLAB, "Only call with UseTLAB");
1075 
1076   ShenandoahRetireGCLABClosure cl(false);
1077 
1078   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {
1079     ThreadLocalAllocBuffer&amp; tlab = t-&gt;tlab();
1080     tlab.make_parsable();
1081     cl.do_thread(t);
1082   }
1083 
1084   workers()-&gt;threads_do(&amp;cl);
1085 }
1086 
1087 void ShenandoahHeap::tlabs_retire(bool resize) {
1088   assert(UseTLAB, "Only call with UseTLAB");
1089   assert(!resize || ResizeTLAB, "Only call for resize when ResizeTLAB is enabled");
1090 
1091   ThreadLocalAllocStats stats;
1092 
1093   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {
1094     ThreadLocalAllocBuffer&amp; tlab = t-&gt;tlab();
1095     tlab.retire(&amp;stats);
1096     if (resize) {
1097       tlab.resize();
1098     }
1099   }
1100 
1101   stats.publish();
1102 
1103 #ifdef ASSERT
1104   ShenandoahCheckCleanGCLABClosure cl;
1105   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {
1106     cl.do_thread(t);
1107   }
1108   workers()-&gt;threads_do(&amp;cl);
1109 #endif
1110 }
1111 
1112 void ShenandoahHeap::gclabs_retire(bool resize) {
1113   assert(UseTLAB, "Only call with UseTLAB");
1114   assert(!resize || ResizeTLAB, "Only call for resize when ResizeTLAB is enabled");
1115 
1116   ShenandoahRetireGCLABClosure cl(resize);
1117   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {
1118     cl.do_thread(t);
1119   }
1120   workers()-&gt;threads_do(&amp;cl);
1121 }
1122 
1123 class ShenandoahEvacuateUpdateRootsTask : public AbstractGangTask {
1124 private:
1125   ShenandoahRootEvacuator* _rp;
1126 
1127 public:
1128   ShenandoahEvacuateUpdateRootsTask(ShenandoahRootEvacuator* rp) :
1129     AbstractGangTask("Shenandoah Evacuate/Update Roots"),
1130     _rp(rp) {}
1131 
1132   void work(uint worker_id) {
1133     ShenandoahParallelWorkerSession worker_session(worker_id);
1134     ShenandoahEvacOOMScope oom_evac_scope;
1135     ShenandoahEvacuateUpdateRootsClosure&lt;&gt; cl;
1136     MarkingCodeBlobClosure blobsCl(&amp;cl, CodeBlobToOopClosure::FixRelocations);
1137     _rp-&gt;roots_do(worker_id, &amp;cl);
1138   }
1139 };
1140 
1141 void ShenandoahHeap::evacuate_and_update_roots() {
1142 #if COMPILER2_OR_JVMCI
1143   DerivedPointerTable::clear();
1144 #endif
1145   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), "Only iterate roots while world is stopped");
1146   {
1147     // Include concurrent roots if current cycle can not process those roots concurrently
1148     ShenandoahRootEvacuator rp(workers()-&gt;active_workers(),
1149                                ShenandoahPhaseTimings::init_evac,
1150                                !ShenandoahConcurrentRoots::should_do_concurrent_roots(),
1151                                !ShenandoahConcurrentRoots::should_do_concurrent_class_unloading());
1152     ShenandoahEvacuateUpdateRootsTask roots_task(&amp;rp);
1153     workers()-&gt;run_task(&amp;roots_task);
1154   }
1155 
1156 #if COMPILER2_OR_JVMCI
1157   DerivedPointerTable::update_pointers();
1158 #endif
1159 }
1160 
1161 // Returns size in bytes
1162 size_t ShenandoahHeap::unsafe_max_tlab_alloc(Thread *thread) const {
1163   if (ShenandoahElasticTLAB) {
1164     // With Elastic TLABs, return the max allowed size, and let the allocation path
1165     // figure out the safe size for current allocation.
1166     return ShenandoahHeapRegion::max_tlab_size_bytes();
1167   } else {
1168     return MIN2(_free_set-&gt;unsafe_peek_free(), ShenandoahHeapRegion::max_tlab_size_bytes());
1169   }
1170 }
1171 
1172 size_t ShenandoahHeap::max_tlab_size() const {
1173   // Returns size in words
1174   return ShenandoahHeapRegion::max_tlab_size_words();
1175 }
1176 
1177 void ShenandoahHeap::collect(GCCause::Cause cause) {
1178   control_thread()-&gt;request_gc(cause);
1179 }
1180 
1181 void ShenandoahHeap::do_full_collection(bool clear_all_soft_refs) {
1182   //assert(false, "Shouldn't need to do full collections");
1183 }
1184 
1185 HeapWord* ShenandoahHeap::block_start(const void* addr) const {
1186   ShenandoahHeapRegion* r = heap_region_containing(addr);
1187   if (r != NULL) {
1188     return r-&gt;block_start(addr);
1189   }
1190   return NULL;
1191 }
1192 
1193 bool ShenandoahHeap::block_is_obj(const HeapWord* addr) const {
1194   ShenandoahHeapRegion* r = heap_region_containing(addr);
1195   return r-&gt;block_is_obj(addr);
1196 }
1197 
1198 bool ShenandoahHeap::print_location(outputStream* st, void* addr) const {
1199   return BlockLocationPrinter&lt;ShenandoahHeap&gt;::print_location(st, addr);
1200 }
1201 
1202 void ShenandoahHeap::prepare_for_verify() {
1203   if (SafepointSynchronize::is_at_safepoint() &amp;&amp; UseTLAB) {
1204     labs_make_parsable();
1205   }
1206 }
1207 
1208 void ShenandoahHeap::gc_threads_do(ThreadClosure* tcl) const {
1209   workers()-&gt;threads_do(tcl);
1210   if (_safepoint_workers != NULL) {
1211     _safepoint_workers-&gt;threads_do(tcl);
1212   }
1213   if (ShenandoahStringDedup::is_enabled()) {
1214     ShenandoahStringDedup::threads_do(tcl);
1215   }
1216 }
1217 
1218 void ShenandoahHeap::print_tracing_info() const {
1219   LogTarget(Info, gc, stats) lt;
1220   if (lt.is_enabled()) {
1221     ResourceMark rm;
1222     LogStream ls(lt);
1223 
1224     phase_timings()-&gt;print_global_on(&amp;ls);
1225 
1226     ls.cr();
1227     ls.cr();
1228 
1229     shenandoah_policy()-&gt;print_gc_stats(&amp;ls);
1230 
1231     ls.cr();
1232     ls.cr();
1233   }
1234 }
1235 
1236 void ShenandoahHeap::verify(VerifyOption vo) {
1237   if (ShenandoahSafepoint::is_at_shenandoah_safepoint()) {
1238     if (ShenandoahVerify) {
1239       verifier()-&gt;verify_generic(vo);
1240     } else {
1241       // TODO: Consider allocating verification bitmaps on demand,
1242       // and turn this on unconditionally.
1243     }
1244   }
1245 }
1246 size_t ShenandoahHeap::tlab_capacity(Thread *thr) const {
1247   return _free_set-&gt;capacity();
1248 }
1249 
1250 class ObjectIterateScanRootClosure : public BasicOopIterateClosure {
1251 private:
1252   MarkBitMap* _bitmap;
1253   Stack&lt;oop,mtGC&gt;* _oop_stack;
1254   ShenandoahHeap* const _heap;
1255   ShenandoahMarkingContext* const _marking_context;
1256 
1257   template &lt;class T&gt;
1258   void do_oop_work(T* p) {
1259     T o = RawAccess&lt;&gt;::oop_load(p);
1260     if (!CompressedOops::is_null(o)) {
1261       oop obj = CompressedOops::decode_not_null(o);
1262       if (_heap-&gt;is_concurrent_weak_root_in_progress() &amp;&amp; !_marking_context-&gt;is_marked(obj)) {
1263         // There may be dead oops in weak roots in concurrent root phase, do not touch them.
1264         return;
1265       }
1266       obj = ShenandoahBarrierSet::resolve_forwarded_not_null(obj);
1267 
1268       assert(oopDesc::is_oop(obj), "must be a valid oop");
1269       if (!_bitmap-&gt;is_marked(obj)) {
1270         _bitmap-&gt;mark(obj);
1271         _oop_stack-&gt;push(obj);
1272       }
1273     }
1274   }
1275 public:
1276   ObjectIterateScanRootClosure(MarkBitMap* bitmap, Stack&lt;oop,mtGC&gt;* oop_stack) :
1277     _bitmap(bitmap), _oop_stack(oop_stack), _heap(ShenandoahHeap::heap()),
1278     _marking_context(_heap-&gt;marking_context()) {}
1279   void do_oop(oop* p)       { do_oop_work(p); }
1280   void do_oop(narrowOop* p) { do_oop_work(p); }
1281 };
1282 
1283 /*
1284  * This is public API, used in preparation of object_iterate().
1285  * Since we don't do linear scan of heap in object_iterate() (see comment below), we don't
1286  * need to make the heap parsable. For Shenandoah-internal linear heap scans that we can
1287  * control, we call SH::tlabs_retire, SH::gclabs_retire.
1288  */
1289 void ShenandoahHeap::ensure_parsability(bool retire_tlabs) {
1290   // No-op.
1291 }
1292 
1293 /*
1294  * Iterates objects in the heap. This is public API, used for, e.g., heap dumping.
1295  *
1296  * We cannot safely iterate objects by doing a linear scan at random points in time. Linear
1297  * scanning needs to deal with dead objects, which may have dead Klass* pointers (e.g.
1298  * calling oopDesc::size() would crash) or dangling reference fields (crashes) etc. Linear
1299  * scanning therefore depends on having a valid marking bitmap to support it. However, we only
1300  * have a valid marking bitmap after successful marking. In particular, we *don't* have a valid
1301  * marking bitmap during marking, after aborted marking or during/after cleanup (when we just
1302  * wiped the bitmap in preparation for next marking).
1303  *
1304  * For all those reasons, we implement object iteration as a single marking traversal, reporting
1305  * objects as we mark+traverse through the heap, starting from GC roots. JVMTI IterateThroughHeap
1306  * is allowed to report dead objects, but is not required to do so.
1307  */
1308 void ShenandoahHeap::object_iterate(ObjectClosure* cl) {
1309   assert(SafepointSynchronize::is_at_safepoint(), "safe iteration is only available during safepoints");
1310   if (!_aux_bitmap_region_special &amp;&amp; !os::commit_memory((char*)_aux_bitmap_region.start(), _aux_bitmap_region.byte_size(), false)) {
1311     log_warning(gc)("Could not commit native memory for auxiliary marking bitmap for heap iteration");
1312     return;
1313   }
1314 
1315   // Reset bitmap
1316   _aux_bit_map.clear();
1317 
1318   Stack&lt;oop,mtGC&gt; oop_stack;
1319 
1320   ObjectIterateScanRootClosure oops(&amp;_aux_bit_map, &amp;oop_stack);
1321 
1322   {
1323     // First, we process GC roots according to current GC cycle.
1324     // This populates the work stack with initial objects.
1325     // It is important to relinquish the associated locks before diving
1326     // into heap dumper.
1327     ShenandoahHeapIterationRootScanner rp;
1328     rp.roots_do(&amp;oops);
1329   }
1330 
1331   // Work through the oop stack to traverse heap.
1332   while (! oop_stack.is_empty()) {
1333     oop obj = oop_stack.pop();
1334     assert(oopDesc::is_oop(obj), "must be a valid oop");
1335     cl-&gt;do_object(obj);
1336     obj-&gt;oop_iterate(&amp;oops);
1337   }
1338 
1339   assert(oop_stack.is_empty(), "should be empty");
1340 
1341   if (!_aux_bitmap_region_special &amp;&amp; !os::uncommit_memory((char*)_aux_bitmap_region.start(), _aux_bitmap_region.byte_size())) {
1342     log_warning(gc)("Could not uncommit native memory for auxiliary marking bitmap for heap iteration");
1343   }
1344 }
1345 
1346 // Keep alive an object that was loaded with AS_NO_KEEPALIVE.
1347 void ShenandoahHeap::keep_alive(oop obj) {
1348   if (is_concurrent_mark_in_progress() &amp;&amp; (obj != NULL)) {
1349     ShenandoahBarrierSet::barrier_set()-&gt;enqueue(obj);
1350   }
1351 }
1352 
1353 void ShenandoahHeap::heap_region_iterate(ShenandoahHeapRegionClosure* blk) const {
1354   for (size_t i = 0; i &lt; num_regions(); i++) {
1355     ShenandoahHeapRegion* current = get_region(i);
1356     blk-&gt;heap_region_do(current);
1357   }
1358 }
1359 
1360 class ShenandoahParallelHeapRegionTask : public AbstractGangTask {
1361 private:
1362   ShenandoahHeap* const _heap;
1363   ShenandoahHeapRegionClosure* const _blk;
1364 
1365   shenandoah_padding(0);
1366   volatile size_t _index;
1367   shenandoah_padding(1);
1368 
1369 public:
1370   ShenandoahParallelHeapRegionTask(ShenandoahHeapRegionClosure* blk) :
1371           AbstractGangTask("Shenandoah Parallel Region Operation"),
1372           _heap(ShenandoahHeap::heap()), _blk(blk), _index(0) {}
1373 
1374   void work(uint worker_id) {
1375     ShenandoahParallelWorkerSession worker_session(worker_id);
1376     size_t stride = ShenandoahParallelRegionStride;
1377 
1378     size_t max = _heap-&gt;num_regions();
1379     while (_index &lt; max) {
1380       size_t cur = Atomic::fetch_and_add(&amp;_index, stride);
1381       size_t start = cur;
1382       size_t end = MIN2(cur + stride, max);
1383       if (start &gt;= max) break;
1384 
1385       for (size_t i = cur; i &lt; end; i++) {
1386         ShenandoahHeapRegion* current = _heap-&gt;get_region(i);
1387         _blk-&gt;heap_region_do(current);
1388       }
1389     }
1390   }
1391 };
1392 
1393 void ShenandoahHeap::parallel_heap_region_iterate(ShenandoahHeapRegionClosure* blk) const {
1394   assert(blk-&gt;is_thread_safe(), "Only thread-safe closures here");
1395   if (num_regions() &gt; ShenandoahParallelRegionStride) {
1396     ShenandoahParallelHeapRegionTask task(blk);
1397     workers()-&gt;run_task(&amp;task);
1398   } else {
1399     heap_region_iterate(blk);
1400   }
1401 }
1402 
1403 class ShenandoahInitMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {
1404 private:
1405   ShenandoahMarkingContext* const _ctx;
1406 public:
1407   ShenandoahInitMarkUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()-&gt;marking_context()) {}
1408 
1409   void heap_region_do(ShenandoahHeapRegion* r) {
1410     assert(!r-&gt;has_live(), "Region " SIZE_FORMAT " should have no live data", r-&gt;index());
1411     if (r-&gt;is_active()) {
1412       // Check if region needs updating its TAMS. We have updated it already during concurrent
1413       // reset, so it is very likely we don't need to do another write here.
1414       if (_ctx-&gt;top_at_mark_start(r) != r-&gt;top()) {
1415         _ctx-&gt;capture_top_at_mark_start(r);
1416       }
1417     } else {
1418       assert(_ctx-&gt;top_at_mark_start(r) == r-&gt;top(),
1419              "Region " SIZE_FORMAT " should already have correct TAMS", r-&gt;index());
1420     }
1421   }
1422 
1423   bool is_thread_safe() { return true; }
1424 };
1425 
1426 void ShenandoahHeap::op_init_mark() {
1427   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), "Should be at safepoint");
1428   assert(Thread::current()-&gt;is_VM_thread(), "can only do this in VMThread");
1429 
1430   assert(marking_context()-&gt;is_bitmap_clear(), "need clear marking bitmap");
1431   assert(!marking_context()-&gt;is_complete(), "should not be complete");
1432   assert(!has_forwarded_objects(), "No forwarded objects on this path");
1433 
1434   if (ShenandoahVerify) {
1435     verifier()-&gt;verify_before_concmark();
1436   }
1437 
1438   if (VerifyBeforeGC) {
1439     Universe::verify();
1440   }
1441 
1442   set_concurrent_mark_in_progress(true);
1443 
1444   // We need to reset all TLABs because they might be below the TAMS, and we need to mark
1445   // the objects in them. Do not let mutators allocate any new objects in their current TLABs.
1446   // It is also a good place to resize the TLAB sizes for future allocations.
1447   if (UseTLAB) {
1448     ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_manage_tlabs);
1449     tlabs_retire(ResizeTLAB);
1450   }
1451 
1452   {
1453     ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_region_states);
1454     ShenandoahInitMarkUpdateRegionStateClosure cl;
1455     parallel_heap_region_iterate(&amp;cl);
1456   }
1457 
1458   // Make above changes visible to worker threads
1459   OrderAccess::fence();
1460 
1461   concurrent_mark()-&gt;mark_roots(ShenandoahPhaseTimings::scan_roots);
1462 
1463   if (ShenandoahPacing) {
1464     pacer()-&gt;setup_for_mark();
1465   }
1466 
1467   // Arm nmethods for concurrent marking. When a nmethod is about to be executed,
1468   // we need to make sure that all its metadata are marked. alternative is to remark
1469   // thread roots at final mark pause, but it can be potential latency killer.
1470   if (ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
1471     ShenandoahCodeRoots::arm_nmethods();
1472   }
1473 }
1474 
1475 void ShenandoahHeap::op_mark() {
1476   concurrent_mark()-&gt;mark_from_roots();
1477 }
1478 
1479 class ShenandoahFinalMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {
1480 private:
1481   ShenandoahMarkingContext* const _ctx;
1482   ShenandoahHeapLock* const _lock;
1483 
1484 public:
1485   ShenandoahFinalMarkUpdateRegionStateClosure() :
1486     _ctx(ShenandoahHeap::heap()-&gt;complete_marking_context()), _lock(ShenandoahHeap::heap()-&gt;lock()) {}
1487 
1488   void heap_region_do(ShenandoahHeapRegion* r) {
1489     if (r-&gt;is_active()) {
1490       // All allocations past TAMS are implicitly live, adjust the region data.
1491       // Bitmaps/TAMS are swapped at this point, so we need to poll complete bitmap.
1492       HeapWord *tams = _ctx-&gt;top_at_mark_start(r);
1493       HeapWord *top = r-&gt;top();
1494       if (top &gt; tams) {
1495         r-&gt;increase_live_data_alloc_words(pointer_delta(top, tams));
1496       }
1497 
1498       // We are about to select the collection set, make sure it knows about
1499       // current pinning status. Also, this allows trashing more regions that
1500       // now have their pinning status dropped.
1501       if (r-&gt;is_pinned()) {
1502         if (r-&gt;pin_count() == 0) {
1503           ShenandoahHeapLocker locker(_lock);
1504           r-&gt;make_unpinned();
1505         }
1506       } else {
1507         if (r-&gt;pin_count() &gt; 0) {
1508           ShenandoahHeapLocker locker(_lock);
1509           r-&gt;make_pinned();
1510         }
1511       }
1512 
1513       // Remember limit for updating refs. It's guaranteed that we get no
1514       // from-space-refs written from here on.
1515       r-&gt;set_update_watermark_at_safepoint(r-&gt;top());
1516     } else {
1517       assert(!r-&gt;has_live(), "Region " SIZE_FORMAT " should have no live data", r-&gt;index());
1518       assert(_ctx-&gt;top_at_mark_start(r) == r-&gt;top(),
1519              "Region " SIZE_FORMAT " should have correct TAMS", r-&gt;index());
1520     }
1521   }
1522 
1523   bool is_thread_safe() { return true; }
1524 };
1525 
1526 void ShenandoahHeap::op_final_mark() {
1527   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), "Should be at safepoint");
1528   assert(!has_forwarded_objects(), "No forwarded objects on this path");
1529 
1530   // It is critical that we
1531   // evacuate roots right after finishing marking, so that we don't
1532   // get unmarked objects in the roots.
1533 
1534   if (!cancelled_gc()) {
1535     concurrent_mark()-&gt;finish_mark_from_roots(/* full_gc = */ false);
1536 
1537     // Marking is completed, deactivate SATB barrier
1538     set_concurrent_mark_in_progress(false);
1539     mark_complete_marking_context();
1540 
1541     parallel_cleaning(false /* full gc*/);
1542 
1543     if (ShenandoahVerify) {
1544       verifier()-&gt;verify_roots_no_forwarded();
1545     }
1546 
1547     {
1548       ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_region_states);
1549       ShenandoahFinalMarkUpdateRegionStateClosure cl;
1550       parallel_heap_region_iterate(&amp;cl);
1551 
1552       assert_pinned_region_status();
1553     }
1554 
1555     // Retire the TLABs, which will force threads to reacquire their TLABs after the pause.
1556     // This is needed for two reasons. Strong one: new allocations would be with new freeset,
1557     // which would be outside the collection set, so no cset writes would happen there.
1558     // Weaker one: new allocations would happen past update watermark, and so less work would
1559     // be needed for reference updates (would update the large filler instead).
1560     if (UseTLAB) {
1561       ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_manage_labs);
1562       tlabs_retire(false);
1563     }
1564 
1565     {
1566       ShenandoahGCPhase phase(ShenandoahPhaseTimings::choose_cset);
1567       ShenandoahHeapLocker locker(lock());
1568       _collection_set-&gt;clear();
1569       heuristics()-&gt;choose_collection_set(_collection_set);
1570     }
1571 
1572     {
1573       ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_rebuild_freeset);
1574       ShenandoahHeapLocker locker(lock());
1575       _free_set-&gt;rebuild();
1576     }
1577 
1578     if (!is_degenerated_gc_in_progress()) {
1579       prepare_concurrent_roots();
1580       prepare_concurrent_unloading();
1581     }
1582 
1583     // If collection set has candidates, start evacuation.
1584     // Otherwise, bypass the rest of the cycle.
1585     if (!collection_set()-&gt;is_empty()) {
1586       ShenandoahGCPhase init_evac(ShenandoahPhaseTimings::init_evac);
1587 
1588       if (ShenandoahVerify) {
1589         verifier()-&gt;verify_before_evacuation();
1590       }
1591 
1592       set_evacuation_in_progress(true);
1593       // From here on, we need to update references.
1594       set_has_forwarded_objects(true);
1595 
1596       if (!is_degenerated_gc_in_progress()) {
1597         if (ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
1598           ShenandoahCodeRoots::arm_nmethods();
1599         }
1600         evacuate_and_update_roots();
1601       }
1602 
1603       if (ShenandoahPacing) {
1604         pacer()-&gt;setup_for_evac();
1605       }
1606 
1607       if (ShenandoahVerify) {
1608         // If OOM while evacuating/updating of roots, there is no guarantee of their consistencies
1609         if (!cancelled_gc()) {
1610           ShenandoahRootVerifier::RootTypes types = ShenandoahRootVerifier::None;
1611           if (ShenandoahConcurrentRoots::should_do_concurrent_roots()) {
1612             types = ShenandoahRootVerifier::combine(ShenandoahRootVerifier::JNIHandleRoots, ShenandoahRootVerifier::WeakRoots);
1613             types = ShenandoahRootVerifier::combine(types, ShenandoahRootVerifier::CLDGRoots);
1614             types = ShenandoahRootVerifier::combine(types, ShenandoahRootVerifier::StringDedupRoots);
1615           }
1616 
1617           if (ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
1618             types = ShenandoahRootVerifier::combine(types, ShenandoahRootVerifier::CodeRoots);
1619           }
1620           verifier()-&gt;verify_roots_no_forwarded_except(types);
1621         }
1622         verifier()-&gt;verify_during_evacuation();
1623       }
1624     } else {
1625       if (ShenandoahVerify) {
1626         verifier()-&gt;verify_after_concmark();
1627       }
1628 
1629       if (VerifyAfterGC) {
1630         Universe::verify();
1631       }
1632     }
1633 
1634   } else {
1635     // If this cycle was updating references, we need to keep the has_forwarded_objects
1636     // flag on, for subsequent phases to deal with it.
1637     concurrent_mark()-&gt;cancel();
1638     set_concurrent_mark_in_progress(false);
1639 
1640     if (process_references()) {
1641       // Abandon reference processing right away: pre-cleaning must have failed.
1642       ReferenceProcessor *rp = ref_processor();
1643       rp-&gt;disable_discovery();
1644       rp-&gt;abandon_partial_discovery();
1645       rp-&gt;verify_no_references_recorded();
1646     }
1647   }
1648 }
1649 
1650 void ShenandoahHeap::op_conc_evac() {
1651   ShenandoahEvacuationTask task(this, _collection_set, true);
1652   workers()-&gt;run_task(&amp;task);
1653 }
1654 
1655 void ShenandoahHeap::op_stw_evac() {
1656   ShenandoahEvacuationTask task(this, _collection_set, false);
1657   workers()-&gt;run_task(&amp;task);
1658 }
1659 
1660 void ShenandoahHeap::op_updaterefs() {
1661   update_heap_references(true);
1662 }
1663 
1664 void ShenandoahHeap::op_cleanup_early() {
1665   free_set()-&gt;recycle_trash();
1666 }
1667 
1668 void ShenandoahHeap::op_cleanup_complete() {
1669   free_set()-&gt;recycle_trash();
1670 }
1671 
1672 class ShenandoahConcurrentRootsEvacUpdateTask : public AbstractGangTask {
1673 private:
1674   ShenandoahVMRoots&lt;true /*concurrent*/&gt;        _vm_roots;
1675   ShenandoahClassLoaderDataRoots&lt;true /*concurrent*/, false /*single threaded*/&gt; _cld_roots;
1676 
1677 public:
1678   ShenandoahConcurrentRootsEvacUpdateTask(ShenandoahPhaseTimings::Phase phase) :
1679     AbstractGangTask("Shenandoah Evacuate/Update Concurrent Strong Roots"),
1680     _vm_roots(phase),
1681     _cld_roots(phase, ShenandoahHeap::heap()-&gt;workers()-&gt;active_workers()) {}
1682 
1683   void work(uint worker_id) {
1684     ShenandoahConcurrentWorkerSession worker_session(worker_id);
1685     ShenandoahEvacOOMScope oom;
1686     {
1687       // vm_roots and weak_roots are OopStorage backed roots, concurrent iteration
1688       // may race against OopStorage::release() calls.
1689       ShenandoahEvacUpdateOopStorageRootsClosure cl;
1690       _vm_roots.oops_do&lt;ShenandoahEvacUpdateOopStorageRootsClosure&gt;(&amp;cl, worker_id);
1691     }
1692 
1693     {
1694       ShenandoahEvacuateUpdateRootsClosure&lt;&gt; cl;
1695       CLDToOopClosure clds(&amp;cl, ClassLoaderData::_claim_strong);
1696       _cld_roots.cld_do(&amp;clds, worker_id);
1697     }
1698   }
1699 };
1700 
1701 class ShenandoahEvacUpdateCleanupOopStorageRootsClosure : public BasicOopIterateClosure {
1702 private:
1703   ShenandoahHeap* const _heap;
1704   ShenandoahMarkingContext* const _mark_context;
1705   bool  _evac_in_progress;
1706   Thread* const _thread;
1707 
1708 public:
1709   ShenandoahEvacUpdateCleanupOopStorageRootsClosure();
1710   void do_oop(oop* p);
1711   void do_oop(narrowOop* p);
1712 };
1713 
1714 ShenandoahEvacUpdateCleanupOopStorageRootsClosure::ShenandoahEvacUpdateCleanupOopStorageRootsClosure() :
1715   _heap(ShenandoahHeap::heap()),
1716   _mark_context(ShenandoahHeap::heap()-&gt;marking_context()),
1717   _evac_in_progress(ShenandoahHeap::heap()-&gt;is_evacuation_in_progress()),
1718   _thread(Thread::current()) {
1719 }
1720 
1721 void ShenandoahEvacUpdateCleanupOopStorageRootsClosure::do_oop(oop* p) {
1722   const oop obj = RawAccess&lt;&gt;::oop_load(p);
1723   if (!CompressedOops::is_null(obj)) {
1724     if (!_mark_context-&gt;is_marked(obj)) {
1725       shenandoah_assert_correct(p, obj);
1726       Atomic::cmpxchg(p, obj, oop(NULL));
1727     } else if (_evac_in_progress &amp;&amp; _heap-&gt;in_collection_set(obj)) {
1728       oop resolved = ShenandoahBarrierSet::resolve_forwarded_not_null(obj);
1729       if (resolved == obj) {
1730         resolved = _heap-&gt;evacuate_object(obj, _thread);
1731       }
1732       Atomic::cmpxchg(p, obj, resolved);
1733       assert(_heap-&gt;cancelled_gc() ||
1734              _mark_context-&gt;is_marked(resolved) &amp;&amp; !_heap-&gt;in_collection_set(resolved),
1735              "Sanity");
1736     }
1737   }
1738 }
1739 
1740 void ShenandoahEvacUpdateCleanupOopStorageRootsClosure::do_oop(narrowOop* p) {
1741   ShouldNotReachHere();
1742 }
1743 
1744 class ShenandoahIsCLDAliveClosure : public CLDClosure {
1745 public:
1746   void do_cld(ClassLoaderData* cld) {
1747     cld-&gt;is_alive();
1748   }
1749 };
1750 
1751 class ShenandoahIsNMethodAliveClosure: public NMethodClosure {
1752 public:
1753   void do_nmethod(nmethod* n) {
1754     n-&gt;is_unloading();
1755   }
1756 };
1757 
1758 // This task not only evacuates/updates marked weak roots, but also "NULL"
1759 // dead weak roots.
1760 class ShenandoahConcurrentWeakRootsEvacUpdateTask : public AbstractGangTask {
1761 private:
1762   ShenandoahVMWeakRoots&lt;true /*concurrent*/&gt; _vm_roots;
1763 
1764   // Roots related to concurrent class unloading
1765   ShenandoahClassLoaderDataRoots&lt;true /* concurrent */, false /* single thread*/&gt;
1766                                              _cld_roots;
1767   ShenandoahConcurrentNMethodIterator        _nmethod_itr;
1768   ShenandoahConcurrentStringDedupRoots       _dedup_roots;
1769   bool                                       _concurrent_class_unloading;
1770 
1771 public:
1772   ShenandoahConcurrentWeakRootsEvacUpdateTask(ShenandoahPhaseTimings::Phase phase) :
1773     AbstractGangTask("Shenandoah Evacuate/Update Concurrent Weak Roots"),
1774     _vm_roots(phase),
1775     _cld_roots(phase, ShenandoahHeap::heap()-&gt;workers()-&gt;active_workers()),
1776     _nmethod_itr(ShenandoahCodeRoots::table()),
1777     _dedup_roots(phase),
1778     _concurrent_class_unloading(ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
1779     if (_concurrent_class_unloading) {
1780       MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);
1781       _nmethod_itr.nmethods_do_begin();
1782     }
1783   }
1784 
1785   ~ShenandoahConcurrentWeakRootsEvacUpdateTask() {
1786     if (_concurrent_class_unloading) {
1787       MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);
1788       _nmethod_itr.nmethods_do_end();
1789     }
1790     // Notify runtime data structures of potentially dead oops
1791     _vm_roots.report_num_dead();
1792   }
1793 
1794   void work(uint worker_id) {
1795     ShenandoahConcurrentWorkerSession worker_session(worker_id);
1796     {
1797       ShenandoahEvacOOMScope oom;
1798       // jni_roots and weak_roots are OopStorage backed roots, concurrent iteration
1799       // may race against OopStorage::release() calls.
1800       ShenandoahEvacUpdateCleanupOopStorageRootsClosure cl;
1801       _vm_roots.oops_do(&amp;cl, worker_id);
1802 
1803       // String dedup weak roots
1804       ShenandoahForwardedIsAliveClosure is_alive;
1805       ShenandoahEvacuateUpdateRootsClosure&lt;MO_RELEASE&gt; keep_alive;
1806       _dedup_roots.oops_do(&amp;is_alive, &amp;keep_alive, worker_id);
1807     }
1808 
1809     // If we are going to perform concurrent class unloading later on, we need to
1810     // cleanup the weak oops in CLD and determinate nmethod's unloading state, so that we
1811     // can cleanup immediate garbage sooner.
1812     if (_concurrent_class_unloading) {
1813       // Applies ShenandoahIsCLDAlive closure to CLDs, native barrier will either NULL the
1814       // CLD's holder or evacuate it.
1815       ShenandoahIsCLDAliveClosure is_cld_alive;
1816       _cld_roots.cld_do(&amp;is_cld_alive, worker_id);
1817 
1818       // Applies ShenandoahIsNMethodAliveClosure to registered nmethods.
1819       // The closure calls nmethod-&gt;is_unloading(). The is_unloading
1820       // state is cached, therefore, during concurrent class unloading phase,
1821       // we will not touch the metadata of unloading nmethods
1822       ShenandoahIsNMethodAliveClosure is_nmethod_alive;
1823       _nmethod_itr.nmethods_do(&amp;is_nmethod_alive);
1824     }
1825   }
1826 };
1827 
1828 void ShenandoahHeap::op_weak_roots() {
1829   if (is_concurrent_weak_root_in_progress()) {
1830     // Concurrent weak root processing
1831     {
1832       ShenandoahTimingsTracker t(ShenandoahPhaseTimings::conc_weak_roots_work);
1833       ShenandoahGCWorkerPhase worker_phase(ShenandoahPhaseTimings::conc_weak_roots_work);
1834       ShenandoahConcurrentWeakRootsEvacUpdateTask task(ShenandoahPhaseTimings::conc_weak_roots_work);
1835       workers()-&gt;run_task(&amp;task);
1836       if (!ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
1837         set_concurrent_weak_root_in_progress(false);
1838       }
1839     }
1840 
1841     // Perform handshake to flush out dead oops
1842     {
1843       ShenandoahTimingsTracker t(ShenandoahPhaseTimings::conc_weak_roots_rendezvous);
1844       rendezvous_threads();
1845     }
1846   }
1847 }
1848 
1849 void ShenandoahHeap::op_class_unloading() {
1850   assert (is_concurrent_weak_root_in_progress() &amp;&amp;
1851           ShenandoahConcurrentRoots::should_do_concurrent_class_unloading(),
1852           "Checked by caller");
1853   _unloader.unload();
1854   set_concurrent_weak_root_in_progress(false);
1855 }
1856 
1857 void ShenandoahHeap::op_strong_roots() {
1858   assert(is_concurrent_strong_root_in_progress(), "Checked by caller");
1859   ShenandoahConcurrentRootsEvacUpdateTask task(ShenandoahPhaseTimings::conc_strong_roots);
1860   workers()-&gt;run_task(&amp;task);
1861   set_concurrent_strong_root_in_progress(false);
1862 }
1863 
1864 void ShenandoahHeap::op_rendezvous_roots() {
1865   rendezvous_threads();
1866 }
1867 
1868 void ShenandoahHeap::rendezvous_threads() {
1869   ShenandoahRendezvousClosure cl;
1870   Handshake::execute(&amp;cl);
1871 }
1872 
1873 class ShenandoahResetUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {
1874 private:
1875   ShenandoahMarkingContext* const _ctx;
1876 public:
1877   ShenandoahResetUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()-&gt;marking_context()) {}
1878 
1879   void heap_region_do(ShenandoahHeapRegion* r) {
1880     if (r-&gt;is_active()) {
1881       // Reset live data and set TAMS optimistically. We would recheck these under the pause
1882       // anyway to capture any updates that happened since now.
1883       r-&gt;clear_live_data();
1884       _ctx-&gt;capture_top_at_mark_start(r);
1885     }
1886   }
1887 
1888   bool is_thread_safe() { return true; }
1889 };
1890 
1891 void ShenandoahHeap::op_reset() {
1892   if (ShenandoahPacing) {
1893     pacer()-&gt;setup_for_reset();
1894   }
1895   reset_mark_bitmap();
1896 
1897   ShenandoahResetUpdateRegionStateClosure cl;
1898   parallel_heap_region_iterate(&amp;cl);
1899 }
1900 
1901 void ShenandoahHeap::op_preclean() {
1902   if (ShenandoahPacing) {
1903     pacer()-&gt;setup_for_preclean();
1904   }
1905   concurrent_mark()-&gt;preclean_weak_refs();
1906 }
1907 
1908 void ShenandoahHeap::op_full(GCCause::Cause cause) {
1909   ShenandoahMetricsSnapshot metrics;
1910   metrics.snap_before();
1911 
1912   full_gc()-&gt;do_it(cause);
1913 
1914   metrics.snap_after();
1915 
1916   if (metrics.is_good_progress()) {
1917     _progress_last_gc.set();
1918   } else {
1919     // Nothing to do. Tell the allocation path that we have failed to make
1920     // progress, and it can finally fail.
1921     _progress_last_gc.unset();
1922   }
1923 }
1924 
1925 void ShenandoahHeap::op_degenerated(ShenandoahDegenPoint point) {
1926   // Degenerated GC is STW, but it can also fail. Current mechanics communicates
1927   // GC failure via cancelled_concgc() flag. So, if we detect the failure after
1928   // some phase, we have to upgrade the Degenerate GC to Full GC.
1929 
1930   clear_cancelled_gc();
1931 
1932   ShenandoahMetricsSnapshot metrics;
1933   metrics.snap_before();
1934 
1935   switch (point) {
1936     // The cases below form the Duff's-like device: it describes the actual GC cycle,
1937     // but enters it at different points, depending on which concurrent phase had
1938     // degenerated.
1939 
1940     case _degenerated_outside_cycle:
1941       // We have degenerated from outside the cycle, which means something is bad with
1942       // the heap, most probably heavy humongous fragmentation, or we are very low on free
1943       // space. It makes little sense to wait for Full GC to reclaim as much as it can, when
1944       // we can do the most aggressive degen cycle, which includes processing references and
1945       // class unloading, unless those features are explicitly disabled.
1946       //
1947       // Note that we can only do this for "outside-cycle" degens, otherwise we would risk
1948       // changing the cycle parameters mid-cycle during concurrent -&gt; degenerated handover.
1949       set_process_references(heuristics()-&gt;can_process_references());
1950       set_unload_classes(heuristics()-&gt;can_unload_classes());
1951 
1952       op_reset();
1953 
1954       op_init_mark();
1955       if (cancelled_gc()) {
1956         op_degenerated_fail();
1957         return;
1958       }
1959 
1960     case _degenerated_mark:
1961       op_final_mark();
1962       if (cancelled_gc()) {
1963         op_degenerated_fail();
1964         return;
1965       }
1966 
1967       if (!has_forwarded_objects() &amp;&amp; ShenandoahConcurrentRoots::can_do_concurrent_class_unloading()) {
1968         // Disarm nmethods that armed for concurrent mark. On normal cycle, it would
1969         // be disarmed while conc-roots phase is running.
1970         // TODO: Call op_conc_roots() here instead
1971         ShenandoahCodeRoots::disarm_nmethods();
1972       }
1973 
1974       op_cleanup_early();
1975 
1976     case _degenerated_evac:
1977       // If heuristics thinks we should do the cycle, this flag would be set,
1978       // and we can do evacuation. Otherwise, it would be the shortcut cycle.
1979       if (is_evacuation_in_progress()) {
1980 
1981         // Degeneration under oom-evac protocol might have left some objects in
1982         // collection set un-evacuated. Restart evacuation from the beginning to
1983         // capture all objects. For all the objects that are already evacuated,
1984         // it would be a simple check, which is supposed to be fast. This is also
1985         // safe to do even without degeneration, as CSet iterator is at beginning
1986         // in preparation for evacuation anyway.
1987         //
1988         // Before doing that, we need to make sure we never had any cset-pinned
1989         // regions. This may happen if allocation failure happened when evacuating
1990         // the about-to-be-pinned object, oom-evac protocol left the object in
1991         // the collection set, and then the pin reached the cset region. If we continue
1992         // the cycle here, we would trash the cset and alive objects in it. To avoid
1993         // it, we fail degeneration right away and slide into Full GC to recover.
1994 
1995         {
1996           sync_pinned_region_status();
1997           collection_set()-&gt;clear_current_index();
1998 
1999           ShenandoahHeapRegion* r;
2000           while ((r = collection_set()-&gt;next()) != NULL) {
2001             if (r-&gt;is_pinned()) {
2002               cancel_gc(GCCause::_shenandoah_upgrade_to_full_gc);
2003               op_degenerated_fail();
2004               return;
2005             }
2006           }
2007 
2008           collection_set()-&gt;clear_current_index();
2009         }
2010 
2011         op_stw_evac();
2012         if (cancelled_gc()) {
2013           op_degenerated_fail();
2014           return;
2015         }
2016       }
2017 
2018       // If heuristics thinks we should do the cycle, this flag would be set,
2019       // and we need to do update-refs. Otherwise, it would be the shortcut cycle.
2020       if (has_forwarded_objects()) {
2021         op_init_updaterefs();
2022         if (cancelled_gc()) {
2023           op_degenerated_fail();
2024           return;
2025         }
2026       }
2027 
2028     case _degenerated_updaterefs:
2029       if (has_forwarded_objects()) {
2030         op_final_updaterefs();
2031         if (cancelled_gc()) {
2032           op_degenerated_fail();
2033           return;
2034         }
2035       }
2036 
2037       op_cleanup_complete();
2038       break;
2039 
2040     default:
2041       ShouldNotReachHere();
2042   }
2043 
2044   if (ShenandoahVerify) {
2045     verifier()-&gt;verify_after_degenerated();
2046   }
2047 
2048   if (VerifyAfterGC) {
2049     Universe::verify();
2050   }
2051 
2052   metrics.snap_after();
2053 
2054   // Check for futility and fail. There is no reason to do several back-to-back Degenerated cycles,
2055   // because that probably means the heap is overloaded and/or fragmented.
2056   if (!metrics.is_good_progress()) {
2057     _progress_last_gc.unset();
2058     cancel_gc(GCCause::_shenandoah_upgrade_to_full_gc);
2059     op_degenerated_futile();
2060   } else {
2061     _progress_last_gc.set();
2062   }
2063 }
2064 
2065 void ShenandoahHeap::op_degenerated_fail() {
2066   log_info(gc)("Cannot finish degeneration, upgrading to Full GC");
2067   shenandoah_policy()-&gt;record_degenerated_upgrade_to_full();
2068   op_full(GCCause::_shenandoah_upgrade_to_full_gc);
2069 }
2070 
2071 void ShenandoahHeap::op_degenerated_futile() {
2072   shenandoah_policy()-&gt;record_degenerated_upgrade_to_full();
2073   op_full(GCCause::_shenandoah_upgrade_to_full_gc);
2074 }
2075 
2076 void ShenandoahHeap::force_satb_flush_all_threads() {
2077   if (!is_concurrent_mark_in_progress()) {
2078     // No need to flush SATBs
2079     return;
2080   }
2081 
2082   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {
2083     ShenandoahThreadLocalData::set_force_satb_flush(t, true);
2084   }
2085   // The threads are not "acquiring" their thread-local data, but it does not
2086   // hurt to "release" the updates here anyway.
2087   OrderAccess::fence();
2088 }
2089 
2090 void ShenandoahHeap::set_gc_state_all_threads(char state) {
2091   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {
2092     ShenandoahThreadLocalData::set_gc_state(t, state);
2093   }
2094 }
2095 
2096 void ShenandoahHeap::set_gc_state_mask(uint mask, bool value) {
2097   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), "Should really be Shenandoah safepoint");
2098   _gc_state.set_cond(mask, value);
2099   set_gc_state_all_threads(_gc_state.raw_value());
2100 }
2101 
2102 void ShenandoahHeap::set_concurrent_mark_in_progress(bool in_progress) {
2103   if (has_forwarded_objects()) {
2104     set_gc_state_mask(MARKING | UPDATEREFS, in_progress);
2105   } else {
2106     set_gc_state_mask(MARKING, in_progress);
2107   }
2108   ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(in_progress, !in_progress);
2109 }
2110 
2111 void ShenandoahHeap::set_evacuation_in_progress(bool in_progress) {
2112   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), "Only call this at safepoint");
2113   set_gc_state_mask(EVACUATION, in_progress);
2114 }
2115 
2116 void ShenandoahHeap::set_concurrent_strong_root_in_progress(bool in_progress) {
2117   assert(ShenandoahConcurrentRoots::can_do_concurrent_roots(), "Why set the flag?");
2118   if (in_progress) {
2119     _concurrent_strong_root_in_progress.set();
2120   } else {
2121     _concurrent_strong_root_in_progress.unset();
2122   }
2123 }
2124 
2125 void ShenandoahHeap::set_concurrent_weak_root_in_progress(bool in_progress) {
2126   assert(ShenandoahConcurrentRoots::can_do_concurrent_roots(), "Why set the flag?");
2127   if (in_progress) {
2128     _concurrent_weak_root_in_progress.set();
2129   } else {
2130     _concurrent_weak_root_in_progress.unset();
2131   }
2132 }
2133 
2134 void ShenandoahHeap::ref_processing_init() {
2135   assert(_max_workers &gt; 0, "Sanity");
2136 
2137   _ref_processor =
2138     new ReferenceProcessor(&amp;_subject_to_discovery,  // is_subject_to_discovery
2139                            _ref_proc_mt_processing, // MT processing
2140                            _max_workers,            // Degree of MT processing
2141                            _ref_proc_mt_discovery,  // MT discovery
2142                            _max_workers,            // Degree of MT discovery
2143                            false,                   // Reference discovery is not atomic
2144                            NULL,                    // No closure, should be installed before use
2145                            true);                   // Scale worker threads
2146 
2147   shenandoah_assert_rp_isalive_not_installed();
2148 }
2149 
2150 GCTracer* ShenandoahHeap::tracer() {
2151   return shenandoah_policy()-&gt;tracer();
2152 }
2153 
2154 size_t ShenandoahHeap::tlab_used(Thread* thread) const {
2155   return _free_set-&gt;used();
2156 }
2157 
2158 bool ShenandoahHeap::try_cancel_gc() {
2159   while (true) {
2160     jbyte prev = _cancelled_gc.cmpxchg(CANCELLED, CANCELLABLE);
2161     if (prev == CANCELLABLE) return true;
2162     else if (prev == CANCELLED) return false;
2163     assert(ShenandoahSuspendibleWorkers, "should not get here when not using suspendible workers");
2164     assert(prev == NOT_CANCELLED, "must be NOT_CANCELLED");
2165     if (Thread::current()-&gt;is_Java_thread()) {
2166       // We need to provide a safepoint here, otherwise we might
2167       // spin forever if a SP is pending.
2168       ThreadBlockInVM sp(JavaThread::current());
2169       SpinPause();
2170     }
2171   }
2172 }
2173 
2174 void ShenandoahHeap::cancel_gc(GCCause::Cause cause) {
2175   if (try_cancel_gc()) {
2176     FormatBuffer&lt;&gt; msg("Cancelling GC: %s", GCCause::to_string(cause));
2177     log_info(gc)("%s", msg.buffer());
2178     Events::log(Thread::current(), "%s", msg.buffer());
2179   }
2180 }
2181 
2182 uint ShenandoahHeap::max_workers() {
2183   return _max_workers;
2184 }
2185 
2186 void ShenandoahHeap::stop() {
2187   // The shutdown sequence should be able to terminate when GC is running.
2188 
2189   // Step 0. Notify policy to disable event recording.
2190   _shenandoah_policy-&gt;record_shutdown();
2191 
2192   // Step 1. Notify control thread that we are in shutdown.
2193   // Note that we cannot do that with stop(), because stop() is blocking and waits for the actual shutdown.
2194   // Doing stop() here would wait for the normal GC cycle to complete, never falling through to cancel below.
2195   control_thread()-&gt;prepare_for_graceful_shutdown();
2196 
2197   // Step 2. Notify GC workers that we are cancelling GC.
2198   cancel_gc(GCCause::_shenandoah_stop_vm);
2199 
2200   // Step 3. Wait until GC worker exits normally.
2201   control_thread()-&gt;stop();
2202 
2203   // Step 4. Stop String Dedup thread if it is active
2204   if (ShenandoahStringDedup::is_enabled()) {
2205     ShenandoahStringDedup::stop();
2206   }
2207 }
2208 
2209 void ShenandoahHeap::stw_unload_classes(bool full_gc) {
2210   if (!unload_classes()) return;
2211 
2212   // Unload classes and purge SystemDictionary.
2213   {
2214     ShenandoahGCPhase phase(full_gc ?
2215                             ShenandoahPhaseTimings::full_gc_purge_class_unload :
2216                             ShenandoahPhaseTimings::purge_class_unload);
2217     bool purged_class = SystemDictionary::do_unloading(gc_timer());
2218 
2219     ShenandoahIsAliveSelector is_alive;
2220     uint num_workers = _workers-&gt;active_workers();
2221     ShenandoahClassUnloadingTask unlink_task(is_alive.is_alive_closure(), num_workers, purged_class);
2222     _workers-&gt;run_task(&amp;unlink_task);
2223   }
2224 
2225   {
2226     ShenandoahGCPhase phase(full_gc ?
2227                             ShenandoahPhaseTimings::full_gc_purge_cldg :
2228                             ShenandoahPhaseTimings::purge_cldg);
2229     ClassLoaderDataGraph::purge(/*at_safepoint*/true);
2230   }
2231   // Resize and verify metaspace
2232   MetaspaceGC::compute_new_size();
2233   DEBUG_ONLY(MetaspaceUtils::verify();)
2234 }
2235 
2236 // Weak roots are either pre-evacuated (final mark) or updated (final updaterefs),
2237 // so they should not have forwarded oops.
2238 // However, we do need to "null" dead oops in the roots, if can not be done
2239 // in concurrent cycles.
2240 void ShenandoahHeap::stw_process_weak_roots(bool full_gc) {
2241   ShenandoahGCPhase root_phase(full_gc ?
2242                                ShenandoahPhaseTimings::full_gc_purge :
2243                                ShenandoahPhaseTimings::purge);
2244   uint num_workers = _workers-&gt;active_workers();
2245   ShenandoahPhaseTimings::Phase timing_phase = full_gc ?
2246                                                ShenandoahPhaseTimings::full_gc_purge_weak_par :
2247                                                ShenandoahPhaseTimings::purge_weak_par;
2248   ShenandoahGCPhase phase(timing_phase);
2249   ShenandoahGCWorkerPhase worker_phase(timing_phase);
2250 
2251   // Cleanup weak roots
2252   if (has_forwarded_objects()) {
2253     ShenandoahForwardedIsAliveClosure is_alive;
2254     ShenandoahUpdateRefsClosure keep_alive;
2255     ShenandoahParallelWeakRootsCleaningTask&lt;ShenandoahForwardedIsAliveClosure, ShenandoahUpdateRefsClosure&gt;
2256       cleaning_task(timing_phase, &amp;is_alive, &amp;keep_alive, num_workers, !ShenandoahConcurrentRoots::should_do_concurrent_class_unloading());
2257     _workers-&gt;run_task(&amp;cleaning_task);
2258   } else {
2259     ShenandoahIsAliveClosure is_alive;
2260 #ifdef ASSERT
2261     ShenandoahAssertNotForwardedClosure verify_cl;
2262     ShenandoahParallelWeakRootsCleaningTask&lt;ShenandoahIsAliveClosure, ShenandoahAssertNotForwardedClosure&gt;
2263       cleaning_task(timing_phase, &amp;is_alive, &amp;verify_cl, num_workers, !ShenandoahConcurrentRoots::should_do_concurrent_class_unloading());
2264 #else
2265     ShenandoahParallelWeakRootsCleaningTask&lt;ShenandoahIsAliveClosure, DoNothingClosure&gt;
2266       cleaning_task(timing_phase, &amp;is_alive, &amp;do_nothing_cl, num_workers, !ShenandoahConcurrentRoots::should_do_concurrent_class_unloading());
2267 #endif
2268     _workers-&gt;run_task(&amp;cleaning_task);
2269   }
2270 }
2271 
2272 void ShenandoahHeap::parallel_cleaning(bool full_gc) {
2273   assert(SafepointSynchronize::is_at_safepoint(), "Must be at a safepoint");
2274   stw_process_weak_roots(full_gc);
2275   if (!ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
2276     stw_unload_classes(full_gc);
2277   }
2278 }
2279 
2280 void ShenandoahHeap::set_has_forwarded_objects(bool cond) {
2281   set_gc_state_mask(HAS_FORWARDED, cond);
2282 }
2283 
2284 void ShenandoahHeap::set_process_references(bool pr) {
2285   _process_references.set_cond(pr);
2286 }
2287 
2288 void ShenandoahHeap::set_unload_classes(bool uc) {
2289   _unload_classes.set_cond(uc);
2290 }
2291 
2292 bool ShenandoahHeap::process_references() const {
2293   return _process_references.is_set();
2294 }
2295 
2296 bool ShenandoahHeap::unload_classes() const {
2297   return _unload_classes.is_set();
2298 }
2299 
2300 address ShenandoahHeap::in_cset_fast_test_addr() {
2301   ShenandoahHeap* heap = ShenandoahHeap::heap();
2302   assert(heap-&gt;collection_set() != NULL, "Sanity");
2303   return (address) heap-&gt;collection_set()-&gt;biased_map_address();
2304 }
2305 
2306 address ShenandoahHeap::cancelled_gc_addr() {
2307   return (address) ShenandoahHeap::heap()-&gt;_cancelled_gc.addr_of();
2308 }
2309 
2310 address ShenandoahHeap::gc_state_addr() {
2311   return (address) ShenandoahHeap::heap()-&gt;_gc_state.addr_of();
2312 }
2313 
2314 size_t ShenandoahHeap::bytes_allocated_since_gc_start() {
2315   return Atomic::load_acquire(&amp;_bytes_allocated_since_gc_start);
2316 }
2317 
2318 void ShenandoahHeap::reset_bytes_allocated_since_gc_start() {
2319   Atomic::release_store_fence(&amp;_bytes_allocated_since_gc_start, (size_t)0);
2320 }
2321 
2322 void ShenandoahHeap::set_degenerated_gc_in_progress(bool in_progress) {
2323   _degenerated_gc_in_progress.set_cond(in_progress);
2324 }
2325 
2326 void ShenandoahHeap::set_full_gc_in_progress(bool in_progress) {
2327   _full_gc_in_progress.set_cond(in_progress);
2328 }
2329 
2330 void ShenandoahHeap::set_full_gc_move_in_progress(bool in_progress) {
2331   assert (is_full_gc_in_progress(), "should be");
2332   _full_gc_move_in_progress.set_cond(in_progress);
2333 }
2334 
2335 void ShenandoahHeap::set_update_refs_in_progress(bool in_progress) {
2336   set_gc_state_mask(UPDATEREFS, in_progress);
2337 }
2338 
2339 void ShenandoahHeap::register_nmethod(nmethod* nm) {
2340   ShenandoahCodeRoots::register_nmethod(nm);
2341 }
2342 
2343 void ShenandoahHeap::unregister_nmethod(nmethod* nm) {
2344   ShenandoahCodeRoots::unregister_nmethod(nm);
2345 }
2346 
2347 void ShenandoahHeap::flush_nmethod(nmethod* nm) {
2348   ShenandoahCodeRoots::flush_nmethod(nm);
2349 }
2350 
2351 oop ShenandoahHeap::pin_object(JavaThread* thr, oop o) {
2352   heap_region_containing(o)-&gt;record_pin();
2353   return o;
2354 }
2355 
2356 void ShenandoahHeap::unpin_object(JavaThread* thr, oop o) {
2357   heap_region_containing(o)-&gt;record_unpin();
2358 }
2359 
2360 void ShenandoahHeap::sync_pinned_region_status() {
2361   ShenandoahHeapLocker locker(lock());
2362 
2363   for (size_t i = 0; i &lt; num_regions(); i++) {
2364     ShenandoahHeapRegion *r = get_region(i);
2365     if (r-&gt;is_active()) {
2366       if (r-&gt;is_pinned()) {
2367         if (r-&gt;pin_count() == 0) {
2368           r-&gt;make_unpinned();
2369         }
2370       } else {
2371         if (r-&gt;pin_count() &gt; 0) {
2372           r-&gt;make_pinned();
2373         }
2374       }
2375     }
2376   }
2377 
2378   assert_pinned_region_status();
2379 }
2380 
2381 #ifdef ASSERT
2382 void ShenandoahHeap::assert_pinned_region_status() {
2383   for (size_t i = 0; i &lt; num_regions(); i++) {
2384     ShenandoahHeapRegion* r = get_region(i);
2385     assert((r-&gt;is_pinned() &amp;&amp; r-&gt;pin_count() &gt; 0) || (!r-&gt;is_pinned() &amp;&amp; r-&gt;pin_count() == 0),
2386            "Region " SIZE_FORMAT " pinning status is inconsistent", i);
2387   }
2388 }
2389 #endif
2390 
2391 ConcurrentGCTimer* ShenandoahHeap::gc_timer() const {
2392   return _gc_timer;
2393 }
2394 
2395 void ShenandoahHeap::prepare_concurrent_roots() {
2396   assert(SafepointSynchronize::is_at_safepoint(), "Must be at a safepoint");
2397   if (ShenandoahConcurrentRoots::should_do_concurrent_roots()) {
2398     set_concurrent_strong_root_in_progress(!collection_set()-&gt;is_empty());
2399     set_concurrent_weak_root_in_progress(true);
2400   }
2401 }
2402 
2403 void ShenandoahHeap::prepare_concurrent_unloading() {
2404   assert(SafepointSynchronize::is_at_safepoint(), "Must be at a safepoint");
2405   if (ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
2406     _unloader.prepare();
2407   }
2408 }
2409 
2410 void ShenandoahHeap::finish_concurrent_unloading() {
2411   assert(SafepointSynchronize::is_at_safepoint(), "Must be at a safepoint");
2412   if (ShenandoahConcurrentRoots::should_do_concurrent_class_unloading()) {
2413     _unloader.finish();
2414   }
2415 }
2416 
2417 #ifdef ASSERT
2418 void ShenandoahHeap::assert_gc_workers(uint nworkers) {
2419   assert(nworkers &gt; 0 &amp;&amp; nworkers &lt;= max_workers(), "Sanity");
2420 
2421   if (ShenandoahSafepoint::is_at_shenandoah_safepoint()) {
2422     if (UseDynamicNumberOfGCThreads) {
2423       assert(nworkers &lt;= ParallelGCThreads, "Cannot use more than it has");
2424     } else {
2425       // Use ParallelGCThreads inside safepoints
2426       assert(nworkers == ParallelGCThreads, "Use ParallelGCThreads within safepoints");
2427     }
2428   } else {
2429     if (UseDynamicNumberOfGCThreads) {
2430       assert(nworkers &lt;= ConcGCThreads, "Cannot use more than it has");
2431     } else {
2432       // Use ConcGCThreads outside safepoints
2433       assert(nworkers == ConcGCThreads, "Use ConcGCThreads outside safepoints");
2434     }
2435   }
2436 }
2437 #endif
2438 
2439 ShenandoahVerifier* ShenandoahHeap::verifier() {
2440   guarantee(ShenandoahVerify, "Should be enabled");
2441   assert (_verifier != NULL, "sanity");
2442   return _verifier;
2443 }
2444 
2445 template&lt;class T&gt;
2446 class ShenandoahUpdateHeapRefsTask : public AbstractGangTask {
2447 private:
2448   T cl;
2449   ShenandoahHeap* _heap;
2450   ShenandoahRegionIterator* _regions;
2451   bool _concurrent;
2452 public:
2453   ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions, bool concurrent) :
2454     AbstractGangTask("Shenandoah Update References"),
2455     cl(T()),
2456     _heap(ShenandoahHeap::heap()),
2457     _regions(regions),
2458     _concurrent(concurrent) {
2459   }
2460 
2461   void work(uint worker_id) {
2462     if (_concurrent) {
2463       ShenandoahConcurrentWorkerSession worker_session(worker_id);
2464       ShenandoahSuspendibleThreadSetJoiner stsj(ShenandoahSuspendibleWorkers);
2465       do_work();
2466     } else {
2467       ShenandoahParallelWorkerSession worker_session(worker_id);
2468       do_work();
2469     }
2470   }
2471 
2472 private:
2473   void do_work() {
2474     ShenandoahHeapRegion* r = _regions-&gt;next();
2475     ShenandoahMarkingContext* const ctx = _heap-&gt;complete_marking_context();
2476     while (r != NULL) {
2477       HeapWord* update_watermark = r-&gt;get_update_watermark();
2478       assert (update_watermark &gt;= r-&gt;bottom(), "sanity");
2479       if (r-&gt;is_active() &amp;&amp; !r-&gt;is_cset()) {
2480         _heap-&gt;marked_object_oop_iterate(r, &amp;cl, update_watermark);
2481       }
2482       if (ShenandoahPacing) {
2483         _heap-&gt;pacer()-&gt;report_updaterefs(pointer_delta(update_watermark, r-&gt;bottom()));
2484       }
2485       if (_heap-&gt;check_cancelled_gc_and_yield(_concurrent)) {
2486         return;
2487       }
2488       r = _regions-&gt;next();
2489     }
2490   }
2491 };
2492 
2493 void ShenandoahHeap::update_heap_references(bool concurrent) {
2494   ShenandoahUpdateHeapRefsTask&lt;ShenandoahUpdateHeapRefsClosure&gt; task(&amp;_update_refs_iterator, concurrent);
2495   workers()-&gt;run_task(&amp;task);
2496 }
2497 
2498 void ShenandoahHeap::op_init_updaterefs() {
2499   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), "must be at safepoint");
2500 
2501   set_evacuation_in_progress(false);
2502 
2503   // Evacuation is over, no GCLABs are needed anymore. GCLABs are under URWM, so we need to
2504   // make them parsable for update code to work correctly. Plus, we can compute new sizes
2505   // for future GCLABs here.
2506   if (UseTLAB) {
2507     ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_refs_manage_gclabs);
2508     gclabs_retire(ResizeTLAB);
2509   }
2510 
2511   if (ShenandoahVerify) {
2512     if (!is_degenerated_gc_in_progress()) {
2513       verifier()-&gt;verify_roots_in_to_space_except(ShenandoahRootVerifier::ThreadRoots);
2514     }
2515     verifier()-&gt;verify_before_updaterefs();
2516   }
2517 
2518   set_update_refs_in_progress(true);
2519 
2520   _update_refs_iterator.reset();
2521 
2522   if (ShenandoahPacing) {
2523     pacer()-&gt;setup_for_updaterefs();
2524   }
2525 }
2526 
2527 class ShenandoahFinalUpdateRefsUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {
2528 private:
2529   ShenandoahHeapLock* const _lock;
2530 
2531 public:
2532   ShenandoahFinalUpdateRefsUpdateRegionStateClosure() : _lock(ShenandoahHeap::heap()-&gt;lock()) {}
2533 
2534   void heap_region_do(ShenandoahHeapRegion* r) {
2535     // Drop unnecessary "pinned" state from regions that does not have CP marks
2536     // anymore, as this would allow trashing them.
2537 
2538     if (r-&gt;is_active()) {
2539       if (r-&gt;is_pinned()) {
2540         if (r-&gt;pin_count() == 0) {
2541           ShenandoahHeapLocker locker(_lock);
2542           r-&gt;make_unpinned();
2543         }
2544       } else {
2545         if (r-&gt;pin_count() &gt; 0) {
2546           ShenandoahHeapLocker locker(_lock);
2547           r-&gt;make_pinned();
2548         }
2549       }
2550     }
2551   }
2552 
2553   bool is_thread_safe() { return true; }
2554 };
2555 
2556 void ShenandoahHeap::op_final_updaterefs() {
2557   assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), "must be at safepoint");
2558 
2559   finish_concurrent_unloading();
2560 
2561   // Check if there is left-over work, and finish it
2562   if (_update_refs_iterator.has_next()) {
2563     ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_refs_finish_work);
2564 
2565     // Finish updating references where we left off.
2566     clear_cancelled_gc();
2567     update_heap_references(false);
2568   }
2569 
2570   // Clear cancelled GC, if set. On cancellation path, the block before would handle
2571   // everything. On degenerated paths, cancelled gc would not be set anyway.
2572   if (cancelled_gc()) {
2573     clear_cancelled_gc();
2574   }
2575   assert(!cancelled_gc(), "Should have been done right before");
2576 
2577   if (ShenandoahVerify &amp;&amp; !is_degenerated_gc_in_progress()) {
2578     verifier()-&gt;verify_roots_in_to_space_except(ShenandoahRootVerifier::ThreadRoots);
2579   }
2580 
2581   if (is_degenerated_gc_in_progress()) {
2582     concurrent_mark()-&gt;update_roots(ShenandoahPhaseTimings::degen_gc_update_roots);
2583   } else {
2584     concurrent_mark()-&gt;update_thread_roots(ShenandoahPhaseTimings::final_update_refs_roots);
2585   }
2586 
2587   // Has to be done before cset is clear
2588   if (ShenandoahVerify) {
2589     verifier()-&gt;verify_roots_in_to_space();
2590   }
2591 
2592   {
2593     ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_refs_update_region_states);
2594     ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl;
2595     parallel_heap_region_iterate(&amp;cl);
2596 
2597     assert_pinned_region_status();
2598   }
2599 
2600   {
2601     ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_refs_trash_cset);
2602     trash_cset_regions();
2603   }
2604 
2605   set_has_forwarded_objects(false);
2606   set_update_refs_in_progress(false);
2607 
2608   if (ShenandoahVerify) {
2609     verifier()-&gt;verify_after_updaterefs();
2610   }
2611 
2612   if (VerifyAfterGC) {
2613     Universe::verify();
2614   }
2615 
2616   {
2617     ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_refs_rebuild_freeset);
2618     ShenandoahHeapLocker locker(lock());
2619     _free_set-&gt;rebuild();
2620   }
2621 }
2622 
2623 void ShenandoahHeap::print_extended_on(outputStream *st) const {
2624   print_on(st);
2625   print_heap_regions_on(st);
2626 }
2627 
2628 bool ShenandoahHeap::is_bitmap_slice_committed(ShenandoahHeapRegion* r, bool skip_self) {
2629   size_t slice = r-&gt;index() / _bitmap_regions_per_slice;
2630 
2631   size_t regions_from = _bitmap_regions_per_slice * slice;
2632   size_t regions_to   = MIN2(num_regions(), _bitmap_regions_per_slice * (slice + 1));
2633   for (size_t g = regions_from; g &lt; regions_to; g++) {
2634     assert (g / _bitmap_regions_per_slice == slice, "same slice");
2635     if (skip_self &amp;&amp; g == r-&gt;index()) continue;
2636     if (get_region(g)-&gt;is_committed()) {
2637       return true;
2638     }
2639   }
2640   return false;
2641 }
2642 
2643 bool ShenandoahHeap::commit_bitmap_slice(ShenandoahHeapRegion* r) {
2644   shenandoah_assert_heaplocked();
2645 
2646   // Bitmaps in special regions do not need commits
2647   if (_bitmap_region_special) {
2648     return true;
2649   }
2650 
2651   if (is_bitmap_slice_committed(r, true)) {
2652     // Some other region from the group is already committed, meaning the bitmap
2653     // slice is already committed, we exit right away.
2654     return true;
2655   }
2656 
2657   // Commit the bitmap slice:
2658   size_t slice = r-&gt;index() / _bitmap_regions_per_slice;
2659   size_t off = _bitmap_bytes_per_slice * slice;
2660   size_t len = _bitmap_bytes_per_slice;
2661   char* start = (char*) _bitmap_region.start() + off;
2662 
2663   if (!os::commit_memory(start, len, false)) {
2664     return false;
2665   }
2666 
2667   if (AlwaysPreTouch) {
2668     os::pretouch_memory(start, start + len, _pretouch_bitmap_page_size);
2669   }
2670 
2671   return true;
2672 }
2673 
2674 bool ShenandoahHeap::uncommit_bitmap_slice(ShenandoahHeapRegion *r) {
2675   shenandoah_assert_heaplocked();
2676 
2677   // Bitmaps in special regions do not need uncommits
2678   if (_bitmap_region_special) {
2679     return true;
2680   }
2681 
2682   if (is_bitmap_slice_committed(r, true)) {
2683     // Some other region from the group is still committed, meaning the bitmap
2684     // slice is should stay committed, exit right away.
2685     return true;
2686   }
2687 
2688   // Uncommit the bitmap slice:
2689   size_t slice = r-&gt;index() / _bitmap_regions_per_slice;
2690   size_t off = _bitmap_bytes_per_slice * slice;
2691   size_t len = _bitmap_bytes_per_slice;
2692   if (!os::uncommit_memory((char*)_bitmap_region.start() + off, len)) {
2693     return false;
2694   }
2695   return true;
2696 }
2697 
2698 void ShenandoahHeap::safepoint_synchronize_begin() {
2699   if (ShenandoahSuspendibleWorkers || UseStringDeduplication) {
2700     SuspendibleThreadSet::synchronize();
2701   }
2702 }
2703 
2704 void ShenandoahHeap::safepoint_synchronize_end() {
2705   if (ShenandoahSuspendibleWorkers || UseStringDeduplication) {
2706     SuspendibleThreadSet::desynchronize();
2707   }
2708 }
2709 
2710 void ShenandoahHeap::vmop_entry_init_mark() {
2711   TraceCollectorStats tcs(monitoring_support()-&gt;stw_collection_counters());
2712   ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::init_mark_gross);
2713 
2714   try_inject_alloc_failure();
2715   VM_ShenandoahInitMark op;
2716   VMThread::execute(&amp;op); // jump to entry_init_mark() under safepoint
2717 }
2718 
2719 void ShenandoahHeap::vmop_entry_final_mark() {
2720   TraceCollectorStats tcs(monitoring_support()-&gt;stw_collection_counters());
2721   ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::final_mark_gross);
2722 
2723   try_inject_alloc_failure();
2724   VM_ShenandoahFinalMarkStartEvac op;
2725   VMThread::execute(&amp;op); // jump to entry_final_mark under safepoint
2726 }
2727 
2728 void ShenandoahHeap::vmop_entry_init_updaterefs() {
2729   TraceCollectorStats tcs(monitoring_support()-&gt;stw_collection_counters());
2730   ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::init_update_refs_gross);
2731 
2732   try_inject_alloc_failure();
2733   VM_ShenandoahInitUpdateRefs op;
2734   VMThread::execute(&amp;op);
2735 }
2736 
2737 void ShenandoahHeap::vmop_entry_final_updaterefs() {
2738   TraceCollectorStats tcs(monitoring_support()-&gt;stw_collection_counters());
2739   ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::final_update_refs_gross);
2740 
2741   try_inject_alloc_failure();
2742   VM_ShenandoahFinalUpdateRefs op;
2743   VMThread::execute(&amp;op);
2744 }
2745 
2746 void ShenandoahHeap::vmop_entry_full(GCCause::Cause cause) {
2747   TraceCollectorStats tcs(monitoring_support()-&gt;full_stw_collection_counters());
2748   ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::full_gc_gross);
2749 
2750   try_inject_alloc_failure();
2751   VM_ShenandoahFullGC op(cause);
2752   VMThread::execute(&amp;op);
2753 }
2754 
2755 void ShenandoahHeap::vmop_degenerated(ShenandoahDegenPoint point) {
2756   TraceCollectorStats tcs(monitoring_support()-&gt;full_stw_collection_counters());
2757   ShenandoahTimingsTracker timing(ShenandoahPhaseTimings::degen_gc_gross);
2758 
2759   VM_ShenandoahDegeneratedGC degenerated_gc((int)point);
2760   VMThread::execute(&amp;degenerated_gc);
2761 }
2762 
2763 void ShenandoahHeap::entry_init_mark() {
2764   const char* msg = init_mark_event_message();
2765   ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::init_mark);
2766   EventMark em("%s", msg);
2767 
2768   ShenandoahWorkerScope scope(workers(),
2769                               ShenandoahWorkerPolicy::calc_workers_for_init_marking(),
2770                               "init marking");
2771 
2772   op_init_mark();
2773 }
2774 
2775 void ShenandoahHeap::entry_final_mark() {
2776   const char* msg = final_mark_event_message();
2777   ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::final_mark);
2778   EventMark em("%s", msg);
2779 
2780   ShenandoahWorkerScope scope(workers(),
2781                               ShenandoahWorkerPolicy::calc_workers_for_final_marking(),
2782                               "final marking");
2783 
2784   op_final_mark();
2785 }
2786 
2787 void ShenandoahHeap::entry_init_updaterefs() {
2788   static const char* msg = "Pause Init Update Refs";
2789   ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::init_update_refs);
2790   EventMark em("%s", msg);
2791 
2792   // No workers used in this phase, no setup required
2793 
2794   op_init_updaterefs();
2795 }
2796 
2797 void ShenandoahHeap::entry_final_updaterefs() {
2798   static const char* msg = "Pause Final Update Refs";
2799   ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::final_update_refs);
2800   EventMark em("%s", msg);
2801 
2802   ShenandoahWorkerScope scope(workers(),
2803                               ShenandoahWorkerPolicy::calc_workers_for_final_update_ref(),
2804                               "final reference update");
2805 
2806   op_final_updaterefs();
2807 }
2808 
2809 void ShenandoahHeap::entry_full(GCCause::Cause cause) {
2810   static const char* msg = "Pause Full";
2811   ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::full_gc, true /* log_heap_usage */);
2812   EventMark em("%s", msg);
2813 
2814   ShenandoahWorkerScope scope(workers(),
2815                               ShenandoahWorkerPolicy::calc_workers_for_fullgc(),
2816                               "full gc");
2817 
2818   op_full(cause);
2819 }
2820 
2821 void ShenandoahHeap::entry_degenerated(int point) {
2822   ShenandoahDegenPoint dpoint = (ShenandoahDegenPoint)point;
2823   const char* msg = degen_event_message(dpoint);
2824   ShenandoahPausePhase gc_phase(msg, ShenandoahPhaseTimings::degen_gc, true /* log_heap_usage */);
2825   EventMark em("%s", msg);
2826 
2827   ShenandoahWorkerScope scope(workers(),
2828                               ShenandoahWorkerPolicy::calc_workers_for_stw_degenerated(),
2829                               "stw degenerated gc");
2830 
2831   set_degenerated_gc_in_progress(true);
2832   op_degenerated(dpoint);
2833   set_degenerated_gc_in_progress(false);
2834 }
2835 
2836 void ShenandoahHeap::entry_mark() {
2837   TraceCollectorStats tcs(monitoring_support()-&gt;concurrent_collection_counters());
2838 
2839   const char* msg = conc_mark_event_message();
2840   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_mark);
2841   EventMark em("%s", msg);
2842 
2843   ShenandoahWorkerScope scope(workers(),
2844                               ShenandoahWorkerPolicy::calc_workers_for_conc_marking(),
2845                               "concurrent marking");
2846 
2847   try_inject_alloc_failure();
2848   op_mark();
2849 }
2850 
2851 void ShenandoahHeap::entry_evac() {
2852   TraceCollectorStats tcs(monitoring_support()-&gt;concurrent_collection_counters());
2853 
2854   static const char* msg = "Concurrent evacuation";
2855   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_evac);
2856   EventMark em("%s", msg);
2857 
2858   ShenandoahWorkerScope scope(workers(),
2859                               ShenandoahWorkerPolicy::calc_workers_for_conc_evac(),
2860                               "concurrent evacuation");
2861 
2862   try_inject_alloc_failure();
2863   op_conc_evac();
2864 }
2865 
2866 void ShenandoahHeap::entry_updaterefs() {
2867   static const char* msg = "Concurrent update references";
2868   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_update_refs);
2869   EventMark em("%s", msg);
2870 
2871   ShenandoahWorkerScope scope(workers(),
2872                               ShenandoahWorkerPolicy::calc_workers_for_conc_update_ref(),
2873                               "concurrent reference update");
2874 
2875   try_inject_alloc_failure();
2876   op_updaterefs();
2877 }
2878 
2879 void ShenandoahHeap::entry_weak_roots() {
2880   static const char* msg = "Concurrent weak roots";
2881   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_weak_roots);
2882   EventMark em("%s", msg);
2883 
2884   ShenandoahWorkerScope scope(workers(),
2885                               ShenandoahWorkerPolicy::calc_workers_for_conc_root_processing(),
2886                               "concurrent weak root");
2887 
2888   try_inject_alloc_failure();
2889   op_weak_roots();
2890 }
2891 
2892 void ShenandoahHeap::entry_class_unloading() {
2893   static const char* msg = "Concurrent class unloading";
2894   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_class_unload);
2895   EventMark em("%s", msg);
2896 
2897   ShenandoahWorkerScope scope(workers(),
2898                               ShenandoahWorkerPolicy::calc_workers_for_conc_root_processing(),
2899                               "concurrent class unloading");
2900 
2901   try_inject_alloc_failure();
2902   op_class_unloading();
2903 }
2904 
2905 void ShenandoahHeap::entry_strong_roots() {
2906   static const char* msg = "Concurrent strong roots";
2907   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_strong_roots);
2908   EventMark em("%s", msg);
2909 
2910   ShenandoahGCWorkerPhase worker_phase(ShenandoahPhaseTimings::conc_strong_roots);
2911 
2912   ShenandoahWorkerScope scope(workers(),
2913                               ShenandoahWorkerPolicy::calc_workers_for_conc_root_processing(),
2914                               "concurrent strong root");
2915 
2916   try_inject_alloc_failure();
2917   op_strong_roots();
2918 }
2919 
2920 void ShenandoahHeap::entry_cleanup_early() {
2921   static const char* msg = "Concurrent cleanup";
2922   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_cleanup_early, true /* log_heap_usage */);
2923   EventMark em("%s", msg);
2924 
2925   // This phase does not use workers, no need for setup
2926 
2927   try_inject_alloc_failure();
2928   op_cleanup_early();
2929 }
2930 
2931 void ShenandoahHeap::entry_rendezvous_roots() {
2932   static const char* msg = "Rendezvous roots";
2933   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_rendezvous_roots);
2934   EventMark em("%s", msg);
2935 
2936   // This phase does not use workers, no need for setup
2937   try_inject_alloc_failure();
2938   op_rendezvous_roots();
2939 }
2940 
2941 void ShenandoahHeap::entry_cleanup_complete() {
2942   static const char* msg = "Concurrent cleanup";
2943   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_cleanup_complete, true /* log_heap_usage */);
2944   EventMark em("%s", msg);
2945 
2946   // This phase does not use workers, no need for setup
2947 
2948   try_inject_alloc_failure();
2949   op_cleanup_complete();
2950 }
2951 
2952 void ShenandoahHeap::entry_reset() {
2953   static const char* msg = "Concurrent reset";
2954   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset);
2955   EventMark em("%s", msg);
2956 
2957   ShenandoahWorkerScope scope(workers(),
2958                               ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),
2959                               "concurrent reset");
2960 
2961   try_inject_alloc_failure();
2962   op_reset();
2963 }
2964 
2965 void ShenandoahHeap::entry_preclean() {
2966   if (ShenandoahPreclean &amp;&amp; process_references()) {
2967     static const char* msg = "Concurrent precleaning";
2968     ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_preclean);
2969     EventMark em("%s", msg);
2970 
2971     ShenandoahWorkerScope scope(workers(),
2972                                 ShenandoahWorkerPolicy::calc_workers_for_conc_preclean(),
2973                                 "concurrent preclean",
2974                                 /* check_workers = */ false);
2975 
2976     try_inject_alloc_failure();
2977     op_preclean();
2978   }
2979 }
2980 
2981 void ShenandoahHeap::entry_uncommit(double shrink_before, size_t shrink_until) {
2982   static const char *msg = "Concurrent uncommit";
2983   ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_uncommit, true /* log_heap_usage */);
2984   EventMark em("%s", msg);
2985 
2986   op_uncommit(shrink_before, shrink_until);
2987 }
2988 
2989 void ShenandoahHeap::try_inject_alloc_failure() {
2990   if (ShenandoahAllocFailureALot &amp;&amp; !cancelled_gc() &amp;&amp; ((os::random() % 1000) &gt; 950)) {
2991     _inject_alloc_failure.set();
2992     os::naked_short_sleep(1);
2993     if (cancelled_gc()) {
2994       log_info(gc)("Allocation failure was successfully injected");
2995     }
2996   }
2997 }
2998 
2999 bool ShenandoahHeap::should_inject_alloc_failure() {
3000   return _inject_alloc_failure.is_set() &amp;&amp; _inject_alloc_failure.try_unset();
3001 }
3002 
3003 void ShenandoahHeap::initialize_serviceability() {
3004   _memory_pool = new ShenandoahMemoryPool(this);
3005   _cycle_memory_manager.add_pool(_memory_pool);
3006   _stw_memory_manager.add_pool(_memory_pool);
3007 }
3008 
3009 GrowableArray&lt;GCMemoryManager*&gt; ShenandoahHeap::memory_managers() {
3010   GrowableArray&lt;GCMemoryManager*&gt; memory_managers(2);
3011   memory_managers.append(&amp;_cycle_memory_manager);
3012   memory_managers.append(&amp;_stw_memory_manager);
3013   return memory_managers;
3014 }
3015 
3016 GrowableArray&lt;MemoryPool*&gt; ShenandoahHeap::memory_pools() {
3017   GrowableArray&lt;MemoryPool*&gt; memory_pools(1);
3018   memory_pools.append(_memory_pool);
3019   return memory_pools;
3020 }
3021 
3022 MemoryUsage ShenandoahHeap::memory_usage() {
3023   return _memory_pool-&gt;get_memory_usage();
3024 }
3025 
3026 ShenandoahRegionIterator::ShenandoahRegionIterator() :
3027   _heap(ShenandoahHeap::heap()),
3028   _index(0) {}
3029 
3030 ShenandoahRegionIterator::ShenandoahRegionIterator(ShenandoahHeap* heap) :
3031   _heap(heap),
3032   _index(0) {}
3033 
3034 void ShenandoahRegionIterator::reset() {
3035   _index = 0;
3036 }
3037 
3038 bool ShenandoahRegionIterator::has_next() const {
3039   return _index &lt; _heap-&gt;num_regions();
3040 }
3041 
3042 char ShenandoahHeap::gc_state() const {
3043   return _gc_state.raw_value();
3044 }
3045 
3046 void ShenandoahHeap::deduplicate_string(oop str) {
3047   assert(java_lang_String::is_instance(str), "invariant");
3048 
3049   if (ShenandoahStringDedup::is_enabled()) {
3050     ShenandoahStringDedup::deduplicate(str);
3051   }
3052 }
3053 
3054 const char* ShenandoahHeap::init_mark_event_message() const {
3055   assert(!has_forwarded_objects(), "Should not have forwarded objects here");
3056 
3057   bool proc_refs = process_references();
3058   bool unload_cls = unload_classes();
3059 
3060   if (proc_refs &amp;&amp; unload_cls) {
3061     return "Pause Init Mark (process weakrefs) (unload classes)";
3062   } else if (proc_refs) {
3063     return "Pause Init Mark (process weakrefs)";
3064   } else if (unload_cls) {
3065     return "Pause Init Mark (unload classes)";
3066   } else {
3067     return "Pause Init Mark";
3068   }
3069 }
3070 
3071 const char* ShenandoahHeap::final_mark_event_message() const {
3072   assert(!has_forwarded_objects(), "Should not have forwarded objects here");
3073 
3074   bool proc_refs = process_references();
3075   bool unload_cls = unload_classes();
3076 
3077   if (proc_refs &amp;&amp; unload_cls) {
3078     return "Pause Final Mark (process weakrefs) (unload classes)";
3079   } else if (proc_refs) {
3080     return "Pause Final Mark (process weakrefs)";
3081   } else if (unload_cls) {
3082     return "Pause Final Mark (unload classes)";
3083   } else {
3084     return "Pause Final Mark";
3085   }
3086 }
3087 
3088 const char* ShenandoahHeap::conc_mark_event_message() const {
3089   assert(!has_forwarded_objects(), "Should not have forwarded objects here");
3090 
3091   bool proc_refs = process_references();
3092   bool unload_cls = unload_classes();
3093 
3094   if (proc_refs &amp;&amp; unload_cls) {
3095     return "Concurrent marking (process weakrefs) (unload classes)";
3096   } else if (proc_refs) {
3097     return "Concurrent marking (process weakrefs)";
3098   } else if (unload_cls) {
3099     return "Concurrent marking (unload classes)";
3100   } else {
3101     return "Concurrent marking";
3102   }
3103 }
3104 
3105 const char* ShenandoahHeap::degen_event_message(ShenandoahDegenPoint point) const {
3106   switch (point) {
3107     case _degenerated_unset:
3108       return "Pause Degenerated GC (&lt;UNSET&gt;)";
3109     case _degenerated_outside_cycle:
3110       return "Pause Degenerated GC (Outside of Cycle)";
3111     case _degenerated_mark:
3112       return "Pause Degenerated GC (Mark)";
3113     case _degenerated_evac:
3114       return "Pause Degenerated GC (Evacuation)";
3115     case _degenerated_updaterefs:
3116       return "Pause Degenerated GC (Update Refs)";
3117     default:
3118       ShouldNotReachHere();
3119       return "ERROR";
3120   }
3121 }
3122 
3123 ShenandoahLiveData* ShenandoahHeap::get_liveness_cache(uint worker_id) {
3124 #ifdef ASSERT
3125   assert(_liveness_cache != NULL, "sanity");
3126   assert(worker_id &lt; _max_workers, "sanity");
3127   for (uint i = 0; i &lt; num_regions(); i++) {
3128     assert(_liveness_cache[worker_id][i] == 0, "liveness cache should be empty");
3129   }
3130 #endif
3131   return _liveness_cache[worker_id];
3132 }
3133 
3134 void ShenandoahHeap::flush_liveness_cache(uint worker_id) {
3135   assert(worker_id &lt; _max_workers, "sanity");
3136   assert(_liveness_cache != NULL, "sanity");
3137   ShenandoahLiveData* ld = _liveness_cache[worker_id];
3138   for (uint i = 0; i &lt; num_regions(); i++) {
3139     ShenandoahLiveData live = ld[i];
3140     if (live &gt; 0) {
3141       ShenandoahHeapRegion* r = get_region(i);
3142       r-&gt;increase_live_data_gc_words(live);
3143       ld[i] = 0;
3144     }
3145   }
3146 }
</pre></body></html>
