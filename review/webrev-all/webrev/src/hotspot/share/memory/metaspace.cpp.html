<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>New src/hotspot/share/memory/metaspace.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 2011, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 
  27 #include "aot/aotLoader.hpp"
  28 #include "gc/shared/collectedHeap.hpp"
  29 #include "logging/log.hpp"
  30 #include "logging/logStream.hpp"
  31 #include "memory/filemap.hpp"
  32 #include "memory/metaspace/metaspaceSizesSnapshot.hpp"
  33 #include "memory/metaspace/msChunkHeaderPool.hpp"
  34 #include "memory/metaspace/msChunkManager.hpp"
  35 #include "memory/metaspace/msCommitLimiter.hpp"
  36 #include "memory/metaspace/msCommon.hpp"
  37 #include "memory/metaspace/msContext.hpp"
  38 #include "memory/metaspace/msReport.hpp"
  39 #include "memory/metaspace/msRunningCounters.hpp"
  40 #include "memory/metaspace/msSettings.hpp"
  41 #include "memory/metaspace/msVirtualSpaceList.hpp"
  42 #include "memory/metaspace.hpp"
  43 #include "memory/metaspaceShared.hpp"
  44 #include "memory/metaspaceTracer.hpp"
  45 #include "memory/universe.hpp"
  46 #include "oops/compressedOops.hpp"
  47 #include "runtime/arguments.hpp"
  48 #include "runtime/atomic.hpp"
  49 #include "runtime/init.hpp"
  50 #include "runtime/java.hpp"
  51 #include "services/memTracker.hpp"
  52 #include "utilities/copy.hpp"
  53 #include "utilities/debug.hpp"
  54 #include "utilities/formatBuffer.hpp"
  55 #include "utilities/globalDefinitions.hpp"
  56 
  57 
  58 using metaspace::ChunkManager;
  59 using metaspace::CommitLimiter;
  60 using metaspace::MetaspaceContext;
  61 using metaspace::MetaspaceReporter;
  62 using metaspace::RunningCounters;
  63 using metaspace::VirtualSpaceList;
  64 
  65 
  66 size_t MetaspaceUtils::used_words() {
  67   return RunningCounters::used_words();
  68 }
  69 
  70 size_t MetaspaceUtils::used_words(Metaspace::MetadataType mdtype) {
  71   return Metaspace::is_class_space_allocation(mdtype) ? RunningCounters::used_words_class() : RunningCounters::used_words_nonclass();
  72 }
  73 
  74 size_t MetaspaceUtils::reserved_words() {
  75   return RunningCounters::reserved_words();
  76 }
  77 
  78 size_t MetaspaceUtils::reserved_words(Metaspace::MetadataType mdtype) {
  79   return Metaspace::is_class_space_allocation(mdtype) ? RunningCounters::reserved_words_class() : RunningCounters::reserved_words_nonclass();
  80 }
  81 
  82 size_t MetaspaceUtils::committed_words() {
  83   return RunningCounters::committed_words();
  84 }
  85 
  86 size_t MetaspaceUtils::committed_words(Metaspace::MetadataType mdtype) {
  87   return Metaspace::is_class_space_allocation(mdtype) ? RunningCounters::committed_words_class() : RunningCounters::committed_words_nonclass();
  88 }
  89 
  90 
  91 
  92 void MetaspaceUtils::print_metaspace_change(const metaspace::MetaspaceSizesSnapshot&amp; pre_meta_values) {
  93   const metaspace::MetaspaceSizesSnapshot meta_values;
  94 
  95   // We print used and committed since these are the most useful at-a-glance vitals for Metaspace:
  96   // - used tells you how much memory is actually used for metadata
  97   // - committed tells you how much memory is committed for the purpose of metadata
  98   // The difference between those two would be waste, which can have various forms (freelists,
  99   //   unused parts of committed chunks etc)
 100   //
 101   // Left out is reserved, since this is not as exciting as the first two values: for class space,
 102   // it is a constant (to uninformed users, often confusingly large). For non-class space, it would
 103   // be interesting since free chunks can be uncommitted, but for now it is left out.
 104 
 105   if (Metaspace::using_class_space()) {
 106     log_info(gc, metaspace)(HEAP_CHANGE_FORMAT" "
 107                             HEAP_CHANGE_FORMAT" "
 108                             HEAP_CHANGE_FORMAT,
 109                             HEAP_CHANGE_FORMAT_ARGS("Metaspace",
 110                                                     pre_meta_values.used(),
 111                                                     pre_meta_values.committed(),
 112                                                     meta_values.used(),
 113                                                     meta_values.committed()),
 114                             HEAP_CHANGE_FORMAT_ARGS("NonClass",
 115                                                     pre_meta_values.non_class_used(),
 116                                                     pre_meta_values.non_class_committed(),
 117                                                     meta_values.non_class_used(),
 118                                                     meta_values.non_class_committed()),
 119                             HEAP_CHANGE_FORMAT_ARGS("Class",
 120                                                     pre_meta_values.class_used(),
 121                                                     pre_meta_values.class_committed(),
 122                                                     meta_values.class_used(),
 123                                                     meta_values.class_committed()));
 124   } else {
 125     log_info(gc, metaspace)(HEAP_CHANGE_FORMAT,
 126                             HEAP_CHANGE_FORMAT_ARGS("Metaspace",
 127                                                     pre_meta_values.used(),
 128                                                     pre_meta_values.committed(),
 129                                                     meta_values.used(),
 130                                                     meta_values.committed()));
 131   }
 132 }
 133 
 134 // This will print out a basic metaspace usage report but
 135 // unlike print_report() is guaranteed not to lock or to walk the CLDG.
 136 void MetaspaceUtils::print_basic_report(outputStream* out, size_t scale) {
 137   MetaspaceReporter::print_basic_report(out, scale);
 138 }
 139 
 140 // Prints a report about the current metaspace state.
 141 // Optional parts can be enabled via flags.
 142 // Function will walk the CLDG and will lock the expand lock; if that is not
 143 // convenient, use print_basic_report() instead.
 144 void MetaspaceUtils::print_report(outputStream* out, size_t scale) {
 145   const int flags =
 146       (int)MetaspaceReporter::Option::ShowLoaders |
 147       (int)MetaspaceReporter::Option::BreakDownByChunkType |
 148       (int)MetaspaceReporter::Option::ShowClasses;
 149   MetaspaceReporter::print_report(out, scale, flags);
 150 }
 151 
 152 void MetaspaceUtils::print_on(outputStream* out) {
 153 
 154   // Used from all GCs. It first prints out totals, then, separately, the class space portion.
 155 
 156   out-&gt;print_cr(" Metaspace       "
 157                 "used "      SIZE_FORMAT "K, "
 158                 "committed " SIZE_FORMAT "K, "
 159                 "reserved "  SIZE_FORMAT "K",
 160                 used_bytes()/K,
 161                 committed_bytes()/K,
 162                 reserved_bytes()/K);
 163 
 164   if (Metaspace::using_class_space()) {
 165     const Metaspace::MetadataType ct = Metaspace::ClassType;
 166     out-&gt;print_cr("  class space    "
 167                   "used "      SIZE_FORMAT "K, "
 168                   "committed " SIZE_FORMAT "K, "
 169                   "reserved "  SIZE_FORMAT "K",
 170                   used_bytes(ct)/K,
 171                   committed_bytes(ct)/K,
 172                   reserved_bytes(ct)/K);
 173   }
 174 }
 175 
 176 #ifdef ASSERT
 177 void MetaspaceUtils::verify() {
 178   if (Metaspace::initialized()) {
 179 
 180     // Verify non-class chunkmanager...
 181     ChunkManager* cm = ChunkManager::chunkmanager_nonclass();
 182     cm-&gt;verify();
 183 
 184     // ... and space list.
 185     VirtualSpaceList* vsl = VirtualSpaceList::vslist_nonclass();
 186     vsl-&gt;verify();
 187 
 188     if (Metaspace::using_class_space()) {
 189       // If we use compressed class pointers, verify class chunkmanager...
 190       cm = ChunkManager::chunkmanager_class();
 191       cm-&gt;verify();
 192 
 193       // ... and class spacelist.
 194       vsl = VirtualSpaceList::vslist_class();
 195       vsl-&gt;verify();
 196     }
 197 
 198   }
 199 }
 200 #endif
 201 
 202 ////////////////////////////////7
 203 // MetaspaceGC methods
 204 
 205 volatile size_t MetaspaceGC::_capacity_until_GC = 0;
 206 uint MetaspaceGC::_shrink_factor = 0;
 207 
 208 // VM_CollectForMetadataAllocation is the vm operation used to GC.
 209 // Within the VM operation after the GC the attempt to allocate the metadata
 210 // should succeed.  If the GC did not free enough space for the metaspace
 211 // allocation, the HWM is increased so that another virtualspace will be
 212 // allocated for the metadata.  With perm gen the increase in the perm
 213 // gen had bounds, MinMetaspaceExpansion and MaxMetaspaceExpansion.  The
 214 // metaspace policy uses those as the small and large steps for the HWM.
 215 //
 216 // After the GC the compute_new_size() for MetaspaceGC is called to
 217 // resize the capacity of the metaspaces.  The current implementation
 218 // is based on the flags MinMetaspaceFreeRatio and MaxMetaspaceFreeRatio used
 219 // to resize the Java heap by some GC's.  New flags can be implemented
 220 // if really needed.  MinMetaspaceFreeRatio is used to calculate how much
 221 // free space is desirable in the metaspace capacity to decide how much
 222 // to increase the HWM.  MaxMetaspaceFreeRatio is used to decide how much
 223 // free space is desirable in the metaspace capacity before decreasing
 224 // the HWM.
 225 
 226 // Calculate the amount to increase the high water mark (HWM).
 227 // Increase by a minimum amount (MinMetaspaceExpansion) so that
 228 // another expansion is not requested too soon.  If that is not
 229 // enough to satisfy the allocation, increase by MaxMetaspaceExpansion.
 230 // If that is still not enough, expand by the size of the allocation
 231 // plus some.
 232 size_t MetaspaceGC::delta_capacity_until_GC(size_t bytes) {
 233   size_t min_delta = MinMetaspaceExpansion;
 234   size_t max_delta = MaxMetaspaceExpansion;
 235   size_t delta = align_up(bytes, Metaspace::commit_alignment());
 236 
 237   if (delta &lt;= min_delta) {
 238     delta = min_delta;
 239   } else if (delta &lt;= max_delta) {
 240     // Don't want to hit the high water mark on the next
 241     // allocation so make the delta greater than just enough
 242     // for this allocation.
 243     delta = max_delta;
 244   } else {
 245     // This allocation is large but the next ones are probably not
 246     // so increase by the minimum.
 247     delta = delta + min_delta;
 248   }
 249 
 250   assert_is_aligned(delta, Metaspace::commit_alignment());
 251 
 252   return delta;
 253 }
 254 
 255 size_t MetaspaceGC::capacity_until_GC() {
 256   size_t value = Atomic::load_acquire(&amp;_capacity_until_GC);
 257   assert(value &gt;= MetaspaceSize, "Not initialized properly?");
 258   return value;
 259 }
 260 
 261 // Try to increase the _capacity_until_GC limit counter by v bytes.
 262 // Returns true if it succeeded. It may fail if either another thread
 263 // concurrently increased the limit or the new limit would be larger
 264 // than MaxMetaspaceSize.
 265 // On success, optionally returns new and old metaspace capacity in
 266 // new_cap_until_GC and old_cap_until_GC respectively.
 267 // On error, optionally sets can_retry to indicate whether if there is
 268 // actually enough space remaining to satisfy the request.
 269 bool MetaspaceGC::inc_capacity_until_GC(size_t v, size_t* new_cap_until_GC, size_t* old_cap_until_GC, bool* can_retry) {
 270   assert_is_aligned(v, Metaspace::commit_alignment());
 271 
 272   size_t old_capacity_until_GC = _capacity_until_GC;
 273   size_t new_value = old_capacity_until_GC + v;
 274 
 275   if (new_value &lt; old_capacity_until_GC) {
 276     // The addition wrapped around, set new_value to aligned max value.
 277     new_value = align_down(max_uintx, Metaspace::commit_alignment());
 278   }
 279 
 280   if (new_value &gt; MaxMetaspaceSize) {
 281     if (can_retry != NULL) {
 282       *can_retry = false;
 283     }
 284     return false;
 285   }
 286 
 287   if (can_retry != NULL) {
 288     *can_retry = true;
 289   }
 290   size_t prev_value = Atomic::cmpxchg(&amp;_capacity_until_GC, old_capacity_until_GC, new_value);
 291 
 292   if (old_capacity_until_GC != prev_value) {
 293     return false;
 294   }
 295 
 296   if (new_cap_until_GC != NULL) {
 297     *new_cap_until_GC = new_value;
 298   }
 299   if (old_cap_until_GC != NULL) {
 300     *old_cap_until_GC = old_capacity_until_GC;
 301   }
 302   return true;
 303 }
 304 
 305 size_t MetaspaceGC::dec_capacity_until_GC(size_t v) {
 306   assert_is_aligned(v, Metaspace::commit_alignment());
 307 
 308   return Atomic::sub(&amp;_capacity_until_GC, v);
 309 }
 310 
 311 void MetaspaceGC::initialize() {
 312   // Set the high-water mark to MaxMetapaceSize during VM initializaton since
 313   // we can't do a GC during initialization.
 314   _capacity_until_GC = MaxMetaspaceSize;
 315 }
 316 
 317 void MetaspaceGC::post_initialize() {
 318   // Reset the high-water mark once the VM initialization is done.
 319   _capacity_until_GC = MAX2(MetaspaceUtils::committed_bytes(), MetaspaceSize);
 320 }
 321 
 322 bool MetaspaceGC::can_expand(size_t word_size, bool is_class) {
 323   // Check if the compressed class space is full.
 324   if (is_class &amp;&amp; Metaspace::using_class_space()) {
 325     size_t class_committed = MetaspaceUtils::committed_bytes(Metaspace::ClassType);
 326     if (class_committed + word_size * BytesPerWord &gt; CompressedClassSpaceSize) {
 327       log_trace(gc, metaspace, freelist)("Cannot expand %s metaspace by " SIZE_FORMAT " words (CompressedClassSpaceSize = " SIZE_FORMAT " words)",
 328                 (is_class ? "class" : "non-class"), word_size, CompressedClassSpaceSize / sizeof(MetaWord));
 329       return false;
 330     }
 331   }
 332 
 333   // Check if the user has imposed a limit on the metaspace memory.
 334   size_t committed_bytes = MetaspaceUtils::committed_bytes();
 335   if (committed_bytes + word_size * BytesPerWord &gt; MaxMetaspaceSize) {
 336     log_trace(gc, metaspace, freelist)("Cannot expand %s metaspace by " SIZE_FORMAT " words (MaxMetaspaceSize = " SIZE_FORMAT " words)",
 337               (is_class ? "class" : "non-class"), word_size, MaxMetaspaceSize / sizeof(MetaWord));
 338     return false;
 339   }
 340 
 341   return true;
 342 }
 343 
 344 size_t MetaspaceGC::allowed_expansion() {
 345   size_t committed_bytes = MetaspaceUtils::committed_bytes();
 346   size_t capacity_until_gc = capacity_until_GC();
 347 
 348   assert(capacity_until_gc &gt;= committed_bytes,
 349          "capacity_until_gc: " SIZE_FORMAT " &lt; committed_bytes: " SIZE_FORMAT,
 350          capacity_until_gc, committed_bytes);
 351 
 352   size_t left_until_max  = MaxMetaspaceSize - committed_bytes;
 353   size_t left_until_GC = capacity_until_gc - committed_bytes;
 354   size_t left_to_commit = MIN2(left_until_GC, left_until_max);
 355   log_trace(gc, metaspace, freelist)("allowed expansion words: " SIZE_FORMAT
 356             " (left_until_max: " SIZE_FORMAT ", left_until_GC: " SIZE_FORMAT ".",
 357             left_to_commit / BytesPerWord, left_until_max / BytesPerWord, left_until_GC / BytesPerWord);
 358 
 359   return left_to_commit / BytesPerWord;
 360 }
 361 
 362 void MetaspaceGC::compute_new_size() {
 363   assert(_shrink_factor &lt;= 100, "invalid shrink factor");
 364   uint current_shrink_factor = _shrink_factor;
 365   _shrink_factor = 0;
 366 
 367   // Using committed_bytes() for used_after_gc is an overestimation, since the
 368   // chunk free lists are included in committed_bytes() and the memory in an
 369   // un-fragmented chunk free list is available for future allocations.
 370   // However, if the chunk free lists becomes fragmented, then the memory may
 371   // not be available for future allocations and the memory is therefore "in use".
 372   // Including the chunk free lists in the definition of "in use" is therefore
 373   // necessary. Not including the chunk free lists can cause capacity_until_GC to
 374   // shrink below committed_bytes() and this has caused serious bugs in the past.
 375   const size_t used_after_gc = MetaspaceUtils::committed_bytes();
 376   const size_t capacity_until_GC = MetaspaceGC::capacity_until_GC();
 377 
 378   const double minimum_free_percentage = MinMetaspaceFreeRatio / 100.0;
 379   const double maximum_used_percentage = 1.0 - minimum_free_percentage;
 380 
 381   const double min_tmp = used_after_gc / maximum_used_percentage;
 382   size_t minimum_desired_capacity =
 383     (size_t)MIN2(min_tmp, double(MaxMetaspaceSize));
 384   // Don't shrink less than the initial generation size
 385   minimum_desired_capacity = MAX2(minimum_desired_capacity,
 386                                   MetaspaceSize);
 387 
 388   log_trace(gc, metaspace)("MetaspaceGC::compute_new_size: ");
 389   log_trace(gc, metaspace)("    minimum_free_percentage: %6.2f  maximum_used_percentage: %6.2f",
 390                            minimum_free_percentage, maximum_used_percentage);
 391   log_trace(gc, metaspace)("     used_after_gc       : %6.1fKB", used_after_gc / (double) K);
 392 
 393 
 394   size_t shrink_bytes = 0;
 395   if (capacity_until_GC &lt; minimum_desired_capacity) {
 396     // If we have less capacity below the metaspace HWM, then
 397     // increment the HWM.
 398     size_t expand_bytes = minimum_desired_capacity - capacity_until_GC;
 399     expand_bytes = align_up(expand_bytes, Metaspace::commit_alignment());
 400     // Don't expand unless it's significant
 401     if (expand_bytes &gt;= MinMetaspaceExpansion) {
 402       size_t new_capacity_until_GC = 0;
 403       bool succeeded = MetaspaceGC::inc_capacity_until_GC(expand_bytes, &amp;new_capacity_until_GC);
 404       assert(succeeded, "Should always succesfully increment HWM when at safepoint");
 405 
 406       Metaspace::tracer()-&gt;report_gc_threshold(capacity_until_GC,
 407                                                new_capacity_until_GC,
 408                                                MetaspaceGCThresholdUpdater::ComputeNewSize);
 409       log_trace(gc, metaspace)("    expanding:  minimum_desired_capacity: %6.1fKB  expand_bytes: %6.1fKB  MinMetaspaceExpansion: %6.1fKB  new metaspace HWM:  %6.1fKB",
 410                                minimum_desired_capacity / (double) K,
 411                                expand_bytes / (double) K,
 412                                MinMetaspaceExpansion / (double) K,
 413                                new_capacity_until_GC / (double) K);
 414     }
 415     return;
 416   }
 417 
 418   // No expansion, now see if we want to shrink
 419   // We would never want to shrink more than this
 420   assert(capacity_until_GC &gt;= minimum_desired_capacity,
 421          SIZE_FORMAT " &gt;= " SIZE_FORMAT,
 422          capacity_until_GC, minimum_desired_capacity);
 423   size_t max_shrink_bytes = capacity_until_GC - minimum_desired_capacity;
 424 
 425   // Should shrinking be considered?
 426   if (MaxMetaspaceFreeRatio &lt; 100) {
 427     const double maximum_free_percentage = MaxMetaspaceFreeRatio / 100.0;
 428     const double minimum_used_percentage = 1.0 - maximum_free_percentage;
 429     const double max_tmp = used_after_gc / minimum_used_percentage;
 430     size_t maximum_desired_capacity = (size_t)MIN2(max_tmp, double(MaxMetaspaceSize));
 431     maximum_desired_capacity = MAX2(maximum_desired_capacity,
 432                                     MetaspaceSize);
 433     log_trace(gc, metaspace)("    maximum_free_percentage: %6.2f  minimum_used_percentage: %6.2f",
 434                              maximum_free_percentage, minimum_used_percentage);
 435     log_trace(gc, metaspace)("    minimum_desired_capacity: %6.1fKB  maximum_desired_capacity: %6.1fKB",
 436                              minimum_desired_capacity / (double) K, maximum_desired_capacity / (double) K);
 437 
 438     assert(minimum_desired_capacity &lt;= maximum_desired_capacity,
 439            "sanity check");
 440 
 441     if (capacity_until_GC &gt; maximum_desired_capacity) {
 442       // Capacity too large, compute shrinking size
 443       shrink_bytes = capacity_until_GC - maximum_desired_capacity;
 444       // We don't want shrink all the way back to initSize if people call
 445       // System.gc(), because some programs do that between "phases" and then
 446       // we'd just have to grow the heap up again for the next phase.  So we
 447       // damp the shrinking: 0% on the first call, 10% on the second call, 40%
 448       // on the third call, and 100% by the fourth call.  But if we recompute
 449       // size without shrinking, it goes back to 0%.
 450       shrink_bytes = shrink_bytes / 100 * current_shrink_factor;
 451 
 452       shrink_bytes = align_down(shrink_bytes, Metaspace::commit_alignment());
 453 
 454       assert(shrink_bytes &lt;= max_shrink_bytes,
 455              "invalid shrink size " SIZE_FORMAT " not &lt;= " SIZE_FORMAT,
 456              shrink_bytes, max_shrink_bytes);
 457       if (current_shrink_factor == 0) {
 458         _shrink_factor = 10;
 459       } else {
 460         _shrink_factor = MIN2(current_shrink_factor * 4, (uint) 100);
 461       }
 462       log_trace(gc, metaspace)("    shrinking:  initThreshold: %.1fK  maximum_desired_capacity: %.1fK",
 463                                MetaspaceSize / (double) K, maximum_desired_capacity / (double) K);
 464       log_trace(gc, metaspace)("    shrink_bytes: %.1fK  current_shrink_factor: %d  new shrink factor: %d  MinMetaspaceExpansion: %.1fK",
 465                                shrink_bytes / (double) K, current_shrink_factor, _shrink_factor, MinMetaspaceExpansion / (double) K);
 466     }
 467   }
 468 
 469   // Don't shrink unless it's significant
 470   if (shrink_bytes &gt;= MinMetaspaceExpansion &amp;&amp;
 471       ((capacity_until_GC - shrink_bytes) &gt;= MetaspaceSize)) {
 472     size_t new_capacity_until_GC = MetaspaceGC::dec_capacity_until_GC(shrink_bytes);
 473     Metaspace::tracer()-&gt;report_gc_threshold(capacity_until_GC,
 474                                              new_capacity_until_GC,
 475                                              MetaspaceGCThresholdUpdater::ComputeNewSize);
 476   }
 477 }
 478 
 479 
 480 
 481 //////  Metaspace methods /////
 482 
 483 const MetaspaceTracer* Metaspace::_tracer = NULL;
 484 
 485 DEBUG_ONLY(bool Metaspace::_frozen = false;)
 486 
 487 bool Metaspace::initialized() {
 488   return metaspace::MetaspaceContext::context_nonclass() != NULL &amp;&amp;
 489       (using_class_space() ? metaspace::MetaspaceContext::context_class() != NULL : true);
 490 }
 491 
 492 #ifdef _LP64
 493 
 494 void Metaspace::print_compressed_class_space(outputStream* st) {
 495   if (VirtualSpaceList::vslist_class() != NULL) {
 496     MetaWord* base = VirtualSpaceList::vslist_class()-&gt;base_of_first_node();
 497     size_t size = VirtualSpaceList::vslist_class()-&gt;word_size_of_first_node();
 498     MetaWord* top = base + size;
 499     st-&gt;print("Compressed class space mapped at: " PTR_FORMAT "-" PTR_FORMAT ", reserved size: " SIZE_FORMAT,
 500                p2i(base), p2i(top), (top - base) * BytesPerWord);
 501     st-&gt;cr();
 502   }
 503 }
 504 
 505 // Given a prereserved space, use that to set up the compressed class space list.
 506 void Metaspace::initialize_class_space(ReservedSpace rs) {
 507   assert(rs.size() &gt;= CompressedClassSpaceSize,
 508          SIZE_FORMAT " != " SIZE_FORMAT, rs.size(), CompressedClassSpaceSize);
 509   assert(using_class_space(), "Must be using class space");
 510 
 511   assert(rs.size() == CompressedClassSpaceSize, SIZE_FORMAT " != " SIZE_FORMAT,
 512          rs.size(), CompressedClassSpaceSize);
 513   assert(is_aligned(rs.base(), Metaspace::reserve_alignment()) &amp;&amp;
 514          is_aligned(rs.size(), Metaspace::reserve_alignment()),
 515          "wrong alignment");
 516 
 517   MetaspaceContext::initialize_class_space_context(rs);
 518 
 519   // This does currently not work because rs may be the result of a split
 520   // operation and NMT seems not to be able to handle splits.
 521   // Will be fixed with JDK-8243535.
 522   // MemTracker::record_virtual_memory_type((address)rs.base(), mtClass);
 523 
 524 }
 525 
 526 // Returns true if class space has been setup (initialize_class_space).
 527 bool Metaspace::class_space_is_initialized() {
 528   return MetaspaceContext::context_class() != NULL;
 529 }
 530 
 531 // Reserve a range of memory at an address suitable for en/decoding narrow
 532 // Klass pointers (see: CompressedClassPointers::is_valid_base()).
 533 // The returned address shall both be suitable as a compressed class pointers
 534 //  base, and aligned to Metaspace::reserve_alignment (which is equal to or a
 535 //  multiple of allocation granularity).
 536 // On error, returns an unreserved space.
 537 ReservedSpace Metaspace::reserve_address_space_for_compressed_classes(size_t size) {
 538 
 539 #ifdef AARCH64
 540   const size_t alignment = Metaspace::reserve_alignment();
 541 
 542   // AArch64: Try to align metaspace so that we can decode a compressed
 543   // klass with a single MOVK instruction. We can do this iff the
 544   // compressed class base is a multiple of 4G.
 545   // Additionally, above 32G, ensure the lower LogKlassAlignmentInBytes bits
 546   // of the upper 32-bits of the address are zero so we can handle a shift
 547   // when decoding.
 548 
 549   static const struct {
 550     address from;
 551     address to;
 552     size_t increment;
 553   } search_ranges[] = {
 554     {  (address)(4*G),   (address)(32*G),   4*G, },
 555     {  (address)(32*G),  (address)(1024*G), (4 &lt;&lt; LogKlassAlignmentInBytes) * G },
 556     {  NULL, NULL, 0 }
 557   };
 558 
 559   for (int i = 0; search_ranges[i].from != NULL; i ++) {
 560     address a = search_ranges[i].from;
 561     assert(CompressedKlassPointers::is_valid_base(a), "Sanity");
 562     while (a &lt; search_ranges[i].to) {
 563       ReservedSpace rs(size, Metaspace::reserve_alignment(),
 564                        false /*large_pages*/, (char*)a);
 565       if (rs.is_reserved()) {
 566         assert(a == (address)rs.base(), "Sanity");
 567         return rs;
 568       }
 569       a +=  search_ranges[i].increment;
 570     }
 571   }
 572 
 573   // Note: on AARCH64, if the code above does not find any good placement, we
 574   // have no recourse. We return an empty space and the VM will exit.
 575   return ReservedSpace();
 576 #else
 577   // Default implementation: Just reserve anywhere.
 578   return ReservedSpace(size, Metaspace::reserve_alignment(), false, (char*)NULL);
 579 #endif // AARCH64
 580 }
 581 
 582 #endif // _LP64
 583 
 584 
 585 size_t Metaspace::reserve_alignment_words() {
 586   return metaspace::Settings::virtual_space_node_reserve_alignment_words();
 587 }
 588 
 589 size_t Metaspace::commit_alignment_words() {
 590   return metaspace::Settings::commit_granule_words();
 591 }
 592 
 593 void Metaspace::ergo_initialize() {
 594 
 595   // Must happen before using any setting from Settings::---
 596   metaspace::Settings::ergo_initialize();
 597 
 598   // MaxMetaspaceSize and CompressedClassSpaceSize:
 599   //
 600   // MaxMetaspaceSize is the maximum size, in bytes, of memory we are allowed
 601   //  to commit for the Metaspace.
 602   //  It is just a number; a limit we compare against before committing. It
 603   //  does not have to be aligned to anything.
 604   //  It gets used as compare value in class CommitLimiter.
 605   //  It is set to max_uintx in globals.hpp by default, so by default it does
 606   //  not limit anything.
 607   //
 608   // CompressedClassSpaceSize is the size, in bytes, of the address range we
 609   //  pre-reserve for the compressed class space (if we use class space).
 610   //  This size has to be aligned to the metaspace reserve alignment (to the
 611   //  size of a root chunk). It gets aligned up from whatever value the caller
 612   //  gave us to the next multiple of root chunk size.
 613   //
 614   // Note: Strictly speaking MaxMetaspaceSize and CompressedClassSpaceSize have
 615   //  very little to do with each other. The notion often encountered:
 616   //  MaxMetaspaceSize = CompressedClassSpaceSize + &lt;non-class metadata size&gt;
 617   //  is subtly wrong: MaxMetaspaceSize can besmaller than CompressedClassSpaceSize,
 618   //  in which case we just would not be able to fully commit the class space range.
 619   //
 620   // We still adjust CompressedClassSpaceSize to reasonable limits, mainly to
 621   //  save on reserved space, and to make ergnonomics less confusing.
 622 
 623   // (aligned just for cleanliness:)
 624   MaxMetaspaceSize = MAX2(align_down(MaxMetaspaceSize, commit_alignment()), commit_alignment());
 625 
 626   if (UseCompressedClassPointers) {
 627     // Let CCS size not be larger than 80% of MaxMetaspaceSize. Note that is
 628     // grossly over-dimensioned for most usage scenarios; typical ratio of
 629     // class space : non class space usage is about 1:6. With many small classes,
 630     // it can get as low as 1:2. It is not a big deal though since ccs is only
 631     // reserved and will be committed on demand only.
 632     size_t max_ccs_size = MaxMetaspaceSize * 0.8;
 633     size_t adjusted_ccs_size = MIN2(CompressedClassSpaceSize, max_ccs_size);
 634 
 635     // CCS must be aligned to root chunk size, and be at least the size of one
 636     //  root chunk.
 637     adjusted_ccs_size = align_up(adjusted_ccs_size, reserve_alignment());
 638     adjusted_ccs_size = MAX2(adjusted_ccs_size, reserve_alignment());
 639 
 640     // Note: re-adjusting may have us left with a CompressedClassSpaceSize
 641     //  larger than MaxMetaspaceSize for very small values of MaxMetaspaceSize.
 642     //  Lets just live with that, its not a big deal.
 643 
 644     if (adjusted_ccs_size != CompressedClassSpaceSize) {
 645       FLAG_SET_ERGO(CompressedClassSpaceSize, adjusted_ccs_size);
 646       log_info(metaspace)("Setting CompressedClassSpaceSize to " SIZE_FORMAT ".",
 647                           CompressedClassSpaceSize);
 648     }
 649   }
 650 
 651   // Set MetaspaceSize, MinMetaspaceExpansion and MaxMetaspaceExpansion
 652   if (MetaspaceSize &gt; MaxMetaspaceSize) {
 653     MetaspaceSize = MaxMetaspaceSize;
 654   }
 655 
 656   MetaspaceSize = align_down_bounded(MetaspaceSize, commit_alignment());
 657 
 658   assert(MetaspaceSize &lt;= MaxMetaspaceSize, "MetaspaceSize should be limited by MaxMetaspaceSize");
 659 
 660   MinMetaspaceExpansion = align_down_bounded(MinMetaspaceExpansion, commit_alignment());
 661   MaxMetaspaceExpansion = align_down_bounded(MaxMetaspaceExpansion, commit_alignment());
 662 
 663 }
 664 
 665 void Metaspace::global_initialize() {
 666   MetaspaceGC::initialize(); // &lt;- since we do not prealloc init chunks anymore is this still needed?
 667 
 668   metaspace::ChunkHeaderPool::initialize();
 669 
 670   // If UseCompressedClassPointers=1, we have two cases:
 671   // a) if CDS is active (either dump time or runtime), it will create the ccs
 672   //    for us, initialize it and set up CompressedKlassPointers encoding.
 673   //    Class space will be reserved above the mapped archives.
 674   // b) if CDS is not active, we will create the ccs on our own. It will be
 675   //    placed above the java heap, since we assume it has been placed in low
 676   //    address regions. We may rethink this (see JDK-8244943). Failing that,
 677   //    it will be placed anywhere.
 678 
 679 #if INCLUDE_CDS
 680   // case (a)
 681   if (DumpSharedSpaces) {
 682     MetaspaceShared::initialize_dumptime_shared_and_meta_spaces();
 683   } else if (UseSharedSpaces) {
 684     // If any of the archived space fails to map, UseSharedSpaces
 685     // is reset to false.
 686     MetaspaceShared::initialize_runtime_shared_and_meta_spaces();
 687   }
 688 
 689   if (DynamicDumpSharedSpaces &amp;&amp; !UseSharedSpaces) {
 690     vm_exit_during_initialization("DynamicDumpSharedSpaces is unsupported when base CDS archive is not loaded", NULL);
 691   }
 692 #endif // INCLUDE_CDS
 693 
 694 #ifdef _LP64
 695 
 696   if (using_class_space() &amp;&amp; !class_space_is_initialized()) {
 697     assert(!UseSharedSpaces &amp;&amp; !DumpSharedSpaces, "CDS should be off at this point");
 698 
 699     // case (b)
 700     ReservedSpace rs;
 701 
 702     // If UseCompressedOops=1, java heap may have been placed in coops-friendly
 703     //  territory already (lower address regions), so we attempt to place ccs
 704     //  right above the java heap.
 705     // If UseCompressedOops=0, the heap has been placed anywhere - probably in
 706     //  high memory regions. In that case, try to place ccs at the lowest allowed
 707     //  mapping address.
 708     address base = UseCompressedOops ? CompressedOops::end() : (address)HeapBaseMinAddress;
 709     base = align_up(base, Metaspace::reserve_alignment());
 710 
 711     const size_t size = align_up(CompressedClassSpaceSize, Metaspace::reserve_alignment());
 712     if (base != NULL) {
 713       if (CompressedKlassPointers::is_valid_base(base)) {
 714         rs = ReservedSpace(size, Metaspace::reserve_alignment(),
 715                            false /* large */, (char*)base);
 716       }
 717     }
 718 
 719     // ...failing that, reserve anywhere, but let platform do optimized placement:
 720     if (!rs.is_reserved()) {
 721       rs = Metaspace::reserve_address_space_for_compressed_classes(size);
 722     }
 723 
 724     // ...failing that, give up.
 725     if (!rs.is_reserved()) {
 726       vm_exit_during_initialization(
 727           err_msg("Could not allocate compressed class space: " SIZE_FORMAT " bytes",
 728                    CompressedClassSpaceSize));
 729     }
 730 
 731     // Initialize space
 732     Metaspace::initialize_class_space(rs);
 733 
 734     // Set up compressed class pointer encoding.
 735     CompressedKlassPointers::initialize((address)rs.base(), rs.size());
 736   }
 737 
 738 #endif
 739 
 740   // Initialize non-class virtual space list, and its chunk manager:
 741   MetaspaceContext::initialize_nonclass_space_context();
 742 
 743   _tracer = new MetaspaceTracer();
 744 
 745   // We must prevent the very first address of the ccs from being used to store
 746   // metadata, since that address would translate to a narrow pointer of 0, and the
 747   // VM does not distinguish between "narrow 0 as in NULL" and "narrow 0 as in start
 748   //  of ccs".
 749   // Before Elastic Metaspace that did not happen due to the fact that every Metachunk
 750   // had a header and therefore could not allocate anything at offset 0.
 751 #ifdef _LP64
 752   if (using_class_space()) {
 753     // The simplest way to fix this is to allocate a tiny dummy chunk right at the
 754     // start of ccs and do not use it for anything.
 755     MetaspaceContext::context_class()-&gt;cm()-&gt;get_chunk(metaspace::chunklevel::HIGHEST_CHUNK_LEVEL);
 756   }
 757 #endif
 758 
 759 #ifdef _LP64
 760   if (UseCompressedClassPointers) {
 761     // Note: "cds" would be a better fit but keep this for backward compatibility.
 762     LogTarget(Info, gc, metaspace) lt;
 763     if (lt.is_enabled()) {
 764       ResourceMark rm;
 765       LogStream ls(lt);
 766       CDS_ONLY(MetaspaceShared::print_on(&amp;ls);)
 767       Metaspace::print_compressed_class_space(&amp;ls);
 768       CompressedKlassPointers::print_mode(&amp;ls);
 769     }
 770   }
 771 #endif
 772 
 773 }
 774 
 775 void Metaspace::post_initialize() {
 776   MetaspaceGC::post_initialize();
 777 }
 778 
 779 size_t Metaspace::max_allocation_word_size() {
 780   const size_t max_overhead_words = metaspace::get_raw_word_size_for_requested_word_size(1);
 781   return metaspace::chunklevel::MAX_CHUNK_WORD_SIZE - max_overhead_words;
 782 }
 783 
 784 MetaWord* Metaspace::allocate(ClassLoaderData* loader_data, size_t word_size,
 785                               MetaspaceObj::Type type, TRAPS) {
 786   assert(word_size &lt;= Metaspace::max_allocation_word_size(),
 787          "allocation size too large (" SIZE_FORMAT ")", word_size);
 788   assert(!_frozen, "sanity");
 789   assert(!(DumpSharedSpaces &amp;&amp; THREAD-&gt;is_VM_thread()), "sanity");
 790 
 791   if (HAS_PENDING_EXCEPTION) {
 792     assert(false, "Should not allocate with exception pending");
 793     return NULL;  // caller does a CHECK_NULL too
 794   }
 795 
 796   assert(loader_data != NULL, "Should never pass around a NULL loader_data. "
 797         "ClassLoaderData::the_null_class_loader_data() should have been used.");
 798 
 799   MetadataType mdtype = (type == MetaspaceObj::ClassType) ? ClassType : NonClassType;
 800 
 801   // Try to allocate metadata.
 802   MetaWord* result = loader_data-&gt;metaspace_non_null()-&gt;allocate(word_size, mdtype);
 803 
 804   if (result == NULL) {
 805     tracer()-&gt;report_metaspace_allocation_failure(loader_data, word_size, type, mdtype);
 806 
 807     // Allocation failed.
 808     if (is_init_completed()) {
 809       // Only start a GC if the bootstrapping has completed.
 810       // Try to clean out some heap memory and retry. This can prevent premature
 811       // expansion of the metaspace.
 812       result = Universe::heap()-&gt;satisfy_failed_metadata_allocation(loader_data, word_size, mdtype);
 813     }
 814   }
 815 
 816   if (result == NULL) {
 817     if (DumpSharedSpaces) {
 818       // CDS dumping keeps loading classes, so if we hit an OOM we probably will keep hitting OOM.
 819       // We should abort to avoid generating a potentially bad archive.
 820       vm_exit_during_cds_dumping(err_msg("Failed allocating metaspace object type %s of size " SIZE_FORMAT ". CDS dump aborted.",
 821           MetaspaceObj::type_name(type), word_size * BytesPerWord),
 822         err_msg("Please increase MaxMetaspaceSize (currently " SIZE_FORMAT " bytes).", MaxMetaspaceSize));
 823     }
 824     report_metadata_oome(loader_data, word_size, type, mdtype, THREAD);
 825     assert(HAS_PENDING_EXCEPTION, "sanity");
 826     return NULL;
 827   }
 828 
 829   // Zero initialize.
 830   Copy::fill_to_words((HeapWord*)result, word_size, 0);
 831 
 832   log_trace(metaspace)("Metaspace::allocate: type %d return " PTR_FORMAT ".", (int)type, p2i(result));
 833 
 834   return result;
 835 }
 836 
 837 void Metaspace::report_metadata_oome(ClassLoaderData* loader_data, size_t word_size, MetaspaceObj::Type type, MetadataType mdtype, TRAPS) {
 838   tracer()-&gt;report_metadata_oom(loader_data, word_size, type, mdtype);
 839 
 840   // If result is still null, we are out of memory.
 841   Log(gc, metaspace, freelist, oom) log;
 842   if (log.is_info()) {
 843     log.info("Metaspace (%s) allocation failed for size " SIZE_FORMAT,
 844              is_class_space_allocation(mdtype) ? "class" : "data", word_size);
 845     ResourceMark rm;
 846     if (log.is_debug()) {
 847       if (loader_data-&gt;metaspace_or_null() != NULL) {
 848         LogStream ls(log.debug());
 849         loader_data-&gt;print_value_on(&amp;ls);
 850       }
 851     }
 852     LogStream ls(log.info());
 853     // In case of an OOM, log out a short but still useful report.
 854     MetaspaceUtils::print_basic_report(&amp;ls, 0);
 855   }
 856 
 857   // TODO: this exception text may be wrong and misleading. This needs more thinking. See JDK-8252189.
 858   bool out_of_compressed_class_space = false;
 859   if (is_class_space_allocation(mdtype)) {
 860     ClassLoaderMetaspace* metaspace = loader_data-&gt;metaspace_non_null();
 861     out_of_compressed_class_space =
 862       MetaspaceUtils::committed_bytes(Metaspace::ClassType) +
 863       align_up(word_size * BytesPerWord, 4 * M) &gt;
 864       CompressedClassSpaceSize;
 865   }
 866 
 867   // -XX:+HeapDumpOnOutOfMemoryError and -XX:OnOutOfMemoryError support
 868   const char* space_string = out_of_compressed_class_space ?
 869     "Compressed class space" : "Metaspace";
 870 
 871   report_java_out_of_memory(space_string);
 872 
 873   if (JvmtiExport::should_post_resource_exhausted()) {
 874     JvmtiExport::post_resource_exhausted(
 875         JVMTI_RESOURCE_EXHAUSTED_OOM_ERROR,
 876         space_string);
 877   }
 878 
 879   if (!is_init_completed()) {
 880     vm_exit_during_initialization("OutOfMemoryError", space_string);
 881   }
 882 
 883   if (out_of_compressed_class_space) {
 884     THROW_OOP(Universe::out_of_memory_error_class_metaspace());
 885   } else {
 886     THROW_OOP(Universe::out_of_memory_error_metaspace());
 887   }
 888 }
 889 
 890 const char* Metaspace::metadata_type_name(Metaspace::MetadataType mdtype) {
 891   switch (mdtype) {
 892     case Metaspace::ClassType: return "Class";
 893     case Metaspace::NonClassType: return "Metadata";
 894     default:
 895       assert(false, "Got bad mdtype: %d", (int) mdtype);
 896       return NULL;
 897   }
 898 }
 899 
 900 void Metaspace::purge() {
 901   ChunkManager* cm = ChunkManager::chunkmanager_nonclass();
 902   if (cm != NULL) {
 903     cm-&gt;purge();
 904   }
 905   if (using_class_space()) {
 906     cm = ChunkManager::chunkmanager_class();
 907     if (cm != NULL) {
 908       cm-&gt;purge();
 909     }
 910   }
 911 }
 912 
 913 bool Metaspace::contains(const void* ptr) {
 914   if (MetaspaceShared::is_in_shared_metaspace(ptr)) {
 915     return true;
 916   }
 917   return contains_non_shared(ptr);
 918 }
 919 
 920 bool Metaspace::contains_non_shared(const void* ptr) {
 921   if (using_class_space() &amp;&amp; VirtualSpaceList::vslist_class()-&gt;contains((MetaWord*)ptr)) {
 922      return true;
 923   }
 924 
 925   return VirtualSpaceList::vslist_nonclass()-&gt;contains((MetaWord*)ptr);
 926 }
</pre></body></html>
