<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

    <script type="text/javascript" src="../../../../../ancnav.js"></script>
    </head>
    <body id="SUNWwebrev" onkeypress="keypress(event);">
    <a name="0"></a>
    <pre>rev <a href="https://bugs.openjdk.java.net/browse/JDK-60529">60529</a> : imported patch jep387-misc.patch</pre><hr></hr>
<pre>
   1 /*
   2  * Copyright (c) 2001, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "classfile/classLoaderDataGraph.hpp"
  27 #include "classfile/metadataOnStackMark.hpp"
  28 #include "classfile/stringTable.hpp"
  29 #include "code/codeCache.hpp"
  30 #include "code/icBuffer.hpp"
  31 #include "gc/g1/g1Allocator.inline.hpp"
  32 #include "gc/g1/g1Arguments.hpp"
  33 #include "gc/g1/g1BarrierSet.hpp"
  34 #include "gc/g1/g1CardTableEntryClosure.hpp"
  35 #include "gc/g1/g1CollectedHeap.inline.hpp"
  36 #include "gc/g1/g1CollectionSet.hpp"
  37 #include "gc/g1/g1CollectorState.hpp"
  38 #include "gc/g1/g1ConcurrentRefine.hpp"
  39 #include "gc/g1/g1ConcurrentRefineThread.hpp"
  40 #include "gc/g1/g1ConcurrentMarkThread.inline.hpp"
  41 #include "gc/g1/g1DirtyCardQueue.hpp"
  42 #include "gc/g1/g1EvacStats.inline.hpp"
  43 #include "gc/g1/g1FullCollector.hpp"
  44 #include "gc/g1/g1GCParPhaseTimesTracker.hpp"
  45 #include "gc/g1/g1GCPhaseTimes.hpp"
  46 #include "gc/g1/g1HeapSizingPolicy.hpp"
  47 #include "gc/g1/g1HeapTransition.hpp"
  48 #include "gc/g1/g1HeapVerifier.hpp"
  49 #include "gc/g1/g1HotCardCache.hpp"
  50 #include "gc/g1/g1InitLogger.hpp"
  51 #include "gc/g1/g1MemoryPool.hpp"
  52 #include "gc/g1/g1OopClosures.inline.hpp"
  53 #include "gc/g1/g1ParallelCleaning.hpp"
  54 #include "gc/g1/g1ParScanThreadState.inline.hpp"
  55 #include "gc/g1/g1Policy.hpp"
  56 #include "gc/g1/g1RedirtyCardsQueue.hpp"
  57 #include "gc/g1/g1RegionToSpaceMapper.hpp"
  58 #include "gc/g1/g1RemSet.hpp"
  59 #include "gc/g1/g1RootClosures.hpp"
  60 #include "gc/g1/g1RootProcessor.hpp"
  61 #include "gc/g1/g1SATBMarkQueueSet.hpp"
  62 #include "gc/g1/g1StringDedup.hpp"
  63 #include "gc/g1/g1ThreadLocalData.hpp"
  64 #include "gc/g1/g1Trace.hpp"
  65 #include "gc/g1/g1YCTypes.hpp"
  66 #include "gc/g1/g1YoungRemSetSamplingThread.hpp"
  67 #include "gc/g1/g1VMOperations.hpp"
  68 #include "gc/g1/heapRegion.inline.hpp"
  69 #include "gc/g1/heapRegionRemSet.hpp"
  70 #include "gc/g1/heapRegionSet.inline.hpp"
  71 #include "gc/shared/concurrentGCBreakpoints.hpp"
  72 #include "gc/shared/gcBehaviours.hpp"
  73 #include "gc/shared/gcHeapSummary.hpp"
  74 #include "gc/shared/gcId.hpp"
  75 #include "gc/shared/gcLocker.hpp"
  76 #include "gc/shared/gcTimer.hpp"
  77 #include "gc/shared/gcTraceTime.inline.hpp"
  78 #include "gc/shared/generationSpec.hpp"
  79 #include "gc/shared/isGCActiveMark.hpp"
  80 #include "gc/shared/locationPrinter.inline.hpp"
  81 #include "gc/shared/oopStorageParState.hpp"
  82 #include "gc/shared/preservedMarks.inline.hpp"
  83 #include "gc/shared/suspendibleThreadSet.hpp"
  84 #include "gc/shared/referenceProcessor.inline.hpp"
  85 #include "gc/shared/taskTerminator.hpp"
  86 #include "gc/shared/taskqueue.inline.hpp"
  87 #include "gc/shared/weakProcessor.inline.hpp"
  88 #include "gc/shared/workerPolicy.hpp"
  89 #include "logging/log.hpp"
  90 #include "memory/allocation.hpp"
  91 #include "memory/iterator.hpp"
  92 #include "memory/resourceArea.hpp"
  93 #include "memory/universe.hpp"
  94 #include "oops/access.inline.hpp"
  95 #include "oops/compressedOops.inline.hpp"
  96 #include "oops/oop.inline.hpp"
  97 #include "runtime/atomic.hpp"
  98 #include "runtime/handles.inline.hpp"
  99 #include "runtime/init.hpp"
 100 #include "runtime/orderAccess.hpp"
 101 #include "runtime/threadSMR.hpp"
 102 #include "runtime/vmThread.hpp"
 103 #include "utilities/align.hpp"
 104 #include "utilities/autoRestore.hpp"
 105 #include "utilities/bitMap.inline.hpp"
 106 #include "utilities/globalDefinitions.hpp"
 107 #include "utilities/stack.inline.hpp"
 108 
 109 size_t G1CollectedHeap::_humongous_object_threshold_in_words = 0;
 110 
 111 // INVARIANTS/NOTES
 112 //
 113 // All allocation activity covered by the G1CollectedHeap interface is
 114 // serialized by acquiring the HeapLock.  This happens in mem_allocate
 115 // and allocate_new_tlab, which are the "entry" points to the
 116 // allocation code from the rest of the JVM.  (Note that this does not
 117 // apply to TLAB allocation, which is not part of this interface: it
 118 // is done by clients of this interface.)
 119 
 120 class RedirtyLoggedCardTableEntryClosure : public G1CardTableEntryClosure {
 121  private:
 122   size_t _num_dirtied;
 123   G1CollectedHeap* _g1h;
 124   G1CardTable* _g1_ct;
 125 
 126   HeapRegion* region_for_card(CardValue* card_ptr) const {
 127     return _g1h-&gt;heap_region_containing(_g1_ct-&gt;addr_for(card_ptr));
 128   }
 129 
 130   bool will_become_free(HeapRegion* hr) const {
 131     // A region will be freed by free_collection_set if the region is in the
 132     // collection set and has not had an evacuation failure.
 133     return _g1h-&gt;is_in_cset(hr) &amp;&amp; !hr-&gt;evacuation_failed();
 134   }
 135 
 136  public:
 137   RedirtyLoggedCardTableEntryClosure(G1CollectedHeap* g1h) : G1CardTableEntryClosure(),
 138     _num_dirtied(0), _g1h(g1h), _g1_ct(g1h-&gt;card_table()) { }
 139 
 140   void do_card_ptr(CardValue* card_ptr, uint worker_id) {
 141     HeapRegion* hr = region_for_card(card_ptr);
 142 
 143     // Should only dirty cards in regions that won't be freed.
 144     if (!will_become_free(hr)) {
 145       *card_ptr = G1CardTable::dirty_card_val();
 146       _num_dirtied++;
 147     }
 148   }
 149 
 150   size_t num_dirtied()   const { return _num_dirtied; }
 151 };
 152 
 153 
 154 void G1RegionMappingChangedListener::reset_from_card_cache(uint start_idx, size_t num_regions) {
 155   HeapRegionRemSet::invalidate_from_card_cache(start_idx, num_regions);
 156 }
 157 
 158 void G1RegionMappingChangedListener::on_commit(uint start_idx, size_t num_regions, bool zero_filled) {
 159   // The from card cache is not the memory that is actually committed. So we cannot
 160   // take advantage of the zero_filled parameter.
 161   reset_from_card_cache(start_idx, num_regions);
 162 }
 163 
 164 Tickspan G1CollectedHeap::run_task(AbstractGangTask* task) {
 165   Ticks start = Ticks::now();
 166   workers()-&gt;run_task(task, workers()-&gt;active_workers());
 167   return Ticks::now() - start;
 168 }
 169 
 170 HeapRegion* G1CollectedHeap::new_heap_region(uint hrs_index,
 171                                              MemRegion mr) {
 172   return new HeapRegion(hrs_index, bot(), mr);
 173 }
 174 
 175 // Private methods.
 176 
 177 HeapRegion* G1CollectedHeap::new_region(size_t word_size,
 178                                         HeapRegionType type,
 179                                         bool do_expand,
 180                                         uint node_index) {
 181   assert(!is_humongous(word_size) || word_size &lt;= HeapRegion::GrainWords,
 182          "the only time we use this to allocate a humongous region is "
 183          "when we are allocating a single humongous region");
 184 
 185   HeapRegion* res = _hrm-&gt;allocate_free_region(type, node_index);
 186 
 187   if (res == NULL &amp;&amp; do_expand &amp;&amp; _expand_heap_after_alloc_failure) {
 188     // Currently, only attempts to allocate GC alloc regions set
 189     // do_expand to true. So, we should only reach here during a
 190     // safepoint. If this assumption changes we might have to
 191     // reconsider the use of _expand_heap_after_alloc_failure.
 192     assert(SafepointSynchronize::is_at_safepoint(), "invariant");
 193 
 194     log_debug(gc, ergo, heap)("Attempt heap expansion (region allocation request failed). Allocation request: " SIZE_FORMAT "B",
 195                               word_size * HeapWordSize);
 196 
 197     assert(word_size * HeapWordSize &lt; HeapRegion::GrainBytes,
 198            "This kind of expansion should never be more than one region. Size: " SIZE_FORMAT,
 199            word_size * HeapWordSize);
 200     if (expand_single_region(node_index)) {
 201       // Given that expand_single_region() succeeded in expanding the heap, and we
 202       // always expand the heap by an amount aligned to the heap
 203       // region size, the free list should in theory not be empty.
 204       // In either case allocate_free_region() will check for NULL.
 205       res = _hrm-&gt;allocate_free_region(type, node_index);
 206     } else {
 207       _expand_heap_after_alloc_failure = false;
 208     }
 209   }
 210   return res;
 211 }
 212 
 213 HeapWord*
 214 G1CollectedHeap::humongous_obj_allocate_initialize_regions(HeapRegion* first_hr,
 215                                                            uint num_regions,
 216                                                            size_t word_size) {
 217   assert(first_hr != NULL, "pre-condition");
 218   assert(is_humongous(word_size), "word_size should be humongous");
 219   assert(num_regions * HeapRegion::GrainWords &gt;= word_size, "pre-condition");
 220 
 221   // Index of last region in the series.
 222   uint first = first_hr-&gt;hrm_index();
 223   uint last = first + num_regions - 1;
 224 
 225   // We need to initialize the region(s) we just discovered. This is
 226   // a bit tricky given that it can happen concurrently with
 227   // refinement threads refining cards on these regions and
 228   // potentially wanting to refine the BOT as they are scanning
 229   // those cards (this can happen shortly after a cleanup; see CR
 230   // 6991377). So we have to set up the region(s) carefully and in
 231   // a specific order.
 232 
 233   // The word size sum of all the regions we will allocate.
 234   size_t word_size_sum = (size_t) num_regions * HeapRegion::GrainWords;
 235   assert(word_size &lt;= word_size_sum, "sanity");
 236 
 237   // The passed in hr will be the "starts humongous" region. The header
 238   // of the new object will be placed at the bottom of this region.
 239   HeapWord* new_obj = first_hr-&gt;bottom();
 240   // This will be the new top of the new object.
 241   HeapWord* obj_top = new_obj + word_size;
 242 
 243   // First, we need to zero the header of the space that we will be
 244   // allocating. When we update top further down, some refinement
 245   // threads might try to scan the region. By zeroing the header we
 246   // ensure that any thread that will try to scan the region will
 247   // come across the zero klass word and bail out.
 248   //
 249   // NOTE: It would not have been correct to have used
 250   // CollectedHeap::fill_with_object() and make the space look like
 251   // an int array. The thread that is doing the allocation will
 252   // later update the object header to a potentially different array
 253   // type and, for a very short period of time, the klass and length
 254   // fields will be inconsistent. This could cause a refinement
 255   // thread to calculate the object size incorrectly.
 256   Copy::fill_to_words(new_obj, oopDesc::header_size(), 0);
 257 
 258   // Next, pad out the unused tail of the last region with filler
 259   // objects, for improved usage accounting.
 260   // How many words we use for filler objects.
 261   size_t word_fill_size = word_size_sum - word_size;
 262 
 263   // How many words memory we "waste" which cannot hold a filler object.
 264   size_t words_not_fillable = 0;
 265 
 266   if (word_fill_size &gt;= min_fill_size()) {
 267     fill_with_objects(obj_top, word_fill_size);
 268   } else if (word_fill_size &gt; 0) {
 269     // We have space to fill, but we cannot fit an object there.
 270     words_not_fillable = word_fill_size;
 271     word_fill_size = 0;
 272   }
 273 
 274   // We will set up the first region as "starts humongous". This
 275   // will also update the BOT covering all the regions to reflect
 276   // that there is a single object that starts at the bottom of the
 277   // first region.
 278   first_hr-&gt;set_starts_humongous(obj_top, word_fill_size);
 279   _policy-&gt;remset_tracker()-&gt;update_at_allocate(first_hr);
 280   // Then, if there are any, we will set up the "continues
 281   // humongous" regions.
 282   HeapRegion* hr = NULL;
 283   for (uint i = first + 1; i &lt;= last; ++i) {
 284     hr = region_at(i);
 285     hr-&gt;set_continues_humongous(first_hr);
 286     _policy-&gt;remset_tracker()-&gt;update_at_allocate(hr);
 287   }
 288 
 289   // Up to this point no concurrent thread would have been able to
 290   // do any scanning on any region in this series. All the top
 291   // fields still point to bottom, so the intersection between
 292   // [bottom,top] and [card_start,card_end] will be empty. Before we
 293   // update the top fields, we'll do a storestore to make sure that
 294   // no thread sees the update to top before the zeroing of the
 295   // object header and the BOT initialization.
 296   OrderAccess::storestore();
 297 
 298   // Now, we will update the top fields of the "continues humongous"
 299   // regions except the last one.
 300   for (uint i = first; i &lt; last; ++i) {
 301     hr = region_at(i);
 302     hr-&gt;set_top(hr-&gt;end());
 303   }
 304 
 305   hr = region_at(last);
 306   // If we cannot fit a filler object, we must set top to the end
 307   // of the humongous object, otherwise we cannot iterate the heap
 308   // and the BOT will not be complete.
 309   hr-&gt;set_top(hr-&gt;end() - words_not_fillable);
 310 
 311   assert(hr-&gt;bottom() &lt; obj_top &amp;&amp; obj_top &lt;= hr-&gt;end(),
 312          "obj_top should be in last region");
 313 
 314   _verifier-&gt;check_bitmaps("Humongous Region Allocation", first_hr);
 315 
 316   assert(words_not_fillable == 0 ||
 317          first_hr-&gt;bottom() + word_size_sum - words_not_fillable == hr-&gt;top(),
 318          "Miscalculation in humongous allocation");
 319 
 320   increase_used((word_size_sum - words_not_fillable) * HeapWordSize);
 321 
 322   for (uint i = first; i &lt;= last; ++i) {
 323     hr = region_at(i);
 324     _humongous_set.add(hr);
 325     _hr_printer.alloc(hr);
 326   }
 327 
 328   return new_obj;
 329 }
 330 
 331 size_t G1CollectedHeap::humongous_obj_size_in_regions(size_t word_size) {
 332   assert(is_humongous(word_size), "Object of size " SIZE_FORMAT " must be humongous here", word_size);
 333   return align_up(word_size, HeapRegion::GrainWords) / HeapRegion::GrainWords;
 334 }
 335 
 336 // If could fit into free regions w/o expansion, try.
 337 // Otherwise, if can expand, do so.
 338 // Otherwise, if using ex regions might help, try with ex given back.
 339 HeapWord* G1CollectedHeap::humongous_obj_allocate(size_t word_size) {
 340   assert_heap_locked_or_at_safepoint(true /* should_be_vm_thread */);
 341 
 342   _verifier-&gt;verify_region_sets_optional();
 343 
 344   uint obj_regions = (uint) humongous_obj_size_in_regions(word_size);
 345 
 346   // Policy: First try to allocate a humongous object in the free list.
 347   HeapRegion* humongous_start = _hrm-&gt;allocate_humongous(obj_regions);
 348   if (humongous_start == NULL) {
 349     // Policy: We could not find enough regions for the humongous object in the
 350     // free list. Look through the heap to find a mix of free and uncommitted regions.
 351     // If so, expand the heap and allocate the humongous object.
 352     humongous_start = _hrm-&gt;expand_and_allocate_humongous(obj_regions);
 353     if (humongous_start != NULL) {
 354       // We managed to find a region by expanding the heap.
 355       log_debug(gc, ergo, heap)("Heap expansion (humongous allocation request). Allocation request: " SIZE_FORMAT "B",
 356                                 word_size * HeapWordSize);
 357       policy()-&gt;record_new_heap_size(num_regions());
 358     } else {
 359       // Policy: Potentially trigger a defragmentation GC.
 360     }
 361   }
 362 
 363   HeapWord* result = NULL;
 364   if (humongous_start != NULL) {
 365     result = humongous_obj_allocate_initialize_regions(humongous_start, obj_regions, word_size);
 366     assert(result != NULL, "it should always return a valid result");
 367 
 368     // A successful humongous object allocation changes the used space
 369     // information of the old generation so we need to recalculate the
 370     // sizes and update the jstat counters here.
 371     g1mm()-&gt;update_sizes();
 372   }
 373 
 374   _verifier-&gt;verify_region_sets_optional();
 375 
 376   return result;
 377 }
 378 
 379 HeapWord* G1CollectedHeap::allocate_new_tlab(size_t min_size,
 380                                              size_t requested_size,
 381                                              size_t* actual_size) {
 382   assert_heap_not_locked_and_not_at_safepoint();
 383   assert(!is_humongous(requested_size), "we do not allow humongous TLABs");
 384 
 385   return attempt_allocation(min_size, requested_size, actual_size);
 386 }
 387 
 388 HeapWord*
 389 G1CollectedHeap::mem_allocate(size_t word_size,
 390                               bool*  gc_overhead_limit_was_exceeded) {
 391   assert_heap_not_locked_and_not_at_safepoint();
 392 
 393   if (is_humongous(word_size)) {
 394     return attempt_allocation_humongous(word_size);
 395   }
 396   size_t dummy = 0;
 397   return attempt_allocation(word_size, word_size, &amp;dummy);
 398 }
 399 
 400 HeapWord* G1CollectedHeap::attempt_allocation_slow(size_t word_size) {
 401   ResourceMark rm; // For retrieving the thread names in log messages.
 402 
 403   // Make sure you read the note in attempt_allocation_humongous().
 404 
 405   assert_heap_not_locked_and_not_at_safepoint();
 406   assert(!is_humongous(word_size), "attempt_allocation_slow() should not "
 407          "be called for humongous allocation requests");
 408 
 409   // We should only get here after the first-level allocation attempt
 410   // (attempt_allocation()) failed to allocate.
 411 
 412   // We will loop until a) we manage to successfully perform the
 413   // allocation or b) we successfully schedule a collection which
 414   // fails to perform the allocation. b) is the only case when we'll
 415   // return NULL.
 416   HeapWord* result = NULL;
 417   for (uint try_count = 1, gclocker_retry_count = 0; /* we'll return */; try_count += 1) {
 418     bool should_try_gc;
 419     uint gc_count_before;
 420 
 421     {
 422       MutexLocker x(Heap_lock);
 423       result = _allocator-&gt;attempt_allocation_locked(word_size);
 424       if (result != NULL) {
 425         return result;
 426       }
 427 
 428       // If the GCLocker is active and we are bound for a GC, try expanding young gen.
 429       // This is different to when only GCLocker::needs_gc() is set: try to avoid
 430       // waiting because the GCLocker is active to not wait too long.
 431       if (GCLocker::is_active_and_needs_gc() &amp;&amp; policy()-&gt;can_expand_young_list()) {
 432         // No need for an ergo message here, can_expand_young_list() does this when
 433         // it returns true.
 434         result = _allocator-&gt;attempt_allocation_force(word_size);
 435         if (result != NULL) {
 436           return result;
 437         }
 438       }
 439       // Only try a GC if the GCLocker does not signal the need for a GC. Wait until
 440       // the GCLocker initiated GC has been performed and then retry. This includes
 441       // the case when the GC Locker is not active but has not been performed.
 442       should_try_gc = !GCLocker::needs_gc();
 443       // Read the GC count while still holding the Heap_lock.
 444       gc_count_before = total_collections();
 445     }
 446 
 447     if (should_try_gc) {
 448       bool succeeded;
 449       result = do_collection_pause(word_size, gc_count_before, &amp;succeeded,
 450                                    GCCause::_g1_inc_collection_pause);
 451       if (result != NULL) {
 452         assert(succeeded, "only way to get back a non-NULL result");
 453         log_trace(gc, alloc)("%s: Successfully scheduled collection returning " PTR_FORMAT,
 454                              Thread::current()-&gt;name(), p2i(result));
 455         return result;
 456       }
 457 
 458       if (succeeded) {
 459         // We successfully scheduled a collection which failed to allocate. No
 460         // point in trying to allocate further. We'll just return NULL.
 461         log_trace(gc, alloc)("%s: Successfully scheduled collection failing to allocate "
 462                              SIZE_FORMAT " words", Thread::current()-&gt;name(), word_size);
 463         return NULL;
 464       }
 465       log_trace(gc, alloc)("%s: Unsuccessfully scheduled collection allocating " SIZE_FORMAT " words",
 466                            Thread::current()-&gt;name(), word_size);
 467     } else {
 468       // Failed to schedule a collection.
 469       if (gclocker_retry_count &gt; GCLockerRetryAllocationCount) {
 470         log_warning(gc, alloc)("%s: Retried waiting for GCLocker too often allocating "
 471                                SIZE_FORMAT " words", Thread::current()-&gt;name(), word_size);
 472         return NULL;
 473       }
 474       log_trace(gc, alloc)("%s: Stall until clear", Thread::current()-&gt;name());
 475       // The GCLocker is either active or the GCLocker initiated
 476       // GC has not yet been performed. Stall until it is and
 477       // then retry the allocation.
 478       GCLocker::stall_until_clear();
 479       gclocker_retry_count += 1;
 480     }
 481 
 482     // We can reach here if we were unsuccessful in scheduling a
 483     // collection (because another thread beat us to it) or if we were
 484     // stalled due to the GC locker. In either can we should retry the
 485     // allocation attempt in case another thread successfully
 486     // performed a collection and reclaimed enough space. We do the
 487     // first attempt (without holding the Heap_lock) here and the
 488     // follow-on attempt will be at the start of the next loop
 489     // iteration (after taking the Heap_lock).
 490     size_t dummy = 0;
 491     result = _allocator-&gt;attempt_allocation(word_size, word_size, &amp;dummy);
 492     if (result != NULL) {
 493       return result;
 494     }
 495 
 496     // Give a warning if we seem to be looping forever.
 497     if ((QueuedAllocationWarningCount &gt; 0) &amp;&amp;
 498         (try_count % QueuedAllocationWarningCount == 0)) {
 499       log_warning(gc, alloc)("%s:  Retried allocation %u times for " SIZE_FORMAT " words",
 500                              Thread::current()-&gt;name(), try_count, word_size);
 501     }
 502   }
 503 
 504   ShouldNotReachHere();
 505   return NULL;
 506 }
 507 
 508 void G1CollectedHeap::begin_archive_alloc_range(bool open) {
 509   assert_at_safepoint_on_vm_thread();
 510   if (_archive_allocator == NULL) {
 511     _archive_allocator = G1ArchiveAllocator::create_allocator(this, open);
 512   }
 513 }
 514 
 515 bool G1CollectedHeap::is_archive_alloc_too_large(size_t word_size) {
 516   // Allocations in archive regions cannot be of a size that would be considered
 517   // humongous even for a minimum-sized region, because G1 region sizes/boundaries
 518   // may be different at archive-restore time.
 519   return word_size &gt;= humongous_threshold_for(HeapRegion::min_region_size_in_words());
 520 }
 521 
 522 HeapWord* G1CollectedHeap::archive_mem_allocate(size_t word_size) {
 523   assert_at_safepoint_on_vm_thread();
 524   assert(_archive_allocator != NULL, "_archive_allocator not initialized");
 525   if (is_archive_alloc_too_large(word_size)) {
 526     return NULL;
 527   }
 528   return _archive_allocator-&gt;archive_mem_allocate(word_size);
 529 }
 530 
 531 void G1CollectedHeap::end_archive_alloc_range(GrowableArray&lt;MemRegion&gt;* ranges,
 532                                               size_t end_alignment_in_bytes) {
 533   assert_at_safepoint_on_vm_thread();
 534   assert(_archive_allocator != NULL, "_archive_allocator not initialized");
 535 
 536   // Call complete_archive to do the real work, filling in the MemRegion
 537   // array with the archive regions.
 538   _archive_allocator-&gt;complete_archive(ranges, end_alignment_in_bytes);
 539   delete _archive_allocator;
 540   _archive_allocator = NULL;
 541 }
 542 
 543 bool G1CollectedHeap::check_archive_addresses(MemRegion* ranges, size_t count) {
 544   assert(ranges != NULL, "MemRegion array NULL");
 545   assert(count != 0, "No MemRegions provided");
 546   MemRegion reserved = _hrm-&gt;reserved();
 547   for (size_t i = 0; i &lt; count; i++) {
 548     if (!reserved.contains(ranges[i].start()) || !reserved.contains(ranges[i].last())) {
 549       return false;
 550     }
 551   }
 552   return true;
 553 }
 554 
 555 bool G1CollectedHeap::alloc_archive_regions(MemRegion* ranges,
 556                                             size_t count,
 557                                             bool open) {
 558   assert(!is_init_completed(), "Expect to be called at JVM init time");
 559   assert(ranges != NULL, "MemRegion array NULL");
 560   assert(count != 0, "No MemRegions provided");
 561   MutexLocker x(Heap_lock);
 562 
 563   MemRegion reserved = _hrm-&gt;reserved();
 564   HeapWord* prev_last_addr = NULL;
 565   HeapRegion* prev_last_region = NULL;
 566 
 567   // Temporarily disable pretouching of heap pages. This interface is used
 568   // when mmap'ing archived heap data in, so pre-touching is wasted.
 569   FlagSetting fs(AlwaysPreTouch, false);
 570 
 571   // Enable archive object checking used by G1MarkSweep. We have to let it know
 572   // about each archive range, so that objects in those ranges aren't marked.
 573   G1ArchiveAllocator::enable_archive_object_check();
 574 
 575   // For each specified MemRegion range, allocate the corresponding G1
 576   // regions and mark them as archive regions. We expect the ranges
 577   // in ascending starting address order, without overlap.
 578   for (size_t i = 0; i &lt; count; i++) {
 579     MemRegion curr_range = ranges[i];
 580     HeapWord* start_address = curr_range.start();
 581     size_t word_size = curr_range.word_size();
 582     HeapWord* last_address = curr_range.last();
 583     size_t commits = 0;
 584 
 585     guarantee(reserved.contains(start_address) &amp;&amp; reserved.contains(last_address),
 586               "MemRegion outside of heap [" PTR_FORMAT ", " PTR_FORMAT "]",
 587               p2i(start_address), p2i(last_address));
 588     guarantee(start_address &gt; prev_last_addr,
 589               "Ranges not in ascending order: " PTR_FORMAT " &lt;= " PTR_FORMAT ,
 590               p2i(start_address), p2i(prev_last_addr));
 591     prev_last_addr = last_address;
 592 
 593     // Check for ranges that start in the same G1 region in which the previous
 594     // range ended, and adjust the start address so we don't try to allocate
 595     // the same region again. If the current range is entirely within that
 596     // region, skip it, just adjusting the recorded top.
 597     HeapRegion* start_region = _hrm-&gt;addr_to_region(start_address);
 598     if ((prev_last_region != NULL) &amp;&amp; (start_region == prev_last_region)) {
 599       start_address = start_region-&gt;end();
 600       if (start_address &gt; last_address) {
 601         increase_used(word_size * HeapWordSize);
 602         start_region-&gt;set_top(last_address + 1);
 603         continue;
 604       }
 605       start_region-&gt;set_top(start_address);
 606       curr_range = MemRegion(start_address, last_address + 1);
 607       start_region = _hrm-&gt;addr_to_region(start_address);
 608     }
 609 
 610     // Perform the actual region allocation, exiting if it fails.
 611     // Then note how much new space we have allocated.
 612     if (!_hrm-&gt;allocate_containing_regions(curr_range, &amp;commits, workers())) {
 613       return false;
 614     }
 615     increase_used(word_size * HeapWordSize);
 616     if (commits != 0) {
 617       log_debug(gc, ergo, heap)("Attempt heap expansion (allocate archive regions). Total size: " SIZE_FORMAT "B",
 618                                 HeapRegion::GrainWords * HeapWordSize * commits);
 619 
 620     }
 621 
 622     // Mark each G1 region touched by the range as archive, add it to
 623     // the old set, and set top.
 624     HeapRegion* curr_region = _hrm-&gt;addr_to_region(start_address);
 625     HeapRegion* last_region = _hrm-&gt;addr_to_region(last_address);
 626     prev_last_region = last_region;
 627 
 628     while (curr_region != NULL) {
 629       assert(curr_region-&gt;is_empty() &amp;&amp; !curr_region-&gt;is_pinned(),
 630              "Region already in use (index %u)", curr_region-&gt;hrm_index());
 631       if (open) {
 632         curr_region-&gt;set_open_archive();
 633       } else {
 634         curr_region-&gt;set_closed_archive();
 635       }
 636       _hr_printer.alloc(curr_region);
 637       _archive_set.add(curr_region);
 638       HeapWord* top;
 639       HeapRegion* next_region;
 640       if (curr_region != last_region) {
 641         top = curr_region-&gt;end();
 642         next_region = _hrm-&gt;next_region_in_heap(curr_region);
 643       } else {
 644         top = last_address + 1;
 645         next_region = NULL;
 646       }
 647       curr_region-&gt;set_top(top);
 648       curr_region = next_region;
 649     }
 650 
 651     // Notify mark-sweep of the archive
 652     G1ArchiveAllocator::set_range_archive(curr_range, open);
 653   }
 654   return true;
 655 }
 656 
 657 void G1CollectedHeap::fill_archive_regions(MemRegion* ranges, size_t count) {
 658   assert(!is_init_completed(), "Expect to be called at JVM init time");
 659   assert(ranges != NULL, "MemRegion array NULL");
 660   assert(count != 0, "No MemRegions provided");
 661   MemRegion reserved = _hrm-&gt;reserved();
 662   HeapWord *prev_last_addr = NULL;
 663   HeapRegion* prev_last_region = NULL;
 664 
 665   // For each MemRegion, create filler objects, if needed, in the G1 regions
 666   // that contain the address range. The address range actually within the
 667   // MemRegion will not be modified. That is assumed to have been initialized
 668   // elsewhere, probably via an mmap of archived heap data.
 669   MutexLocker x(Heap_lock);
 670   for (size_t i = 0; i &lt; count; i++) {
 671     HeapWord* start_address = ranges[i].start();
 672     HeapWord* last_address = ranges[i].last();
 673 
 674     assert(reserved.contains(start_address) &amp;&amp; reserved.contains(last_address),
 675            "MemRegion outside of heap [" PTR_FORMAT ", " PTR_FORMAT "]",
 676            p2i(start_address), p2i(last_address));
 677     assert(start_address &gt; prev_last_addr,
 678            "Ranges not in ascending order: " PTR_FORMAT " &lt;= " PTR_FORMAT ,
 679            p2i(start_address), p2i(prev_last_addr));
 680 
 681     HeapRegion* start_region = _hrm-&gt;addr_to_region(start_address);
 682     HeapRegion* last_region = _hrm-&gt;addr_to_region(last_address);
 683     HeapWord* bottom_address = start_region-&gt;bottom();
 684 
 685     // Check for a range beginning in the same region in which the
 686     // previous one ended.
 687     if (start_region == prev_last_region) {
 688       bottom_address = prev_last_addr + 1;
 689     }
 690 
 691     // Verify that the regions were all marked as archive regions by
 692     // alloc_archive_regions.
 693     HeapRegion* curr_region = start_region;
 694     while (curr_region != NULL) {
 695       guarantee(curr_region-&gt;is_archive(),
 696                 "Expected archive region at index %u", curr_region-&gt;hrm_index());
 697       if (curr_region != last_region) {
 698         curr_region = _hrm-&gt;next_region_in_heap(curr_region);
 699       } else {
 700         curr_region = NULL;
 701       }
 702     }
 703 
 704     prev_last_addr = last_address;
 705     prev_last_region = last_region;
 706 
 707     // Fill the memory below the allocated range with dummy object(s),
 708     // if the region bottom does not match the range start, or if the previous
 709     // range ended within the same G1 region, and there is a gap.
 710     if (start_address != bottom_address) {
 711       size_t fill_size = pointer_delta(start_address, bottom_address);
 712       G1CollectedHeap::fill_with_objects(bottom_address, fill_size);
 713       increase_used(fill_size * HeapWordSize);
 714     }
 715   }
 716 }
 717 
 718 inline HeapWord* G1CollectedHeap::attempt_allocation(size_t min_word_size,
 719                                                      size_t desired_word_size,
 720                                                      size_t* actual_word_size) {
 721   assert_heap_not_locked_and_not_at_safepoint();
 722   assert(!is_humongous(desired_word_size), "attempt_allocation() should not "
 723          "be called for humongous allocation requests");
 724 
 725   HeapWord* result = _allocator-&gt;attempt_allocation(min_word_size, desired_word_size, actual_word_size);
 726 
 727   if (result == NULL) {
 728     *actual_word_size = desired_word_size;
 729     result = attempt_allocation_slow(desired_word_size);
 730   }
 731 
 732   assert_heap_not_locked();
 733   if (result != NULL) {
 734     assert(*actual_word_size != 0, "Actual size must have been set here");
 735     dirty_young_block(result, *actual_word_size);
 736   } else {
 737     *actual_word_size = 0;
 738   }
 739 
 740   return result;
 741 }
 742 
 743 void G1CollectedHeap::dealloc_archive_regions(MemRegion* ranges, size_t count) {
 744   assert(!is_init_completed(), "Expect to be called at JVM init time");
 745   assert(ranges != NULL, "MemRegion array NULL");
 746   assert(count != 0, "No MemRegions provided");
 747   MemRegion reserved = _hrm-&gt;reserved();
 748   HeapWord* prev_last_addr = NULL;
 749   HeapRegion* prev_last_region = NULL;
 750   size_t size_used = 0;
 751   size_t uncommitted_regions = 0;
 752 
 753   // For each Memregion, free the G1 regions that constitute it, and
 754   // notify mark-sweep that the range is no longer to be considered 'archive.'
 755   MutexLocker x(Heap_lock);
 756   for (size_t i = 0; i &lt; count; i++) {
 757     HeapWord* start_address = ranges[i].start();
 758     HeapWord* last_address = ranges[i].last();
 759 
 760     assert(reserved.contains(start_address) &amp;&amp; reserved.contains(last_address),
 761            "MemRegion outside of heap [" PTR_FORMAT ", " PTR_FORMAT "]",
 762            p2i(start_address), p2i(last_address));
 763     assert(start_address &gt; prev_last_addr,
 764            "Ranges not in ascending order: " PTR_FORMAT " &lt;= " PTR_FORMAT ,
 765            p2i(start_address), p2i(prev_last_addr));
 766     size_used += ranges[i].byte_size();
 767     prev_last_addr = last_address;
 768 
 769     HeapRegion* start_region = _hrm-&gt;addr_to_region(start_address);
 770     HeapRegion* last_region = _hrm-&gt;addr_to_region(last_address);
 771 
 772     // Check for ranges that start in the same G1 region in which the previous
 773     // range ended, and adjust the start address so we don't try to free
 774     // the same region again. If the current range is entirely within that
 775     // region, skip it.
 776     if (start_region == prev_last_region) {
 777       start_address = start_region-&gt;end();
 778       if (start_address &gt; last_address) {
 779         continue;
 780       }
 781       start_region = _hrm-&gt;addr_to_region(start_address);
 782     }
 783     prev_last_region = last_region;
 784 
 785     // After verifying that each region was marked as an archive region by
 786     // alloc_archive_regions, set it free and empty and uncommit it.
 787     HeapRegion* curr_region = start_region;
 788     while (curr_region != NULL) {
 789       guarantee(curr_region-&gt;is_archive(),
 790                 "Expected archive region at index %u", curr_region-&gt;hrm_index());
 791       uint curr_index = curr_region-&gt;hrm_index();
 792       _archive_set.remove(curr_region);
 793       curr_region-&gt;set_free();
 794       curr_region-&gt;set_top(curr_region-&gt;bottom());
 795       if (curr_region != last_region) {
 796         curr_region = _hrm-&gt;next_region_in_heap(curr_region);
 797       } else {
 798         curr_region = NULL;
 799       }
 800       _hrm-&gt;shrink_at(curr_index, 1);
 801       uncommitted_regions++;
 802     }
 803 
 804     // Notify mark-sweep that this is no longer an archive range.
 805     G1ArchiveAllocator::clear_range_archive(ranges[i]);
 806   }
 807 
 808   if (uncommitted_regions != 0) {
 809     log_debug(gc, ergo, heap)("Attempt heap shrinking (uncommitted archive regions). Total size: " SIZE_FORMAT "B",
 810                               HeapRegion::GrainWords * HeapWordSize * uncommitted_regions);
 811   }
 812   decrease_used(size_used);
 813 }
 814 
 815 oop G1CollectedHeap::materialize_archived_object(oop obj) {
 816   assert(obj != NULL, "archived obj is NULL");
 817   assert(G1ArchiveAllocator::is_archived_object(obj), "must be archived object");
 818 
 819   // Loading an archived object makes it strongly reachable. If it is
 820   // loaded during concurrent marking, it must be enqueued to the SATB
 821   // queue, shading the previously white object gray.
 822   G1BarrierSet::enqueue(obj);
 823 
 824   return obj;
 825 }
 826 
 827 HeapWord* G1CollectedHeap::attempt_allocation_humongous(size_t word_size) {
 828   ResourceMark rm; // For retrieving the thread names in log messages.
 829 
 830   // The structure of this method has a lot of similarities to
 831   // attempt_allocation_slow(). The reason these two were not merged
 832   // into a single one is that such a method would require several "if
 833   // allocation is not humongous do this, otherwise do that"
 834   // conditional paths which would obscure its flow. In fact, an early
 835   // version of this code did use a unified method which was harder to
 836   // follow and, as a result, it had subtle bugs that were hard to
 837   // track down. So keeping these two methods separate allows each to
 838   // be more readable. It will be good to keep these two in sync as
 839   // much as possible.
 840 
 841   assert_heap_not_locked_and_not_at_safepoint();
 842   assert(is_humongous(word_size), "attempt_allocation_humongous() "
 843          "should only be called for humongous allocations");
 844 
 845   // Humongous objects can exhaust the heap quickly, so we should check if we
 846   // need to start a marking cycle at each humongous object allocation. We do
 847   // the check before we do the actual allocation. The reason for doing it
 848   // before the allocation is that we avoid having to keep track of the newly
 849   // allocated memory while we do a GC.
 850   if (policy()-&gt;need_to_start_conc_mark("concurrent humongous allocation",
 851                                            word_size)) {
 852     collect(GCCause::_g1_humongous_allocation);
 853   }
 854 
 855   // We will loop until a) we manage to successfully perform the
 856   // allocation or b) we successfully schedule a collection which
 857   // fails to perform the allocation. b) is the only case when we'll
 858   // return NULL.
 859   HeapWord* result = NULL;
 860   for (uint try_count = 1, gclocker_retry_count = 0; /* we'll return */; try_count += 1) {
 861     bool should_try_gc;
 862     uint gc_count_before;
 863 
 864 
 865     {
 866       MutexLocker x(Heap_lock);
 867 
 868       // Given that humongous objects are not allocated in young
 869       // regions, we'll first try to do the allocation without doing a
 870       // collection hoping that there's enough space in the heap.
 871       result = humongous_obj_allocate(word_size);
 872       if (result != NULL) {
 873         size_t size_in_regions = humongous_obj_size_in_regions(word_size);
 874         policy()-&gt;old_gen_alloc_tracker()-&gt;
 875           add_allocated_bytes_since_last_gc(size_in_regions * HeapRegion::GrainBytes);
 876         return result;
 877       }
 878 
 879       // Only try a GC if the GCLocker does not signal the need for a GC. Wait until
 880       // the GCLocker initiated GC has been performed and then retry. This includes
 881       // the case when the GC Locker is not active but has not been performed.
 882       should_try_gc = !GCLocker::needs_gc();
 883       // Read the GC count while still holding the Heap_lock.
 884       gc_count_before = total_collections();
 885     }
 886 
 887     if (should_try_gc) {
 888       bool succeeded;
 889       result = do_collection_pause(word_size, gc_count_before, &amp;succeeded,
 890                                    GCCause::_g1_humongous_allocation);
 891       if (result != NULL) {
 892         assert(succeeded, "only way to get back a non-NULL result");
 893         log_trace(gc, alloc)("%s: Successfully scheduled collection returning " PTR_FORMAT,
 894                              Thread::current()-&gt;name(), p2i(result));
 895         return result;
 896       }
 897 
 898       if (succeeded) {
 899         // We successfully scheduled a collection which failed to allocate. No
 900         // point in trying to allocate further. We'll just return NULL.
 901         log_trace(gc, alloc)("%s: Successfully scheduled collection failing to allocate "
 902                              SIZE_FORMAT " words", Thread::current()-&gt;name(), word_size);
 903         return NULL;
 904       }
 905       log_trace(gc, alloc)("%s: Unsuccessfully scheduled collection allocating " SIZE_FORMAT "",
 906                            Thread::current()-&gt;name(), word_size);
 907     } else {
 908       // Failed to schedule a collection.
 909       if (gclocker_retry_count &gt; GCLockerRetryAllocationCount) {
 910         log_warning(gc, alloc)("%s: Retried waiting for GCLocker too often allocating "
 911                                SIZE_FORMAT " words", Thread::current()-&gt;name(), word_size);
 912         return NULL;
 913       }
 914       log_trace(gc, alloc)("%s: Stall until clear", Thread::current()-&gt;name());
 915       // The GCLocker is either active or the GCLocker initiated
 916       // GC has not yet been performed. Stall until it is and
 917       // then retry the allocation.
 918       GCLocker::stall_until_clear();
 919       gclocker_retry_count += 1;
 920     }
 921 
 922 
 923     // We can reach here if we were unsuccessful in scheduling a
 924     // collection (because another thread beat us to it) or if we were
 925     // stalled due to the GC locker. In either can we should retry the
 926     // allocation attempt in case another thread successfully
 927     // performed a collection and reclaimed enough space.
 928     // Humongous object allocation always needs a lock, so we wait for the retry
 929     // in the next iteration of the loop, unlike for the regular iteration case.
 930     // Give a warning if we seem to be looping forever.
 931 
 932     if ((QueuedAllocationWarningCount &gt; 0) &amp;&amp;
 933         (try_count % QueuedAllocationWarningCount == 0)) {
 934       log_warning(gc, alloc)("%s: Retried allocation %u times for " SIZE_FORMAT " words",
 935                              Thread::current()-&gt;name(), try_count, word_size);
 936     }
 937   }
 938 
 939   ShouldNotReachHere();
 940   return NULL;
 941 }
 942 
 943 HeapWord* G1CollectedHeap::attempt_allocation_at_safepoint(size_t word_size,
 944                                                            bool expect_null_mutator_alloc_region) {
 945   assert_at_safepoint_on_vm_thread();
 946   assert(!_allocator-&gt;has_mutator_alloc_region() || !expect_null_mutator_alloc_region,
 947          "the current alloc region was unexpectedly found to be non-NULL");
 948 
 949   if (!is_humongous(word_size)) {
 950     return _allocator-&gt;attempt_allocation_locked(word_size);
 951   } else {
 952     HeapWord* result = humongous_obj_allocate(word_size);
 953     if (result != NULL &amp;&amp; policy()-&gt;need_to_start_conc_mark("STW humongous allocation")) {
 954       collector_state()-&gt;set_initiate_conc_mark_if_possible(true);
 955     }
 956     return result;
 957   }
 958 
 959   ShouldNotReachHere();
 960 }
 961 
 962 class PostCompactionPrinterClosure: public HeapRegionClosure {
 963 private:
 964   G1HRPrinter* _hr_printer;
 965 public:
 966   bool do_heap_region(HeapRegion* hr) {
 967     assert(!hr-&gt;is_young(), "not expecting to find young regions");
 968     _hr_printer-&gt;post_compaction(hr);
 969     return false;
 970   }
 971 
 972   PostCompactionPrinterClosure(G1HRPrinter* hr_printer)
 973     : _hr_printer(hr_printer) { }
 974 };
 975 
 976 void G1CollectedHeap::print_hrm_post_compaction() {
 977   if (_hr_printer.is_active()) {
 978     PostCompactionPrinterClosure cl(hr_printer());
 979     heap_region_iterate(&amp;cl);
 980   }
 981 }
 982 
 983 void G1CollectedHeap::abort_concurrent_cycle() {
 984   // If we start the compaction before the CM threads finish
 985   // scanning the root regions we might trip them over as we'll
 986   // be moving objects / updating references. So let's wait until
 987   // they are done. By telling them to abort, they should complete
 988   // early.
 989   _cm-&gt;root_regions()-&gt;abort();
 990   _cm-&gt;root_regions()-&gt;wait_until_scan_finished();
 991 
 992   // Disable discovery and empty the discovered lists
 993   // for the CM ref processor.
 994   _ref_processor_cm-&gt;disable_discovery();
 995   _ref_processor_cm-&gt;abandon_partial_discovery();
 996   _ref_processor_cm-&gt;verify_no_references_recorded();
 997 
 998   // Abandon current iterations of concurrent marking and concurrent
 999   // refinement, if any are in progress.
1000   concurrent_mark()-&gt;concurrent_cycle_abort();
1001 }
1002 
1003 void G1CollectedHeap::prepare_heap_for_full_collection() {
1004   // Make sure we'll choose a new allocation region afterwards.
1005   _allocator-&gt;release_mutator_alloc_regions();
1006   _allocator-&gt;abandon_gc_alloc_regions();
1007 
1008   // We may have added regions to the current incremental collection
1009   // set between the last GC or pause and now. We need to clear the
1010   // incremental collection set and then start rebuilding it afresh
1011   // after this full GC.
1012   abandon_collection_set(collection_set());
1013 
1014   tear_down_region_sets(false /* free_list_only */);
1015 
1016   hrm()-&gt;prepare_for_full_collection_start();
1017 }
1018 
1019 void G1CollectedHeap::verify_before_full_collection(bool explicit_gc) {
1020   assert(!GCCause::is_user_requested_gc(gc_cause()) || explicit_gc, "invariant");
1021   assert_used_and_recalculate_used_equal(this);
1022   _verifier-&gt;verify_region_sets_optional();
1023   _verifier-&gt;verify_before_gc(G1HeapVerifier::G1VerifyFull);
1024   _verifier-&gt;check_bitmaps("Full GC Start");
1025 }
1026 
1027 void G1CollectedHeap::prepare_heap_for_mutators() {
1028   hrm()-&gt;prepare_for_full_collection_end();
1029 
1030   // Delete metaspaces for unloaded class loaders and clean up loader_data graph
1031   ClassLoaderDataGraph::purge();
<a name="1" id="anc1"></a><span class="changed">1032   MetaspaceUtils::verify_metrics();</span>
1033 
1034   // Prepare heap for normal collections.
1035   assert(num_free_regions() == 0, "we should not have added any free regions");
1036   rebuild_region_sets(false /* free_list_only */);
1037   abort_refinement();
1038   resize_heap_if_necessary();
1039 
1040   // Rebuild the strong code root lists for each region
1041   rebuild_strong_code_roots();
1042 
1043   // Purge code root memory
1044   purge_code_root_memory();
1045 
1046   // Start a new incremental collection set for the next pause
1047   start_new_collection_set();
1048 
1049   _allocator-&gt;init_mutator_alloc_regions();
1050 
1051   // Post collection state updates.
1052   MetaspaceGC::compute_new_size();
1053 }
1054 
1055 void G1CollectedHeap::abort_refinement() {
1056   if (_hot_card_cache-&gt;use_cache()) {
1057     _hot_card_cache-&gt;reset_hot_cache();
1058   }
1059 
1060   // Discard all remembered set updates and reset refinement statistics.
1061   G1BarrierSet::dirty_card_queue_set().abandon_logs();
1062   assert(G1BarrierSet::dirty_card_queue_set().num_cards() == 0,
1063          "DCQS should be empty");
1064   concurrent_refine()-&gt;get_and_reset_refinement_stats();
1065 }
1066 
1067 void G1CollectedHeap::verify_after_full_collection() {
1068   _hrm-&gt;verify_optional();
1069   _verifier-&gt;verify_region_sets_optional();
1070   _verifier-&gt;verify_after_gc(G1HeapVerifier::G1VerifyFull);
1071   // Clear the previous marking bitmap, if needed for bitmap verification.
1072   // Note we cannot do this when we clear the next marking bitmap in
1073   // G1ConcurrentMark::abort() above since VerifyDuringGC verifies the
1074   // objects marked during a full GC against the previous bitmap.
1075   // But we need to clear it before calling check_bitmaps below since
1076   // the full GC has compacted objects and updated TAMS but not updated
1077   // the prev bitmap.
1078   if (G1VerifyBitmaps) {
1079     GCTraceTime(Debug, gc) tm("Clear Prev Bitmap for Verification");
1080     _cm-&gt;clear_prev_bitmap(workers());
1081   }
1082   // This call implicitly verifies that the next bitmap is clear after Full GC.
1083   _verifier-&gt;check_bitmaps("Full GC End");
1084 
1085   // At this point there should be no regions in the
1086   // entire heap tagged as young.
1087   assert(check_young_list_empty(), "young list should be empty at this point");
1088 
1089   // Note: since we've just done a full GC, concurrent
1090   // marking is no longer active. Therefore we need not
1091   // re-enable reference discovery for the CM ref processor.
1092   // That will be done at the start of the next marking cycle.
1093   // We also know that the STW processor should no longer
1094   // discover any new references.
1095   assert(!_ref_processor_stw-&gt;discovery_enabled(), "Postcondition");
1096   assert(!_ref_processor_cm-&gt;discovery_enabled(), "Postcondition");
1097   _ref_processor_stw-&gt;verify_no_references_recorded();
1098   _ref_processor_cm-&gt;verify_no_references_recorded();
1099 }
1100 
1101 void G1CollectedHeap::print_heap_after_full_collection(G1HeapTransition* heap_transition) {
1102   // Post collection logging.
1103   // We should do this after we potentially resize the heap so
1104   // that all the COMMIT / UNCOMMIT events are generated before
1105   // the compaction events.
1106   print_hrm_post_compaction();
1107   heap_transition-&gt;print();
1108   print_heap_after_gc();
1109   print_heap_regions();
1110 }
1111 
1112 bool G1CollectedHeap::do_full_collection(bool explicit_gc,
1113                                          bool clear_all_soft_refs) {
1114   assert_at_safepoint_on_vm_thread();
1115 
1116   if (GCLocker::check_active_before_gc()) {
1117     // Full GC was not completed.
1118     return false;
1119   }
1120 
1121   const bool do_clear_all_soft_refs = clear_all_soft_refs ||
1122       soft_ref_policy()-&gt;should_clear_all_soft_refs();
1123 
1124   G1FullCollector collector(this, explicit_gc, do_clear_all_soft_refs);
1125   GCTraceTime(Info, gc) tm("Pause Full", NULL, gc_cause(), true);
1126 
1127   collector.prepare_collection();
1128   collector.collect();
1129   collector.complete_collection();
1130 
1131   // Full collection was successfully completed.
1132   return true;
1133 }
1134 
1135 void G1CollectedHeap::do_full_collection(bool clear_all_soft_refs) {
1136   // Currently, there is no facility in the do_full_collection(bool) API to notify
1137   // the caller that the collection did not succeed (e.g., because it was locked
1138   // out by the GC locker). So, right now, we'll ignore the return value.
1139   bool dummy = do_full_collection(true,                /* explicit_gc */
1140                                   clear_all_soft_refs);
1141 }
1142 
1143 void G1CollectedHeap::resize_heap_if_necessary() {
1144   assert_at_safepoint_on_vm_thread();
1145 
1146   bool should_expand;
1147   size_t resize_amount = _heap_sizing_policy-&gt;full_collection_resize_amount(should_expand);
1148 
1149   if (resize_amount == 0) {
1150     return;
1151   } else if (should_expand) {
1152     expand(resize_amount, _workers);
1153   } else {
1154     shrink(resize_amount);
1155   }
1156 }
1157 
1158 HeapWord* G1CollectedHeap::satisfy_failed_allocation_helper(size_t word_size,
1159                                                             bool do_gc,
1160                                                             bool clear_all_soft_refs,
1161                                                             bool expect_null_mutator_alloc_region,
1162                                                             bool* gc_succeeded) {
1163   *gc_succeeded = true;
1164   // Let's attempt the allocation first.
1165   HeapWord* result =
1166     attempt_allocation_at_safepoint(word_size,
1167                                     expect_null_mutator_alloc_region);
1168   if (result != NULL) {
1169     return result;
1170   }
1171 
1172   // In a G1 heap, we're supposed to keep allocation from failing by
1173   // incremental pauses.  Therefore, at least for now, we'll favor
1174   // expansion over collection.  (This might change in the future if we can
1175   // do something smarter than full collection to satisfy a failed alloc.)
1176   result = expand_and_allocate(word_size);
1177   if (result != NULL) {
1178     return result;
1179   }
1180 
1181   if (do_gc) {
1182     // Expansion didn't work, we'll try to do a Full GC.
1183     *gc_succeeded = do_full_collection(false, /* explicit_gc */
1184                                        clear_all_soft_refs);
1185   }
1186 
1187   return NULL;
1188 }
1189 
1190 HeapWord* G1CollectedHeap::satisfy_failed_allocation(size_t word_size,
1191                                                      bool* succeeded) {
1192   assert_at_safepoint_on_vm_thread();
1193 
1194   // Attempts to allocate followed by Full GC.
1195   HeapWord* result =
1196     satisfy_failed_allocation_helper(word_size,
1197                                      true,  /* do_gc */
1198                                      false, /* clear_all_soft_refs */
1199                                      false, /* expect_null_mutator_alloc_region */
1200                                      succeeded);
1201 
1202   if (result != NULL || !*succeeded) {
1203     return result;
1204   }
1205 
1206   // Attempts to allocate followed by Full GC that will collect all soft references.
1207   result = satisfy_failed_allocation_helper(word_size,
1208                                             true, /* do_gc */
1209                                             true, /* clear_all_soft_refs */
1210                                             true, /* expect_null_mutator_alloc_region */
1211                                             succeeded);
1212 
1213   if (result != NULL || !*succeeded) {
1214     return result;
1215   }
1216 
1217   // Attempts to allocate, no GC
1218   result = satisfy_failed_allocation_helper(word_size,
1219                                             false, /* do_gc */
1220                                             false, /* clear_all_soft_refs */
1221                                             true,  /* expect_null_mutator_alloc_region */
1222                                             succeeded);
1223 
1224   if (result != NULL) {
1225     return result;
1226   }
1227 
1228   assert(!soft_ref_policy()-&gt;should_clear_all_soft_refs(),
1229          "Flag should have been handled and cleared prior to this point");
1230 
1231   // What else?  We might try synchronous finalization later.  If the total
1232   // space available is large enough for the allocation, then a more
1233   // complete compaction phase than we've tried so far might be
1234   // appropriate.
1235   return NULL;
1236 }
1237 
1238 // Attempting to expand the heap sufficiently
1239 // to support an allocation of the given "word_size".  If
1240 // successful, perform the allocation and return the address of the
1241 // allocated block, or else "NULL".
1242 
1243 HeapWord* G1CollectedHeap::expand_and_allocate(size_t word_size) {
1244   assert_at_safepoint_on_vm_thread();
1245 
1246   _verifier-&gt;verify_region_sets_optional();
1247 
1248   size_t expand_bytes = MAX2(word_size * HeapWordSize, MinHeapDeltaBytes);
1249   log_debug(gc, ergo, heap)("Attempt heap expansion (allocation request failed). Allocation request: " SIZE_FORMAT "B",
1250                             word_size * HeapWordSize);
1251 
1252 
1253   if (expand(expand_bytes, _workers)) {
1254     _hrm-&gt;verify_optional();
1255     _verifier-&gt;verify_region_sets_optional();
1256     return attempt_allocation_at_safepoint(word_size,
1257                                            false /* expect_null_mutator_alloc_region */);
1258   }
1259   return NULL;
1260 }
1261 
1262 bool G1CollectedHeap::expand(size_t expand_bytes, WorkGang* pretouch_workers, double* expand_time_ms) {
1263   size_t aligned_expand_bytes = ReservedSpace::page_align_size_up(expand_bytes);
1264   aligned_expand_bytes = align_up(aligned_expand_bytes,
1265                                        HeapRegion::GrainBytes);
1266 
1267   log_debug(gc, ergo, heap)("Expand the heap. requested expansion amount: " SIZE_FORMAT "B expansion amount: " SIZE_FORMAT "B",
1268                             expand_bytes, aligned_expand_bytes);
1269 
1270   if (is_maximal_no_gc()) {
1271     log_debug(gc, ergo, heap)("Did not expand the heap (heap already fully expanded)");
1272     return false;
1273   }
1274 
1275   double expand_heap_start_time_sec = os::elapsedTime();
1276   uint regions_to_expand = (uint)(aligned_expand_bytes / HeapRegion::GrainBytes);
1277   assert(regions_to_expand &gt; 0, "Must expand by at least one region");
1278 
1279   uint expanded_by = _hrm-&gt;expand_by(regions_to_expand, pretouch_workers);
1280   if (expand_time_ms != NULL) {
1281     *expand_time_ms = (os::elapsedTime() - expand_heap_start_time_sec) * MILLIUNITS;
1282   }
1283 
1284   if (expanded_by &gt; 0) {
1285     size_t actual_expand_bytes = expanded_by * HeapRegion::GrainBytes;
1286     assert(actual_expand_bytes &lt;= aligned_expand_bytes, "post-condition");
1287     policy()-&gt;record_new_heap_size(num_regions());
1288   } else {
1289     log_debug(gc, ergo, heap)("Did not expand the heap (heap expansion operation failed)");
1290 
1291     // The expansion of the virtual storage space was unsuccessful.
1292     // Let's see if it was because we ran out of swap.
1293     if (G1ExitOnExpansionFailure &amp;&amp;
1294         _hrm-&gt;available() &gt;= regions_to_expand) {
1295       // We had head room...
1296       vm_exit_out_of_memory(aligned_expand_bytes, OOM_MMAP_ERROR, "G1 heap expansion");
1297     }
1298   }
1299   return regions_to_expand &gt; 0;
1300 }
1301 
1302 bool G1CollectedHeap::expand_single_region(uint node_index) {
1303   uint expanded_by = _hrm-&gt;expand_on_preferred_node(node_index);
1304 
1305   if (expanded_by == 0) {
1306     assert(is_maximal_no_gc(), "Should be no regions left, available: %u", _hrm-&gt;available());
1307     log_debug(gc, ergo, heap)("Did not expand the heap (heap already fully expanded)");
1308     return false;
1309   }
1310 
1311   policy()-&gt;record_new_heap_size(num_regions());
1312   return true;
1313 }
1314 
1315 void G1CollectedHeap::shrink_helper(size_t shrink_bytes) {
1316   size_t aligned_shrink_bytes =
1317     ReservedSpace::page_align_size_down(shrink_bytes);
1318   aligned_shrink_bytes = align_down(aligned_shrink_bytes,
1319                                          HeapRegion::GrainBytes);
1320   uint num_regions_to_remove = (uint)(shrink_bytes / HeapRegion::GrainBytes);
1321 
1322   uint num_regions_removed = _hrm-&gt;shrink_by(num_regions_to_remove);
1323   size_t shrunk_bytes = num_regions_removed * HeapRegion::GrainBytes;
1324 
1325   log_debug(gc, ergo, heap)("Shrink the heap. requested shrinking amount: " SIZE_FORMAT "B aligned shrinking amount: " SIZE_FORMAT "B attempted shrinking amount: " SIZE_FORMAT "B",
1326                             shrink_bytes, aligned_shrink_bytes, shrunk_bytes);
1327   if (num_regions_removed &gt; 0) {
1328     policy()-&gt;record_new_heap_size(num_regions());
1329   } else {
1330     log_debug(gc, ergo, heap)("Did not expand the heap (heap shrinking operation failed)");
1331   }
1332 }
1333 
1334 void G1CollectedHeap::shrink(size_t shrink_bytes) {
1335   _verifier-&gt;verify_region_sets_optional();
1336 
1337   // We should only reach here at the end of a Full GC or during Remark which
1338   // means we should not not be holding to any GC alloc regions. The method
1339   // below will make sure of that and do any remaining clean up.
1340   _allocator-&gt;abandon_gc_alloc_regions();
1341 
1342   // Instead of tearing down / rebuilding the free lists here, we
1343   // could instead use the remove_all_pending() method on free_list to
1344   // remove only the ones that we need to remove.
1345   tear_down_region_sets(true /* free_list_only */);
1346   shrink_helper(shrink_bytes);
1347   rebuild_region_sets(true /* free_list_only */);
1348 
1349   _hrm-&gt;verify_optional();
1350   _verifier-&gt;verify_region_sets_optional();
1351 }
1352 
1353 class OldRegionSetChecker : public HeapRegionSetChecker {
1354 public:
1355   void check_mt_safety() {
1356     // Master Old Set MT safety protocol:
1357     // (a) If we're at a safepoint, operations on the master old set
1358     // should be invoked:
1359     // - by the VM thread (which will serialize them), or
1360     // - by the GC workers while holding the FreeList_lock, if we're
1361     //   at a safepoint for an evacuation pause (this lock is taken
1362     //   anyway when an GC alloc region is retired so that a new one
1363     //   is allocated from the free list), or
1364     // - by the GC workers while holding the OldSets_lock, if we're at a
1365     //   safepoint for a cleanup pause.
1366     // (b) If we're not at a safepoint, operations on the master old set
1367     // should be invoked while holding the Heap_lock.
1368 
1369     if (SafepointSynchronize::is_at_safepoint()) {
1370       guarantee(Thread::current()-&gt;is_VM_thread() ||
1371                 FreeList_lock-&gt;owned_by_self() || OldSets_lock-&gt;owned_by_self(),
1372                 "master old set MT safety protocol at a safepoint");
1373     } else {
1374       guarantee(Heap_lock-&gt;owned_by_self(), "master old set MT safety protocol outside a safepoint");
1375     }
1376   }
1377   bool is_correct_type(HeapRegion* hr) { return hr-&gt;is_old(); }
1378   const char* get_description() { return "Old Regions"; }
1379 };
1380 
1381 class ArchiveRegionSetChecker : public HeapRegionSetChecker {
1382 public:
1383   void check_mt_safety() {
1384     guarantee(!Universe::is_fully_initialized() || SafepointSynchronize::is_at_safepoint(),
1385               "May only change archive regions during initialization or safepoint.");
1386   }
1387   bool is_correct_type(HeapRegion* hr) { return hr-&gt;is_archive(); }
1388   const char* get_description() { return "Archive Regions"; }
1389 };
1390 
1391 class HumongousRegionSetChecker : public HeapRegionSetChecker {
1392 public:
1393   void check_mt_safety() {
1394     // Humongous Set MT safety protocol:
1395     // (a) If we're at a safepoint, operations on the master humongous
1396     // set should be invoked by either the VM thread (which will
1397     // serialize them) or by the GC workers while holding the
1398     // OldSets_lock.
1399     // (b) If we're not at a safepoint, operations on the master
1400     // humongous set should be invoked while holding the Heap_lock.
1401 
1402     if (SafepointSynchronize::is_at_safepoint()) {
1403       guarantee(Thread::current()-&gt;is_VM_thread() ||
1404                 OldSets_lock-&gt;owned_by_self(),
1405                 "master humongous set MT safety protocol at a safepoint");
1406     } else {
1407       guarantee(Heap_lock-&gt;owned_by_self(),
1408                 "master humongous set MT safety protocol outside a safepoint");
1409     }
1410   }
1411   bool is_correct_type(HeapRegion* hr) { return hr-&gt;is_humongous(); }
1412   const char* get_description() { return "Humongous Regions"; }
1413 };
1414 
1415 G1CollectedHeap::G1CollectedHeap() :
1416   CollectedHeap(),
1417   _young_gen_sampling_thread(NULL),
1418   _workers(NULL),
1419   _card_table(NULL),
1420   _collection_pause_end(Ticks::now()),
1421   _soft_ref_policy(),
1422   _old_set("Old Region Set", new OldRegionSetChecker()),
1423   _archive_set("Archive Region Set", new ArchiveRegionSetChecker()),
1424   _humongous_set("Humongous Region Set", new HumongousRegionSetChecker()),
1425   _bot(NULL),
1426   _listener(),
1427   _numa(G1NUMA::create()),
1428   _hrm(NULL),
1429   _allocator(NULL),
1430   _verifier(NULL),
1431   _summary_bytes_used(0),
1432   _bytes_used_during_gc(0),
1433   _archive_allocator(NULL),
1434   _survivor_evac_stats("Young", YoungPLABSize, PLABWeight),
1435   _old_evac_stats("Old", OldPLABSize, PLABWeight),
1436   _expand_heap_after_alloc_failure(true),
1437   _g1mm(NULL),
1438   _humongous_reclaim_candidates(),
1439   _has_humongous_reclaim_candidates(false),
1440   _hr_printer(),
1441   _collector_state(),
1442   _old_marking_cycles_started(0),
1443   _old_marking_cycles_completed(0),
1444   _eden(),
1445   _survivor(),
1446   _gc_timer_stw(new (ResourceObj::C_HEAP, mtGC) STWGCTimer()),
1447   _gc_tracer_stw(new (ResourceObj::C_HEAP, mtGC) G1NewTracer()),
1448   _policy(G1Policy::create_policy(_gc_timer_stw)),
1449   _heap_sizing_policy(NULL),
1450   _collection_set(this, _policy),
1451   _hot_card_cache(NULL),
1452   _rem_set(NULL),
1453   _cm(NULL),
1454   _cm_thread(NULL),
1455   _cr(NULL),
1456   _task_queues(NULL),
1457   _evacuation_failed(false),
1458   _evacuation_failed_info_array(NULL),
1459   _preserved_marks_set(true /* in_c_heap */),
1460 #ifndef PRODUCT
1461   _evacuation_failure_alot_for_current_gc(false),
1462   _evacuation_failure_alot_gc_number(0),
1463   _evacuation_failure_alot_count(0),
1464 #endif
1465   _ref_processor_stw(NULL),
1466   _is_alive_closure_stw(this),
1467   _is_subject_to_discovery_stw(this),
1468   _ref_processor_cm(NULL),
1469   _is_alive_closure_cm(this),
1470   _is_subject_to_discovery_cm(this),
1471   _region_attr() {
1472 
1473   _verifier = new G1HeapVerifier(this);
1474 
1475   _allocator = new G1Allocator(this);
1476 
1477   _heap_sizing_policy = G1HeapSizingPolicy::create(this, _policy-&gt;analytics());
1478 
1479   _humongous_object_threshold_in_words = humongous_threshold_for(HeapRegion::GrainWords);
1480 
1481   // Override the default _filler_array_max_size so that no humongous filler
1482   // objects are created.
1483   _filler_array_max_size = _humongous_object_threshold_in_words;
1484 
1485   uint n_queues = ParallelGCThreads;
1486   _task_queues = new G1ScannerTasksQueueSet(n_queues);
1487 
1488   _evacuation_failed_info_array = NEW_C_HEAP_ARRAY(EvacuationFailedInfo, n_queues, mtGC);
1489 
1490   for (uint i = 0; i &lt; n_queues; i++) {
1491     G1ScannerTasksQueue* q = new G1ScannerTasksQueue();
1492     q-&gt;initialize();
1493     _task_queues-&gt;register_queue(i, q);
1494     ::new (&amp;_evacuation_failed_info_array[i]) EvacuationFailedInfo();
1495   }
1496 
1497   // Initialize the G1EvacuationFailureALot counters and flags.
1498   NOT_PRODUCT(reset_evacuation_should_fail();)
1499   _gc_tracer_stw-&gt;initialize();
1500 
1501   guarantee(_task_queues != NULL, "task_queues allocation failure.");
1502 }
1503 
1504 static size_t actual_reserved_page_size(ReservedSpace rs) {
1505   size_t page_size = os::vm_page_size();
1506   if (UseLargePages) {
1507     // There are two ways to manage large page memory.
1508     // 1. OS supports committing large page memory.
1509     // 2. OS doesn't support committing large page memory so ReservedSpace manages it.
1510     //    And ReservedSpace calls it 'special'. If we failed to set 'special',
1511     //    we reserved memory without large page.
1512     if (os::can_commit_large_page_memory() || rs.special()) {
1513       // An alignment at ReservedSpace comes from preferred page size or
1514       // heap alignment, and if the alignment came from heap alignment, it could be
1515       // larger than large pages size. So need to cap with the large page size.
1516       page_size = MIN2(rs.alignment(), os::large_page_size());
1517     }
1518   }
1519 
1520   return page_size;
1521 }
1522 
1523 G1RegionToSpaceMapper* G1CollectedHeap::create_aux_memory_mapper(const char* description,
1524                                                                  size_t size,
1525                                                                  size_t translation_factor) {
1526   size_t preferred_page_size = os::page_size_for_region_unaligned(size, 1);
1527   // Allocate a new reserved space, preferring to use large pages.
1528   ReservedSpace rs(size, preferred_page_size);
1529   size_t page_size = actual_reserved_page_size(rs);
1530   G1RegionToSpaceMapper* result  =
1531     G1RegionToSpaceMapper::create_mapper(rs,
1532                                          size,
1533                                          page_size,
1534                                          HeapRegion::GrainBytes,
1535                                          translation_factor,
1536                                          mtGC);
1537 
1538   os::trace_page_sizes_for_requested_size(description,
1539                                           size,
1540                                           preferred_page_size,
1541                                           page_size,
1542                                           rs.base(),
1543                                           rs.size());
1544 
1545   return result;
1546 }
1547 
1548 jint G1CollectedHeap::initialize_concurrent_refinement() {
1549   jint ecode = JNI_OK;
1550   _cr = G1ConcurrentRefine::create(&amp;ecode);
1551   return ecode;
1552 }
1553 
1554 jint G1CollectedHeap::initialize_young_gen_sampling_thread() {
1555   _young_gen_sampling_thread = new G1YoungRemSetSamplingThread();
1556   if (_young_gen_sampling_thread-&gt;osthread() == NULL) {
1557     vm_shutdown_during_initialization("Could not create G1YoungRemSetSamplingThread");
1558     return JNI_ENOMEM;
1559   }
1560   return JNI_OK;
1561 }
1562 
1563 jint G1CollectedHeap::initialize() {
1564 
1565   // Necessary to satisfy locking discipline assertions.
1566 
1567   MutexLocker x(Heap_lock);
1568 
1569   // While there are no constraints in the GC code that HeapWordSize
1570   // be any particular value, there are multiple other areas in the
1571   // system which believe this to be true (e.g. oop-&gt;object_size in some
1572   // cases incorrectly returns the size in wordSize units rather than
1573   // HeapWordSize).
1574   guarantee(HeapWordSize == wordSize, "HeapWordSize must equal wordSize");
1575 
1576   size_t init_byte_size = InitialHeapSize;
1577   size_t reserved_byte_size = G1Arguments::heap_reserved_size_bytes();
1578 
1579   // Ensure that the sizes are properly aligned.
1580   Universe::check_alignment(init_byte_size, HeapRegion::GrainBytes, "g1 heap");
1581   Universe::check_alignment(reserved_byte_size, HeapRegion::GrainBytes, "g1 heap");
1582   Universe::check_alignment(reserved_byte_size, HeapAlignment, "g1 heap");
1583 
1584   // Reserve the maximum.
1585 
1586   // When compressed oops are enabled, the preferred heap base
1587   // is calculated by subtracting the requested size from the
1588   // 32Gb boundary and using the result as the base address for
1589   // heap reservation. If the requested size is not aligned to
1590   // HeapRegion::GrainBytes (i.e. the alignment that is passed
1591   // into the ReservedHeapSpace constructor) then the actual
1592   // base of the reserved heap may end up differing from the
1593   // address that was requested (i.e. the preferred heap base).
1594   // If this happens then we could end up using a non-optimal
1595   // compressed oops mode.
1596 
1597   ReservedHeapSpace heap_rs = Universe::reserve_heap(reserved_byte_size,
1598                                                      HeapAlignment);
1599 
1600   initialize_reserved_region(heap_rs);
1601 
1602   // Create the barrier set for the entire reserved region.
1603   G1CardTable* ct = new G1CardTable(heap_rs.region());
1604   ct-&gt;initialize();
1605   G1BarrierSet* bs = new G1BarrierSet(ct);
1606   bs-&gt;initialize();
1607   assert(bs-&gt;is_a(BarrierSet::G1BarrierSet), "sanity");
1608   BarrierSet::set_barrier_set(bs);
1609   _card_table = ct;
1610 
1611   {
1612     G1SATBMarkQueueSet&amp; satbqs = bs-&gt;satb_mark_queue_set();
1613     satbqs.set_process_completed_buffers_threshold(G1SATBProcessCompletedThreshold);
1614     satbqs.set_buffer_enqueue_threshold_percentage(G1SATBBufferEnqueueingThresholdPercent);
1615   }
1616 
1617   // Create the hot card cache.
1618   _hot_card_cache = new G1HotCardCache(this);
1619 
1620   // Carve out the G1 part of the heap.
1621   ReservedSpace g1_rs = heap_rs.first_part(reserved_byte_size);
1622   size_t page_size = actual_reserved_page_size(heap_rs);
1623   G1RegionToSpaceMapper* heap_storage =
1624     G1RegionToSpaceMapper::create_heap_mapper(g1_rs,
1625                                               g1_rs.size(),
1626                                               page_size,
1627                                               HeapRegion::GrainBytes,
1628                                               1,
1629                                               mtJavaHeap);
1630   if(heap_storage == NULL) {
1631     vm_shutdown_during_initialization("Could not initialize G1 heap");
1632     return JNI_ERR;
1633   }
1634 
1635   os::trace_page_sizes("Heap",
1636                        MinHeapSize,
1637                        reserved_byte_size,
1638                        page_size,
1639                        heap_rs.base(),
1640                        heap_rs.size());
1641   heap_storage-&gt;set_mapping_changed_listener(&amp;_listener);
1642 
1643   // Create storage for the BOT, card table, card counts table (hot card cache) and the bitmaps.
1644   G1RegionToSpaceMapper* bot_storage =
1645     create_aux_memory_mapper("Block Offset Table",
1646                              G1BlockOffsetTable::compute_size(g1_rs.size() / HeapWordSize),
1647                              G1BlockOffsetTable::heap_map_factor());
1648 
1649   G1RegionToSpaceMapper* cardtable_storage =
1650     create_aux_memory_mapper("Card Table",
1651                              G1CardTable::compute_size(g1_rs.size() / HeapWordSize),
1652                              G1CardTable::heap_map_factor());
1653 
1654   G1RegionToSpaceMapper* card_counts_storage =
1655     create_aux_memory_mapper("Card Counts Table",
1656                              G1CardCounts::compute_size(g1_rs.size() / HeapWordSize),
1657                              G1CardCounts::heap_map_factor());
1658 
1659   size_t bitmap_size = G1CMBitMap::compute_size(g1_rs.size());
1660   G1RegionToSpaceMapper* prev_bitmap_storage =
1661     create_aux_memory_mapper("Prev Bitmap", bitmap_size, G1CMBitMap::heap_map_factor());
1662   G1RegionToSpaceMapper* next_bitmap_storage =
1663     create_aux_memory_mapper("Next Bitmap", bitmap_size, G1CMBitMap::heap_map_factor());
1664 
1665   _hrm = HeapRegionManager::create_manager(this);
1666 
1667   _hrm-&gt;initialize(heap_storage, prev_bitmap_storage, next_bitmap_storage, bot_storage, cardtable_storage, card_counts_storage);
1668   _card_table-&gt;initialize(cardtable_storage);
1669 
1670   // Do later initialization work for concurrent refinement.
1671   _hot_card_cache-&gt;initialize(card_counts_storage);
1672 
1673   // 6843694 - ensure that the maximum region index can fit
1674   // in the remembered set structures.
1675   const uint max_region_idx = (1U &lt;&lt; (sizeof(RegionIdx_t)*BitsPerByte-1)) - 1;
1676   guarantee((max_regions() - 1) &lt;= max_region_idx, "too many regions");
1677 
1678   // The G1FromCardCache reserves card with value 0 as "invalid", so the heap must not
1679   // start within the first card.
1680   guarantee(g1_rs.base() &gt;= (char*)G1CardTable::card_size, "Java heap must not start within the first card.");
1681   // Also create a G1 rem set.
1682   _rem_set = new G1RemSet(this, _card_table, _hot_card_cache);
1683   _rem_set-&gt;initialize(max_reserved_capacity(), max_regions());
1684 
1685   size_t max_cards_per_region = ((size_t)1 &lt;&lt; (sizeof(CardIdx_t)*BitsPerByte-1)) - 1;
1686   guarantee(HeapRegion::CardsPerRegion &gt; 0, "make sure it's initialized");
1687   guarantee(HeapRegion::CardsPerRegion &lt; max_cards_per_region,
1688             "too many cards per region");
1689 
1690   FreeRegionList::set_unrealistically_long_length(max_expandable_regions() + 1);
1691 
1692   _bot = new G1BlockOffsetTable(reserved_region(), bot_storage);
1693 
1694   {
1695     HeapWord* start = _hrm-&gt;reserved().start();
1696     HeapWord* end = _hrm-&gt;reserved().end();
1697     size_t granularity = HeapRegion::GrainBytes;
1698 
1699     _region_attr.initialize(start, end, granularity);
1700     _humongous_reclaim_candidates.initialize(start, end, granularity);
1701   }
1702 
1703   _workers = new WorkGang("GC Thread", ParallelGCThreads,
1704                           true /* are_GC_task_threads */,
1705                           false /* are_ConcurrentGC_threads */);
1706   if (_workers == NULL) {
1707     return JNI_ENOMEM;
1708   }
1709   _workers-&gt;initialize_workers();
1710 
1711   _numa-&gt;set_region_info(HeapRegion::GrainBytes, page_size);
1712 
1713   // Create the G1ConcurrentMark data structure and thread.
1714   // (Must do this late, so that "max_regions" is defined.)
1715   _cm = new G1ConcurrentMark(this, prev_bitmap_storage, next_bitmap_storage);
1716   _cm_thread = _cm-&gt;cm_thread();
1717 
1718   // Now expand into the initial heap size.
1719   if (!expand(init_byte_size, _workers)) {
1720     vm_shutdown_during_initialization("Failed to allocate initial heap.");
1721     return JNI_ENOMEM;
1722   }
1723 
1724   // Perform any initialization actions delegated to the policy.
1725   policy()-&gt;init(this, &amp;_collection_set);
1726 
1727   jint ecode = initialize_concurrent_refinement();
1728   if (ecode != JNI_OK) {
1729     return ecode;
1730   }
1731 
1732   ecode = initialize_young_gen_sampling_thread();
1733   if (ecode != JNI_OK) {
1734     return ecode;
1735   }
1736 
1737   {
1738     G1DirtyCardQueueSet&amp; dcqs = G1BarrierSet::dirty_card_queue_set();
1739     dcqs.set_process_cards_threshold(concurrent_refine()-&gt;yellow_zone());
1740     dcqs.set_max_cards(concurrent_refine()-&gt;red_zone());
1741   }
1742 
1743   // Here we allocate the dummy HeapRegion that is required by the
1744   // G1AllocRegion class.
1745   HeapRegion* dummy_region = _hrm-&gt;get_dummy_region();
1746 
1747   // We'll re-use the same region whether the alloc region will
1748   // require BOT updates or not and, if it doesn't, then a non-young
1749   // region will complain that it cannot support allocations without
1750   // BOT updates. So we'll tag the dummy region as eden to avoid that.
1751   dummy_region-&gt;set_eden();
1752   // Make sure it's full.
1753   dummy_region-&gt;set_top(dummy_region-&gt;end());
1754   G1AllocRegion::setup(this, dummy_region);
1755 
1756   _allocator-&gt;init_mutator_alloc_regions();
1757 
1758   // Do create of the monitoring and management support so that
1759   // values in the heap have been properly initialized.
1760   _g1mm = new G1MonitoringSupport(this);
1761 
1762   G1StringDedup::initialize();
1763 
1764   _preserved_marks_set.init(ParallelGCThreads);
1765 
1766   _collection_set.initialize(max_regions());
1767 
1768   G1InitLogger::print();
1769 
1770   return JNI_OK;
1771 }
1772 
1773 void G1CollectedHeap::stop() {
1774   // Stop all concurrent threads. We do this to make sure these threads
1775   // do not continue to execute and access resources (e.g. logging)
1776   // that are destroyed during shutdown.
1777   _cr-&gt;stop();
1778   _young_gen_sampling_thread-&gt;stop();
1779   _cm_thread-&gt;stop();
1780   if (G1StringDedup::is_enabled()) {
1781     G1StringDedup::stop();
1782   }
1783 }
1784 
1785 void G1CollectedHeap::safepoint_synchronize_begin() {
1786   SuspendibleThreadSet::synchronize();
1787 }
1788 
1789 void G1CollectedHeap::safepoint_synchronize_end() {
1790   SuspendibleThreadSet::desynchronize();
1791 }
1792 
1793 void G1CollectedHeap::post_initialize() {
1794   CollectedHeap::post_initialize();
1795   ref_processing_init();
1796 }
1797 
1798 void G1CollectedHeap::ref_processing_init() {
1799   // Reference processing in G1 currently works as follows:
1800   //
1801   // * There are two reference processor instances. One is
1802   //   used to record and process discovered references
1803   //   during concurrent marking; the other is used to
1804   //   record and process references during STW pauses
1805   //   (both full and incremental).
1806   // * Both ref processors need to 'span' the entire heap as
1807   //   the regions in the collection set may be dotted around.
1808   //
1809   // * For the concurrent marking ref processor:
1810   //   * Reference discovery is enabled at concurrent start.
1811   //   * Reference discovery is disabled and the discovered
1812   //     references processed etc during remarking.
1813   //   * Reference discovery is MT (see below).
1814   //   * Reference discovery requires a barrier (see below).
1815   //   * Reference processing may or may not be MT
1816   //     (depending on the value of ParallelRefProcEnabled
1817   //     and ParallelGCThreads).
1818   //   * A full GC disables reference discovery by the CM
1819   //     ref processor and abandons any entries on it's
1820   //     discovered lists.
1821   //
1822   // * For the STW processor:
1823   //   * Non MT discovery is enabled at the start of a full GC.
1824   //   * Processing and enqueueing during a full GC is non-MT.
1825   //   * During a full GC, references are processed after marking.
1826   //
1827   //   * Discovery (may or may not be MT) is enabled at the start
1828   //     of an incremental evacuation pause.
1829   //   * References are processed near the end of a STW evacuation pause.
1830   //   * For both types of GC:
1831   //     * Discovery is atomic - i.e. not concurrent.
1832   //     * Reference discovery will not need a barrier.
1833 
1834   bool mt_processing = ParallelRefProcEnabled &amp;&amp; (ParallelGCThreads &gt; 1);
1835 
1836   // Concurrent Mark ref processor
1837   _ref_processor_cm =
1838     new ReferenceProcessor(&amp;_is_subject_to_discovery_cm,
1839                            mt_processing,                                  // mt processing
1840                            ParallelGCThreads,                              // degree of mt processing
1841                            (ParallelGCThreads &gt; 1) || (ConcGCThreads &gt; 1), // mt discovery
1842                            MAX2(ParallelGCThreads, ConcGCThreads),         // degree of mt discovery
1843                            false,                                          // Reference discovery is not atomic
1844                            &amp;_is_alive_closure_cm,                          // is alive closure
1845                            true);                                          // allow changes to number of processing threads
1846 
1847   // STW ref processor
1848   _ref_processor_stw =
1849     new ReferenceProcessor(&amp;_is_subject_to_discovery_stw,
1850                            mt_processing,                        // mt processing
1851                            ParallelGCThreads,                    // degree of mt processing
1852                            (ParallelGCThreads &gt; 1),              // mt discovery
1853                            ParallelGCThreads,                    // degree of mt discovery
1854                            true,                                 // Reference discovery is atomic
1855                            &amp;_is_alive_closure_stw,               // is alive closure
1856                            true);                                // allow changes to number of processing threads
1857 }
1858 
1859 SoftRefPolicy* G1CollectedHeap::soft_ref_policy() {
1860   return &amp;_soft_ref_policy;
1861 }
1862 
1863 size_t G1CollectedHeap::capacity() const {
1864   return _hrm-&gt;length() * HeapRegion::GrainBytes;
1865 }
1866 
1867 size_t G1CollectedHeap::unused_committed_regions_in_bytes() const {
1868   return _hrm-&gt;total_free_bytes();
1869 }
1870 
1871 void G1CollectedHeap::iterate_hcc_closure(G1CardTableEntryClosure* cl, uint worker_id) {
1872   _hot_card_cache-&gt;drain(cl, worker_id);
1873 }
1874 
1875 // Computes the sum of the storage used by the various regions.
1876 size_t G1CollectedHeap::used() const {
1877   size_t result = _summary_bytes_used + _allocator-&gt;used_in_alloc_regions();
1878   if (_archive_allocator != NULL) {
1879     result += _archive_allocator-&gt;used();
1880   }
1881   return result;
1882 }
1883 
1884 size_t G1CollectedHeap::used_unlocked() const {
1885   return _summary_bytes_used;
1886 }
1887 
1888 class SumUsedClosure: public HeapRegionClosure {
1889   size_t _used;
1890 public:
1891   SumUsedClosure() : _used(0) {}
1892   bool do_heap_region(HeapRegion* r) {
1893     _used += r-&gt;used();
1894     return false;
1895   }
1896   size_t result() { return _used; }
1897 };
1898 
1899 size_t G1CollectedHeap::recalculate_used() const {
1900   SumUsedClosure blk;
1901   heap_region_iterate(&amp;blk);
1902   return blk.result();
1903 }
1904 
1905 bool  G1CollectedHeap::is_user_requested_concurrent_full_gc(GCCause::Cause cause) {
1906   switch (cause) {
1907     case GCCause::_java_lang_system_gc:                 return ExplicitGCInvokesConcurrent;
1908     case GCCause::_dcmd_gc_run:                         return ExplicitGCInvokesConcurrent;
1909     case GCCause::_wb_conc_mark:                        return true;
1910     default :                                           return false;
1911   }
1912 }
1913 
1914 bool G1CollectedHeap::should_do_concurrent_full_gc(GCCause::Cause cause) {
1915   switch (cause) {
1916     case GCCause::_g1_humongous_allocation: return true;
1917     case GCCause::_g1_periodic_collection:  return G1PeriodicGCInvokesConcurrent;
1918     case GCCause::_wb_breakpoint:           return true;
1919     default:                                return is_user_requested_concurrent_full_gc(cause);
1920   }
1921 }
1922 
1923 bool G1CollectedHeap::should_upgrade_to_full_gc(GCCause::Cause cause) {
1924   if (policy()-&gt;force_upgrade_to_full()) {
1925     return true;
1926   } else if (should_do_concurrent_full_gc(_gc_cause)) {
1927     return false;
1928   } else if (has_regions_left_for_allocation()) {
1929     return false;
1930   } else {
1931     return true;
1932   }
1933 }
1934 
1935 #ifndef PRODUCT
1936 void G1CollectedHeap::allocate_dummy_regions() {
1937   // Let's fill up most of the region
1938   size_t word_size = HeapRegion::GrainWords - 1024;
1939   // And as a result the region we'll allocate will be humongous.
1940   guarantee(is_humongous(word_size), "sanity");
1941 
1942   // _filler_array_max_size is set to humongous object threshold
1943   // but temporarily change it to use CollectedHeap::fill_with_object().
1944   AutoModifyRestore&lt;size_t&gt; temporarily(_filler_array_max_size, word_size);
1945 
1946   for (uintx i = 0; i &lt; G1DummyRegionsPerGC; ++i) {
1947     // Let's use the existing mechanism for the allocation
1948     HeapWord* dummy_obj = humongous_obj_allocate(word_size);
1949     if (dummy_obj != NULL) {
1950       MemRegion mr(dummy_obj, word_size);
1951       CollectedHeap::fill_with_object(mr);
1952     } else {
1953       // If we can't allocate once, we probably cannot allocate
1954       // again. Let's get out of the loop.
1955       break;
1956     }
1957   }
1958 }
1959 #endif // !PRODUCT
1960 
1961 void G1CollectedHeap::increment_old_marking_cycles_started() {
1962   assert(_old_marking_cycles_started == _old_marking_cycles_completed ||
1963          _old_marking_cycles_started == _old_marking_cycles_completed + 1,
1964          "Wrong marking cycle count (started: %d, completed: %d)",
1965          _old_marking_cycles_started, _old_marking_cycles_completed);
1966 
1967   _old_marking_cycles_started++;
1968 }
1969 
1970 void G1CollectedHeap::increment_old_marking_cycles_completed(bool concurrent,
1971                                                              bool whole_heap_examined) {
1972   MonitorLocker ml(G1OldGCCount_lock, Mutex::_no_safepoint_check_flag);
1973 
1974   // We assume that if concurrent == true, then the caller is a
1975   // concurrent thread that was joined the Suspendible Thread
1976   // Set. If there's ever a cheap way to check this, we should add an
1977   // assert here.
1978 
1979   // Given that this method is called at the end of a Full GC or of a
1980   // concurrent cycle, and those can be nested (i.e., a Full GC can
1981   // interrupt a concurrent cycle), the number of full collections
1982   // completed should be either one (in the case where there was no
1983   // nesting) or two (when a Full GC interrupted a concurrent cycle)
1984   // behind the number of full collections started.
1985 
1986   // This is the case for the inner caller, i.e. a Full GC.
1987   assert(concurrent ||
1988          (_old_marking_cycles_started == _old_marking_cycles_completed + 1) ||
1989          (_old_marking_cycles_started == _old_marking_cycles_completed + 2),
1990          "for inner caller (Full GC): _old_marking_cycles_started = %u "
1991          "is inconsistent with _old_marking_cycles_completed = %u",
1992          _old_marking_cycles_started, _old_marking_cycles_completed);
1993 
1994   // This is the case for the outer caller, i.e. the concurrent cycle.
1995   assert(!concurrent ||
1996          (_old_marking_cycles_started == _old_marking_cycles_completed + 1),
1997          "for outer caller (concurrent cycle): "
1998          "_old_marking_cycles_started = %u "
1999          "is inconsistent with _old_marking_cycles_completed = %u",
2000          _old_marking_cycles_started, _old_marking_cycles_completed);
2001 
2002   _old_marking_cycles_completed += 1;
2003   if (whole_heap_examined) {
2004     // Signal that we have completed a visit to all live objects.
2005     record_whole_heap_examined_timestamp();
2006   }
2007 
2008   // We need to clear the "in_progress" flag in the CM thread before
2009   // we wake up any waiters (especially when ExplicitInvokesConcurrent
2010   // is set) so that if a waiter requests another System.gc() it doesn't
2011   // incorrectly see that a marking cycle is still in progress.
2012   if (concurrent) {
2013     _cm_thread-&gt;set_idle();
2014   }
2015 
2016   // Notify threads waiting in System.gc() (with ExplicitGCInvokesConcurrent)
2017   // for a full GC to finish that their wait is over.
2018   ml.notify_all();
2019 }
2020 
2021 void G1CollectedHeap::collect(GCCause::Cause cause) {
2022   try_collect(cause);
2023 }
2024 
2025 // Return true if (x &lt; y) with allowance for wraparound.
2026 static bool gc_counter_less_than(uint x, uint y) {
2027   return (x - y) &gt; (UINT_MAX/2);
2028 }
2029 
2030 // LOG_COLLECT_CONCURRENTLY(cause, msg, args...)
2031 // Macro so msg printing is format-checked.
2032 #define LOG_COLLECT_CONCURRENTLY(cause, ...)                            \
2033   do {                                                                  \
2034     LogTarget(Trace, gc) LOG_COLLECT_CONCURRENTLY_lt;                   \
2035     if (LOG_COLLECT_CONCURRENTLY_lt.is_enabled()) {                     \
2036       ResourceMark rm; /* For thread name. */                           \
2037       LogStream LOG_COLLECT_CONCURRENTLY_s(&amp;LOG_COLLECT_CONCURRENTLY_lt); \
2038       LOG_COLLECT_CONCURRENTLY_s.print("%s: Try Collect Concurrently (%s): ", \
2039                                        Thread::current()-&gt;name(),       \
2040                                        GCCause::to_string(cause));      \
2041       LOG_COLLECT_CONCURRENTLY_s.print(__VA_ARGS__);                    \
2042     }                                                                   \
2043   } while (0)
2044 
2045 #define LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, result) \
2046   LOG_COLLECT_CONCURRENTLY(cause, "complete %s", BOOL_TO_STR(result))
2047 
2048 bool G1CollectedHeap::try_collect_concurrently(GCCause::Cause cause,
2049                                                uint gc_counter,
2050                                                uint old_marking_started_before) {
2051   assert_heap_not_locked();
2052   assert(should_do_concurrent_full_gc(cause),
2053          "Non-concurrent cause %s", GCCause::to_string(cause));
2054 
2055   for (uint i = 1; true; ++i) {
2056     // Try to schedule concurrent start evacuation pause that will
2057     // start a concurrent cycle.
2058     LOG_COLLECT_CONCURRENTLY(cause, "attempt %u", i);
2059     VM_G1TryInitiateConcMark op(gc_counter,
2060                                 cause,
2061                                 policy()-&gt;max_pause_time_ms());
2062     VMThread::execute(&amp;op);
2063 
2064     // Request is trivially finished.
2065     if (cause == GCCause::_g1_periodic_collection) {
2066       LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, op.gc_succeeded());
2067       return op.gc_succeeded();
2068     }
2069 
2070     // If VMOp skipped initiating concurrent marking cycle because
2071     // we're terminating, then we're done.
2072     if (op.terminating()) {
2073       LOG_COLLECT_CONCURRENTLY(cause, "skipped: terminating");
2074       return false;
2075     }
2076 
2077     // Lock to get consistent set of values.
2078     uint old_marking_started_after;
2079     uint old_marking_completed_after;
2080     {
2081       MutexLocker ml(Heap_lock);
2082       // Update gc_counter for retrying VMOp if needed. Captured here to be
2083       // consistent with the values we use below for termination tests.  If
2084       // a retry is needed after a possible wait, and another collection
2085       // occurs in the meantime, it will cause our retry to be skipped and
2086       // we'll recheck for termination with updated conditions from that
2087       // more recent collection.  That's what we want, rather than having
2088       // our retry possibly perform an unnecessary collection.
2089       gc_counter = total_collections();
2090       old_marking_started_after = _old_marking_cycles_started;
2091       old_marking_completed_after = _old_marking_cycles_completed;
2092     }
2093 
2094     if (cause == GCCause::_wb_breakpoint) {
2095       if (op.gc_succeeded()) {
2096         LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);
2097         return true;
2098       }
2099       // When _wb_breakpoint there can't be another cycle or deferred.
2100       assert(!op.cycle_already_in_progress(), "invariant");
2101       assert(!op.whitebox_attached(), "invariant");
2102       // Concurrent cycle attempt might have been cancelled by some other
2103       // collection, so retry.  Unlike other cases below, we want to retry
2104       // even if cancelled by a STW full collection, because we really want
2105       // to start a concurrent cycle.
2106       if (old_marking_started_before != old_marking_started_after) {
2107         LOG_COLLECT_CONCURRENTLY(cause, "ignoring STW full GC");
2108         old_marking_started_before = old_marking_started_after;
2109       }
2110     } else if (!GCCause::is_user_requested_gc(cause)) {
2111       // For an "automatic" (not user-requested) collection, we just need to
2112       // ensure that progress is made.
2113       //
2114       // Request is finished if any of
2115       // (1) the VMOp successfully performed a GC,
2116       // (2) a concurrent cycle was already in progress,
2117       // (3) whitebox is controlling concurrent cycles,
2118       // (4) a new cycle was started (by this thread or some other), or
2119       // (5) a Full GC was performed.
2120       // Cases (4) and (5) are detected together by a change to
2121       // _old_marking_cycles_started.
2122       //
2123       // Note that (1) does not imply (4).  If we're still in the mixed
2124       // phase of an earlier concurrent collection, the request to make the
2125       // collection a concurrent start won't be honored.  If we don't check for
2126       // both conditions we'll spin doing back-to-back collections.
2127       if (op.gc_succeeded() ||
2128           op.cycle_already_in_progress() ||
2129           op.whitebox_attached() ||
2130           (old_marking_started_before != old_marking_started_after)) {
2131         LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);
2132         return true;
2133       }
2134     } else {                    // User-requested GC.
2135       // For a user-requested collection, we want to ensure that a complete
2136       // full collection has been performed before returning, but without
2137       // waiting for more than needed.
2138 
2139       // For user-requested GCs (unlike non-UR), a successful VMOp implies a
2140       // new cycle was started.  That's good, because it's not clear what we
2141       // should do otherwise.  Trying again just does back to back GCs.
2142       // Can't wait for someone else to start a cycle.  And returning fails
2143       // to meet the goal of ensuring a full collection was performed.
2144       assert(!op.gc_succeeded() ||
2145              (old_marking_started_before != old_marking_started_after),
2146              "invariant: succeeded %s, started before %u, started after %u",
2147              BOOL_TO_STR(op.gc_succeeded()),
2148              old_marking_started_before, old_marking_started_after);
2149 
2150       // Request is finished if a full collection (concurrent or stw)
2151       // was started after this request and has completed, e.g.
2152       // started_before &lt; completed_after.
2153       if (gc_counter_less_than(old_marking_started_before,
2154                                old_marking_completed_after)) {
2155         LOG_COLLECT_CONCURRENTLY_COMPLETE(cause, true);
2156         return true;
2157       }
2158 
2159       if (old_marking_started_after != old_marking_completed_after) {
2160         // If there is an in-progress cycle (possibly started by us), then
2161         // wait for that cycle to complete, e.g.
2162         // while completed_now &lt; started_after.
2163         LOG_COLLECT_CONCURRENTLY(cause, "wait");
2164         MonitorLocker ml(G1OldGCCount_lock);
2165         while (gc_counter_less_than(_old_marking_cycles_completed,
2166                                     old_marking_started_after)) {
2167           ml.wait();
2168         }
2169         // Request is finished if the collection we just waited for was
2170         // started after this request.
2171         if (old_marking_started_before != old_marking_started_after) {
2172           LOG_COLLECT_CONCURRENTLY(cause, "complete after wait");
2173           return true;
2174         }
2175       }
2176 
2177       // If VMOp was successful then it started a new cycle that the above
2178       // wait &amp;etc should have recognized as finishing this request.  This
2179       // differs from a non-user-request, where gc_succeeded does not imply
2180       // a new cycle was started.
2181       assert(!op.gc_succeeded(), "invariant");
2182 
2183       if (op.cycle_already_in_progress()) {
2184         // If VMOp failed because a cycle was already in progress, it
2185         // is now complete.  But it didn't finish this user-requested
2186         // GC, so try again.
2187         LOG_COLLECT_CONCURRENTLY(cause, "retry after in-progress");
2188         continue;
2189       } else if (op.whitebox_attached()) {
2190         // If WhiteBox wants control, wait for notification of a state
2191         // change in the controller, then try again.  Don't wait for
2192         // release of control, since collections may complete while in
2193         // control.  Note: This won't recognize a STW full collection
2194         // while waiting; we can't wait on multiple monitors.
2195         LOG_COLLECT_CONCURRENTLY(cause, "whitebox control stall");
2196         MonitorLocker ml(ConcurrentGCBreakpoints::monitor());
2197         if (ConcurrentGCBreakpoints::is_controlled()) {
2198           ml.wait();
2199         }
2200         continue;
2201       }
2202     }
2203 
2204     // Collection failed and should be retried.
2205     assert(op.transient_failure(), "invariant");
2206 
2207     if (GCLocker::is_active_and_needs_gc()) {
2208       // If GCLocker is active, wait until clear before retrying.
2209       LOG_COLLECT_CONCURRENTLY(cause, "gc-locker stall");
2210       GCLocker::stall_until_clear();
2211     }
2212 
2213     LOG_COLLECT_CONCURRENTLY(cause, "retry");
2214   }
2215 }
2216 
2217 bool G1CollectedHeap::try_collect(GCCause::Cause cause) {
2218   assert_heap_not_locked();
2219 
2220   // Lock to get consistent set of values.
2221   uint gc_count_before;
2222   uint full_gc_count_before;
2223   uint old_marking_started_before;
2224   {
2225     MutexLocker ml(Heap_lock);
2226     gc_count_before = total_collections();
2227     full_gc_count_before = total_full_collections();
2228     old_marking_started_before = _old_marking_cycles_started;
2229   }
2230 
2231   if (should_do_concurrent_full_gc(cause)) {
2232     return try_collect_concurrently(cause,
2233                                     gc_count_before,
2234                                     old_marking_started_before);
2235   } else if (GCLocker::should_discard(cause, gc_count_before)) {
2236     // Indicate failure to be consistent with VMOp failure due to
2237     // another collection slipping in after our gc_count but before
2238     // our request is processed.
2239     return false;
2240   } else if (cause == GCCause::_gc_locker || cause == GCCause::_wb_young_gc
2241              DEBUG_ONLY(|| cause == GCCause::_scavenge_alot)) {
2242 
2243     // Schedule a standard evacuation pause. We're setting word_size
2244     // to 0 which means that we are not requesting a post-GC allocation.
2245     VM_G1CollectForAllocation op(0,     /* word_size */
2246                                  gc_count_before,
2247                                  cause,
2248                                  policy()-&gt;max_pause_time_ms());
2249     VMThread::execute(&amp;op);
2250     return op.gc_succeeded();
2251   } else {
2252     // Schedule a Full GC.
2253     VM_G1CollectFull op(gc_count_before, full_gc_count_before, cause);
2254     VMThread::execute(&amp;op);
2255     return op.gc_succeeded();
2256   }
2257 }
2258 
2259 bool G1CollectedHeap::is_in(const void* p) const {
2260   if (_hrm-&gt;reserved().contains(p)) {
2261     // Given that we know that p is in the reserved space,
2262     // heap_region_containing() should successfully
2263     // return the containing region.
2264     HeapRegion* hr = heap_region_containing(p);
2265     return hr-&gt;is_in(p);
2266   } else {
2267     return false;
2268   }
2269 }
2270 
2271 #ifdef ASSERT
2272 bool G1CollectedHeap::is_in_exact(const void* p) const {
2273   bool contains = reserved_region().contains(p);
2274   bool available = _hrm-&gt;is_available(addr_to_region((HeapWord*)p));
2275   if (contains &amp;&amp; available) {
2276     return true;
2277   } else {
2278     return false;
2279   }
2280 }
2281 #endif
2282 
2283 // Iteration functions.
2284 
2285 // Iterates an ObjectClosure over all objects within a HeapRegion.
2286 
2287 class IterateObjectClosureRegionClosure: public HeapRegionClosure {
2288   ObjectClosure* _cl;
2289 public:
2290   IterateObjectClosureRegionClosure(ObjectClosure* cl) : _cl(cl) {}
2291   bool do_heap_region(HeapRegion* r) {
2292     if (!r-&gt;is_continues_humongous()) {
2293       r-&gt;object_iterate(_cl);
2294     }
2295     return false;
2296   }
2297 };
2298 
2299 void G1CollectedHeap::object_iterate(ObjectClosure* cl) {
2300   IterateObjectClosureRegionClosure blk(cl);
2301   heap_region_iterate(&amp;blk);
2302 }
2303 
2304 void G1CollectedHeap::keep_alive(oop obj) {
2305   G1BarrierSet::enqueue(obj);
2306 }
2307 
2308 void G1CollectedHeap::heap_region_iterate(HeapRegionClosure* cl) const {
2309   _hrm-&gt;iterate(cl);
2310 }
2311 
2312 void G1CollectedHeap::heap_region_par_iterate_from_worker_offset(HeapRegionClosure* cl,
2313                                                                  HeapRegionClaimer *hrclaimer,
2314                                                                  uint worker_id) const {
2315   _hrm-&gt;par_iterate(cl, hrclaimer, hrclaimer-&gt;offset_for_worker(worker_id));
2316 }
2317 
2318 void G1CollectedHeap::heap_region_par_iterate_from_start(HeapRegionClosure* cl,
2319                                                          HeapRegionClaimer *hrclaimer) const {
2320   _hrm-&gt;par_iterate(cl, hrclaimer, 0);
2321 }
2322 
2323 void G1CollectedHeap::collection_set_iterate_all(HeapRegionClosure* cl) {
2324   _collection_set.iterate(cl);
2325 }
2326 
2327 void G1CollectedHeap::collection_set_par_iterate_all(HeapRegionClosure* cl, HeapRegionClaimer* hr_claimer, uint worker_id) {
2328   _collection_set.par_iterate(cl, hr_claimer, worker_id, workers()-&gt;active_workers());
2329 }
2330 
2331 void G1CollectedHeap::collection_set_iterate_increment_from(HeapRegionClosure *cl, HeapRegionClaimer* hr_claimer, uint worker_id) {
2332   _collection_set.iterate_incremental_part_from(cl, hr_claimer, worker_id, workers()-&gt;active_workers());
2333 }
2334 
2335 HeapWord* G1CollectedHeap::block_start(const void* addr) const {
2336   HeapRegion* hr = heap_region_containing(addr);
2337   return hr-&gt;block_start(addr);
2338 }
2339 
2340 bool G1CollectedHeap::block_is_obj(const HeapWord* addr) const {
2341   HeapRegion* hr = heap_region_containing(addr);
2342   return hr-&gt;block_is_obj(addr);
2343 }
2344 
2345 bool G1CollectedHeap::supports_tlab_allocation() const {
2346   return true;
2347 }
2348 
2349 size_t G1CollectedHeap::tlab_capacity(Thread* ignored) const {
2350   return (_policy-&gt;young_list_target_length() - _survivor.length()) * HeapRegion::GrainBytes;
2351 }
2352 
2353 size_t G1CollectedHeap::tlab_used(Thread* ignored) const {
2354   return _eden.length() * HeapRegion::GrainBytes;
2355 }
2356 
2357 // For G1 TLABs should not contain humongous objects, so the maximum TLAB size
2358 // must be equal to the humongous object limit.
2359 size_t G1CollectedHeap::max_tlab_size() const {
2360   return align_down(_humongous_object_threshold_in_words, MinObjAlignment);
2361 }
2362 
2363 size_t G1CollectedHeap::unsafe_max_tlab_alloc(Thread* ignored) const {
2364   return _allocator-&gt;unsafe_max_tlab_alloc();
2365 }
2366 
2367 size_t G1CollectedHeap::max_capacity() const {
2368   return _hrm-&gt;max_expandable_length() * HeapRegion::GrainBytes;
2369 }
2370 
2371 size_t G1CollectedHeap::max_reserved_capacity() const {
2372   return _hrm-&gt;max_length() * HeapRegion::GrainBytes;
2373 }
2374 
2375 void G1CollectedHeap::deduplicate_string(oop str) {
2376   assert(java_lang_String::is_instance(str), "invariant");
2377 
2378   if (G1StringDedup::is_enabled()) {
2379     G1StringDedup::deduplicate(str);
2380   }
2381 }
2382 
2383 void G1CollectedHeap::prepare_for_verify() {
2384   _verifier-&gt;prepare_for_verify();
2385 }
2386 
2387 void G1CollectedHeap::verify(VerifyOption vo) {
2388   _verifier-&gt;verify(vo);
2389 }
2390 
2391 bool G1CollectedHeap::supports_concurrent_gc_breakpoints() const {
2392   return true;
2393 }
2394 
2395 bool G1CollectedHeap::is_heterogeneous_heap() const {
2396   return G1Arguments::is_heterogeneous_heap();
2397 }
2398 
2399 class PrintRegionClosure: public HeapRegionClosure {
2400   outputStream* _st;
2401 public:
2402   PrintRegionClosure(outputStream* st) : _st(st) {}
2403   bool do_heap_region(HeapRegion* r) {
2404     r-&gt;print_on(_st);
2405     return false;
2406   }
2407 };
2408 
2409 bool G1CollectedHeap::is_obj_dead_cond(const oop obj,
2410                                        const HeapRegion* hr,
2411                                        const VerifyOption vo) const {
2412   switch (vo) {
2413   case VerifyOption_G1UsePrevMarking: return is_obj_dead(obj, hr);
2414   case VerifyOption_G1UseNextMarking: return is_obj_ill(obj, hr);
2415   case VerifyOption_G1UseFullMarking: return is_obj_dead_full(obj, hr);
2416   default:                            ShouldNotReachHere();
2417   }
2418   return false; // keep some compilers happy
2419 }
2420 
2421 bool G1CollectedHeap::is_obj_dead_cond(const oop obj,
2422                                        const VerifyOption vo) const {
2423   switch (vo) {
2424   case VerifyOption_G1UsePrevMarking: return is_obj_dead(obj);
2425   case VerifyOption_G1UseNextMarking: return is_obj_ill(obj);
2426   case VerifyOption_G1UseFullMarking: return is_obj_dead_full(obj);
2427   default:                            ShouldNotReachHere();
2428   }
2429   return false; // keep some compilers happy
2430 }
2431 
2432 void G1CollectedHeap::print_heap_regions() const {
2433   LogTarget(Trace, gc, heap, region) lt;
2434   if (lt.is_enabled()) {
2435     LogStream ls(lt);
2436     print_regions_on(&amp;ls);
2437   }
2438 }
2439 
2440 void G1CollectedHeap::print_on(outputStream* st) const {
2441   st-&gt;print(" %-20s", "garbage-first heap");
2442   if (_hrm != NULL) {
2443     st-&gt;print(" total " SIZE_FORMAT "K, used " SIZE_FORMAT "K",
2444               capacity()/K, used_unlocked()/K);
2445     st-&gt;print(" [" PTR_FORMAT ", " PTR_FORMAT ")",
2446               p2i(_hrm-&gt;reserved().start()),
2447               p2i(_hrm-&gt;reserved().end()));
2448   }
2449   st-&gt;cr();
2450   st-&gt;print("  region size " SIZE_FORMAT "K, ", HeapRegion::GrainBytes / K);
2451   uint young_regions = young_regions_count();
2452   st-&gt;print("%u young (" SIZE_FORMAT "K), ", young_regions,
2453             (size_t) young_regions * HeapRegion::GrainBytes / K);
2454   uint survivor_regions = survivor_regions_count();
2455   st-&gt;print("%u survivors (" SIZE_FORMAT "K)", survivor_regions,
2456             (size_t) survivor_regions * HeapRegion::GrainBytes / K);
2457   st-&gt;cr();
2458   if (_numa-&gt;is_enabled()) {
2459     uint num_nodes = _numa-&gt;num_active_nodes();
2460     st-&gt;print("  remaining free region(s) on each NUMA node: ");
2461     const int* node_ids = _numa-&gt;node_ids();
2462     for (uint node_index = 0; node_index &lt; num_nodes; node_index++) {
2463       uint num_free_regions = (_hrm != NULL ? _hrm-&gt;num_free_regions(node_index) : 0);
2464       st-&gt;print("%d=%u ", node_ids[node_index], num_free_regions);
2465     }
2466     st-&gt;cr();
2467   }
2468   MetaspaceUtils::print_on(st);
2469 }
2470 
2471 void G1CollectedHeap::print_regions_on(outputStream* st) const {
2472   if (_hrm == NULL) {
2473     return;
2474   }
2475 
2476   st-&gt;print_cr("Heap Regions: E=young(eden), S=young(survivor), O=old, "
2477                "HS=humongous(starts), HC=humongous(continues), "
2478                "CS=collection set, F=free, "
2479                "OA=open archive, CA=closed archive, "
2480                "TAMS=top-at-mark-start (previous, next)");
2481   PrintRegionClosure blk(st);
2482   heap_region_iterate(&amp;blk);
2483 }
2484 
2485 void G1CollectedHeap::print_extended_on(outputStream* st) const {
2486   print_on(st);
2487 
2488   // Print the per-region information.
2489   if (_hrm != NULL) {
2490     st-&gt;cr();
2491     print_regions_on(st);
2492   }
2493 }
2494 
2495 void G1CollectedHeap::print_on_error(outputStream* st) const {
2496   this-&gt;CollectedHeap::print_on_error(st);
2497 
2498   if (_cm != NULL) {
2499     st-&gt;cr();
2500     _cm-&gt;print_on_error(st);
2501   }
2502 }
2503 
2504 void G1CollectedHeap::gc_threads_do(ThreadClosure* tc) const {
2505   workers()-&gt;threads_do(tc);
2506   tc-&gt;do_thread(_cm_thread);
2507   _cm-&gt;threads_do(tc);
2508   _cr-&gt;threads_do(tc);
2509   tc-&gt;do_thread(_young_gen_sampling_thread);
2510   if (G1StringDedup::is_enabled()) {
2511     G1StringDedup::threads_do(tc);
2512   }
2513 }
2514 
2515 void G1CollectedHeap::print_tracing_info() const {
2516   rem_set()-&gt;print_summary_info();
2517   concurrent_mark()-&gt;print_summary_info();
2518 }
2519 
2520 #ifndef PRODUCT
2521 // Helpful for debugging RSet issues.
2522 
2523 class PrintRSetsClosure : public HeapRegionClosure {
2524 private:
2525   const char* _msg;
2526   size_t _occupied_sum;
2527 
2528 public:
2529   bool do_heap_region(HeapRegion* r) {
2530     HeapRegionRemSet* hrrs = r-&gt;rem_set();
2531     size_t occupied = hrrs-&gt;occupied();
2532     _occupied_sum += occupied;
2533 
2534     tty-&gt;print_cr("Printing RSet for region " HR_FORMAT, HR_FORMAT_PARAMS(r));
2535     if (occupied == 0) {
2536       tty-&gt;print_cr("  RSet is empty");
2537     } else {
2538       hrrs-&gt;print();
2539     }
2540     tty-&gt;print_cr("----------");
2541     return false;
2542   }
2543 
2544   PrintRSetsClosure(const char* msg) : _msg(msg), _occupied_sum(0) {
2545     tty-&gt;cr();
2546     tty-&gt;print_cr("========================================");
2547     tty-&gt;print_cr("%s", msg);
2548     tty-&gt;cr();
2549   }
2550 
2551   ~PrintRSetsClosure() {
2552     tty-&gt;print_cr("Occupied Sum: " SIZE_FORMAT, _occupied_sum);
2553     tty-&gt;print_cr("========================================");
2554     tty-&gt;cr();
2555   }
2556 };
2557 
2558 void G1CollectedHeap::print_cset_rsets() {
2559   PrintRSetsClosure cl("Printing CSet RSets");
2560   collection_set_iterate_all(&amp;cl);
2561 }
2562 
2563 void G1CollectedHeap::print_all_rsets() {
2564   PrintRSetsClosure cl("Printing All RSets");;
2565   heap_region_iterate(&amp;cl);
2566 }
2567 #endif // PRODUCT
2568 
2569 bool G1CollectedHeap::print_location(outputStream* st, void* addr) const {
2570   return BlockLocationPrinter&lt;G1CollectedHeap&gt;::print_location(st, addr);
2571 }
2572 
2573 G1HeapSummary G1CollectedHeap::create_g1_heap_summary() {
2574 
2575   size_t eden_used_bytes = _eden.used_bytes();
2576   size_t survivor_used_bytes = _survivor.used_bytes();
2577   size_t heap_used = Heap_lock-&gt;owned_by_self() ? used() : used_unlocked();
2578 
2579   size_t eden_capacity_bytes =
2580     (policy()-&gt;young_list_target_length() * HeapRegion::GrainBytes) - survivor_used_bytes;
2581 
2582   VirtualSpaceSummary heap_summary = create_heap_space_summary();
2583   return G1HeapSummary(heap_summary, heap_used, eden_used_bytes,
2584                        eden_capacity_bytes, survivor_used_bytes, num_regions());
2585 }
2586 
2587 G1EvacSummary G1CollectedHeap::create_g1_evac_summary(G1EvacStats* stats) {
2588   return G1EvacSummary(stats-&gt;allocated(), stats-&gt;wasted(), stats-&gt;undo_wasted(),
2589                        stats-&gt;unused(), stats-&gt;used(), stats-&gt;region_end_waste(),
2590                        stats-&gt;regions_filled(), stats-&gt;direct_allocated(),
2591                        stats-&gt;failure_used(), stats-&gt;failure_waste());
2592 }
2593 
2594 void G1CollectedHeap::trace_heap(GCWhen::Type when, const GCTracer* gc_tracer) {
2595   const G1HeapSummary&amp; heap_summary = create_g1_heap_summary();
2596   gc_tracer-&gt;report_gc_heap_summary(when, heap_summary);
2597 
2598   const MetaspaceSummary&amp; metaspace_summary = create_metaspace_summary();
2599   gc_tracer-&gt;report_metaspace_summary(when, metaspace_summary);
2600 }
2601 
2602 void G1CollectedHeap::gc_prologue(bool full) {
2603   assert(InlineCacheBuffer::is_empty(), "should have cleaned up ICBuffer");
2604 
2605   // This summary needs to be printed before incrementing total collections.
2606   rem_set()-&gt;print_periodic_summary_info("Before GC RS summary", total_collections());
2607 
2608   // Update common counters.
2609   increment_total_collections(full /* full gc */);
2610   if (full || collector_state()-&gt;in_concurrent_start_gc()) {
2611     increment_old_marking_cycles_started();
2612   }
2613 
2614   // Fill TLAB's and such
2615   {
2616     Ticks start = Ticks::now();
2617     ensure_parsability(true);
2618     Tickspan dt = Ticks::now() - start;
2619     phase_times()-&gt;record_prepare_tlab_time_ms(dt.seconds() * MILLIUNITS);
2620   }
2621 
2622   if (!full) {
2623     // Flush dirty card queues to qset, so later phases don't need to account
2624     // for partially filled per-thread queues and such.  Not needed for full
2625     // collections, which ignore those logs.
2626     Ticks start = Ticks::now();
2627     G1BarrierSet::dirty_card_queue_set().concatenate_logs();
2628     Tickspan dt = Ticks::now() - start;
2629     phase_times()-&gt;record_concatenate_dirty_card_logs_time_ms(dt.seconds() * MILLIUNITS);
2630   }
2631 }
2632 
2633 void G1CollectedHeap::gc_epilogue(bool full) {
2634   // Update common counters.
2635   if (full) {
2636     // Update the number of full collections that have been completed.
2637     increment_old_marking_cycles_completed(false /* concurrent */, true /* liveness_completed */);
2638   }
2639 
2640   // We are at the end of the GC. Total collections has already been increased.
2641   rem_set()-&gt;print_periodic_summary_info("After GC RS summary", total_collections() - 1);
2642 
2643   // FIXME: what is this about?
2644   // I'm ignoring the "fill_newgen()" call if "alloc_event_enabled"
2645   // is set.
2646 #if COMPILER2_OR_JVMCI
2647   assert(DerivedPointerTable::is_empty(), "derived pointer present");
2648 #endif
2649 
2650   double start = os::elapsedTime();
2651   resize_all_tlabs();
2652   phase_times()-&gt;record_resize_tlab_time_ms((os::elapsedTime() - start) * 1000.0);
2653 
2654   MemoryService::track_memory_usage();
2655   // We have just completed a GC. Update the soft reference
2656   // policy with the new heap occupancy
2657   Universe::update_heap_info_at_gc();
2658 
2659   // Print NUMA statistics.
2660   _numa-&gt;print_statistics();
2661 
2662   _collection_pause_end = Ticks::now();
2663 }
2664 
2665 void G1CollectedHeap::verify_numa_regions(const char* desc) {
2666   LogTarget(Trace, gc, heap, verify) lt;
2667 
2668   if (lt.is_enabled()) {
2669     LogStream ls(lt);
2670     // Iterate all heap regions to print matching between preferred numa id and actual numa id.
2671     G1NodeIndexCheckClosure cl(desc, _numa, &amp;ls);
2672     heap_region_iterate(&amp;cl);
2673   }
2674 }
2675 
2676 HeapWord* G1CollectedHeap::do_collection_pause(size_t word_size,
2677                                                uint gc_count_before,
2678                                                bool* succeeded,
2679                                                GCCause::Cause gc_cause) {
2680   assert_heap_not_locked_and_not_at_safepoint();
2681   VM_G1CollectForAllocation op(word_size,
2682                                gc_count_before,
2683                                gc_cause,
2684                                policy()-&gt;max_pause_time_ms());
2685   VMThread::execute(&amp;op);
2686 
2687   HeapWord* result = op.result();
2688   bool ret_succeeded = op.prologue_succeeded() &amp;&amp; op.gc_succeeded();
2689   assert(result == NULL || ret_succeeded,
2690          "the result should be NULL if the VM did not succeed");
2691   *succeeded = ret_succeeded;
2692 
2693   assert_heap_not_locked();
2694   return result;
2695 }
2696 
2697 void G1CollectedHeap::do_concurrent_mark() {
2698   MutexLocker x(CGC_lock, Mutex::_no_safepoint_check_flag);
2699   if (!_cm_thread-&gt;in_progress()) {
2700     _cm_thread-&gt;set_started();
2701     CGC_lock-&gt;notify();
2702   }
2703 }
2704 
2705 bool G1CollectedHeap::is_potential_eager_reclaim_candidate(HeapRegion* r) const {
2706   // We don't nominate objects with many remembered set entries, on
2707   // the assumption that such objects are likely still live.
2708   HeapRegionRemSet* rem_set = r-&gt;rem_set();
2709 
2710   return G1EagerReclaimHumongousObjectsWithStaleRefs ?
2711          rem_set-&gt;occupancy_less_or_equal_than(G1RSetSparseRegionEntries) :
2712          G1EagerReclaimHumongousObjects &amp;&amp; rem_set-&gt;is_empty();
2713 }
2714 
2715 #ifndef PRODUCT
2716 void G1CollectedHeap::verify_region_attr_remset_update() {
2717   class VerifyRegionAttrRemSet : public HeapRegionClosure {
2718   public:
2719     virtual bool do_heap_region(HeapRegion* r) {
2720       G1CollectedHeap* g1h = G1CollectedHeap::heap();
2721       bool const needs_remset_update = g1h-&gt;region_attr(r-&gt;bottom()).needs_remset_update();
2722       assert(r-&gt;rem_set()-&gt;is_tracked() == needs_remset_update,
2723              "Region %u remset tracking status (%s) different to region attribute (%s)",
2724              r-&gt;hrm_index(), BOOL_TO_STR(r-&gt;rem_set()-&gt;is_tracked()), BOOL_TO_STR(needs_remset_update));
2725       return false;
2726     }
2727   } cl;
2728   heap_region_iterate(&amp;cl);
2729 }
2730 #endif
2731 
2732 class VerifyRegionRemSetClosure : public HeapRegionClosure {
2733   public:
2734     bool do_heap_region(HeapRegion* hr) {
2735       if (!hr-&gt;is_archive() &amp;&amp; !hr-&gt;is_continues_humongous()) {
2736         hr-&gt;verify_rem_set();
2737       }
2738       return false;
2739     }
2740 };
2741 
2742 uint G1CollectedHeap::num_task_queues() const {
2743   return _task_queues-&gt;size();
2744 }
2745 
2746 #if TASKQUEUE_STATS
2747 void G1CollectedHeap::print_taskqueue_stats_hdr(outputStream* const st) {
2748   st-&gt;print_raw_cr("GC Task Stats");
2749   st-&gt;print_raw("thr "); TaskQueueStats::print_header(1, st); st-&gt;cr();
2750   st-&gt;print_raw("--- "); TaskQueueStats::print_header(2, st); st-&gt;cr();
2751 }
2752 
2753 void G1CollectedHeap::print_taskqueue_stats() const {
2754   if (!log_is_enabled(Trace, gc, task, stats)) {
2755     return;
2756   }
2757   Log(gc, task, stats) log;
2758   ResourceMark rm;
2759   LogStream ls(log.trace());
2760   outputStream* st = &amp;ls;
2761 
2762   print_taskqueue_stats_hdr(st);
2763 
2764   TaskQueueStats totals;
2765   const uint n = num_task_queues();
2766   for (uint i = 0; i &lt; n; ++i) {
2767     st-&gt;print("%3u ", i); task_queue(i)-&gt;stats.print(st); st-&gt;cr();
2768     totals += task_queue(i)-&gt;stats;
2769   }
2770   st-&gt;print_raw("tot "); totals.print(st); st-&gt;cr();
2771 
2772   DEBUG_ONLY(totals.verify());
2773 }
2774 
2775 void G1CollectedHeap::reset_taskqueue_stats() {
2776   const uint n = num_task_queues();
2777   for (uint i = 0; i &lt; n; ++i) {
2778     task_queue(i)-&gt;stats.reset();
2779   }
2780 }
2781 #endif // TASKQUEUE_STATS
2782 
2783 void G1CollectedHeap::wait_for_root_region_scanning() {
2784   double scan_wait_start = os::elapsedTime();
2785   // We have to wait until the CM threads finish scanning the
2786   // root regions as it's the only way to ensure that all the
2787   // objects on them have been correctly scanned before we start
2788   // moving them during the GC.
2789   bool waited = _cm-&gt;root_regions()-&gt;wait_until_scan_finished();
2790   double wait_time_ms = 0.0;
2791   if (waited) {
2792     double scan_wait_end = os::elapsedTime();
2793     wait_time_ms = (scan_wait_end - scan_wait_start) * 1000.0;
2794   }
2795   phase_times()-&gt;record_root_region_scan_wait_time(wait_time_ms);
2796 }
2797 
2798 class G1PrintCollectionSetClosure : public HeapRegionClosure {
2799 private:
2800   G1HRPrinter* _hr_printer;
2801 public:
2802   G1PrintCollectionSetClosure(G1HRPrinter* hr_printer) : HeapRegionClosure(), _hr_printer(hr_printer) { }
2803 
2804   virtual bool do_heap_region(HeapRegion* r) {
2805     _hr_printer-&gt;cset(r);
2806     return false;
2807   }
2808 };
2809 
2810 void G1CollectedHeap::start_new_collection_set() {
2811   double start = os::elapsedTime();
2812 
2813   collection_set()-&gt;start_incremental_building();
2814 
2815   clear_region_attr();
2816 
2817   guarantee(_eden.length() == 0, "eden should have been cleared");
2818   policy()-&gt;transfer_survivors_to_cset(survivor());
2819 
2820   // We redo the verification but now wrt to the new CSet which
2821   // has just got initialized after the previous CSet was freed.
2822   _cm-&gt;verify_no_collection_set_oops();
2823 
2824   phase_times()-&gt;record_start_new_cset_time_ms((os::elapsedTime() - start) * 1000.0);
2825 }
2826 
2827 void G1CollectedHeap::calculate_collection_set(G1EvacuationInfo&amp; evacuation_info, double target_pause_time_ms) {
2828 
2829   _collection_set.finalize_initial_collection_set(target_pause_time_ms, &amp;_survivor);
2830   evacuation_info.set_collectionset_regions(collection_set()-&gt;region_length() +
2831                                             collection_set()-&gt;optional_region_length());
2832 
2833   _cm-&gt;verify_no_collection_set_oops();
2834 
2835   if (_hr_printer.is_active()) {
2836     G1PrintCollectionSetClosure cl(&amp;_hr_printer);
2837     _collection_set.iterate(&amp;cl);
2838     _collection_set.iterate_optional(&amp;cl);
2839   }
2840 }
2841 
2842 G1HeapVerifier::G1VerifyType G1CollectedHeap::young_collection_verify_type() const {
2843   if (collector_state()-&gt;in_concurrent_start_gc()) {
2844     return G1HeapVerifier::G1VerifyConcurrentStart;
2845   } else if (collector_state()-&gt;in_young_only_phase()) {
2846     return G1HeapVerifier::G1VerifyYoungNormal;
2847   } else {
2848     return G1HeapVerifier::G1VerifyMixed;
2849   }
2850 }
2851 
2852 void G1CollectedHeap::verify_before_young_collection(G1HeapVerifier::G1VerifyType type) {
2853   if (VerifyRememberedSets) {
2854     log_info(gc, verify)("[Verifying RemSets before GC]");
2855     VerifyRegionRemSetClosure v_cl;
2856     heap_region_iterate(&amp;v_cl);
2857   }
2858   _verifier-&gt;verify_before_gc(type);
2859   _verifier-&gt;check_bitmaps("GC Start");
2860   verify_numa_regions("GC Start");
2861 }
2862 
2863 void G1CollectedHeap::verify_after_young_collection(G1HeapVerifier::G1VerifyType type) {
2864   if (VerifyRememberedSets) {
2865     log_info(gc, verify)("[Verifying RemSets after GC]");
2866     VerifyRegionRemSetClosure v_cl;
2867     heap_region_iterate(&amp;v_cl);
2868   }
2869   _verifier-&gt;verify_after_gc(type);
2870   _verifier-&gt;check_bitmaps("GC End");
2871   verify_numa_regions("GC End");
2872 }
2873 
2874 void G1CollectedHeap::expand_heap_after_young_collection(){
2875   size_t expand_bytes = _heap_sizing_policy-&gt;young_collection_expansion_amount();
2876   if (expand_bytes &gt; 0) {
2877     // No need for an ergo logging here,
2878     // expansion_amount() does this when it returns a value &gt; 0.
2879     double expand_ms;
2880     if (!expand(expand_bytes, _workers, &amp;expand_ms)) {
2881       // We failed to expand the heap. Cannot do anything about it.
2882     }
2883     phase_times()-&gt;record_expand_heap_time(expand_ms);
2884   }
2885 }
2886 
2887 const char* G1CollectedHeap::young_gc_name() const {
2888   if (collector_state()-&gt;in_concurrent_start_gc()) {
2889     return "Pause Young (Concurrent Start)";
2890   } else if (collector_state()-&gt;in_young_only_phase()) {
2891     if (collector_state()-&gt;in_young_gc_before_mixed()) {
2892       return "Pause Young (Prepare Mixed)";
2893     } else {
2894       return "Pause Young (Normal)";
2895     }
2896   } else {
2897     return "Pause Young (Mixed)";
2898   }
2899 }
2900 
2901 bool G1CollectedHeap::do_collection_pause_at_safepoint(double target_pause_time_ms) {
2902   assert_at_safepoint_on_vm_thread();
2903   guarantee(!is_gc_active(), "collection is not reentrant");
2904 
2905   if (GCLocker::check_active_before_gc()) {
2906     return false;
2907   }
2908 
2909   do_collection_pause_at_safepoint_helper(target_pause_time_ms);
2910   if (should_upgrade_to_full_gc(gc_cause())) {
2911     log_info(gc, ergo)("Attempting maximally compacting collection");
2912     bool result = do_full_collection(false /* explicit gc */,
2913                                      true /* clear_all_soft_refs */);
2914     // do_full_collection only fails if blocked by GC locker, but
2915     // we've already checked for that above.
2916     assert(result, "invariant");
2917   }
2918   return true;
2919 }
2920 
2921 void G1CollectedHeap::do_collection_pause_at_safepoint_helper(double target_pause_time_ms) {
2922   GCIdMark gc_id_mark;
2923 
2924   SvcGCMarker sgcm(SvcGCMarker::MINOR);
2925   ResourceMark rm;
2926 
2927   policy()-&gt;note_gc_start();
2928 
2929   _gc_timer_stw-&gt;register_gc_start();
2930   _gc_tracer_stw-&gt;report_gc_start(gc_cause(), _gc_timer_stw-&gt;gc_start());
2931 
2932   wait_for_root_region_scanning();
2933 
2934   print_heap_before_gc();
2935   print_heap_regions();
2936   trace_heap_before_gc(_gc_tracer_stw);
2937 
2938   _verifier-&gt;verify_region_sets_optional();
2939   _verifier-&gt;verify_dirty_young_regions();
2940 
2941   // We should not be doing concurrent start unless the concurrent mark thread is running
2942   if (!_cm_thread-&gt;should_terminate()) {
2943     // This call will decide whether this pause is a concurrent start
2944     // pause. If it is, in_concurrent_start_gc() will return true
2945     // for the duration of this pause.
2946     policy()-&gt;decide_on_conc_mark_initiation();
2947   }
2948 
2949   // We do not allow concurrent start to be piggy-backed on a mixed GC.
2950   assert(!collector_state()-&gt;in_concurrent_start_gc() ||
2951          collector_state()-&gt;in_young_only_phase(), "sanity");
2952   // We also do not allow mixed GCs during marking.
2953   assert(!collector_state()-&gt;mark_or_rebuild_in_progress() || collector_state()-&gt;in_young_only_phase(), "sanity");
2954 
2955   // Record whether this pause is a concurrent start. When the current
2956   // thread has completed its logging output and it's safe to signal
2957   // the CM thread, the flag's value in the policy has been reset.
2958   bool should_start_conc_mark = collector_state()-&gt;in_concurrent_start_gc();
2959   if (should_start_conc_mark) {
2960     _cm-&gt;gc_tracer_cm()-&gt;set_gc_cause(gc_cause());
2961   }
2962 
2963   // Inner scope for scope based logging, timers, and stats collection
2964   {
2965     G1EvacuationInfo evacuation_info;
2966 
2967     _gc_tracer_stw-&gt;report_yc_type(collector_state()-&gt;yc_type());
2968 
2969     GCTraceCPUTime tcpu;
2970 
2971     GCTraceTime(Info, gc) tm(young_gc_name(), NULL, gc_cause(), true);
2972 
2973     uint active_workers = WorkerPolicy::calc_active_workers(workers()-&gt;total_workers(),
2974                                                             workers()-&gt;active_workers(),
2975                                                             Threads::number_of_non_daemon_threads());
2976     active_workers = workers()-&gt;update_active_workers(active_workers);
2977     log_info(gc,task)("Using %u workers of %u for evacuation", active_workers, workers()-&gt;total_workers());
2978 
2979     G1MonitoringScope ms(g1mm(),
2980                          false /* full_gc */,
2981                          collector_state()-&gt;yc_type() == Mixed /* all_memory_pools_affected */);
2982 
2983     G1HeapTransition heap_transition(this);
2984 
2985     {
2986       IsGCActiveMark x;
2987 
2988       gc_prologue(false);
2989 
2990       G1HeapVerifier::G1VerifyType verify_type = young_collection_verify_type();
2991       verify_before_young_collection(verify_type);
2992 
2993       {
2994         // The elapsed time induced by the start time below deliberately elides
2995         // the possible verification above.
2996         double sample_start_time_sec = os::elapsedTime();
2997 
2998         // Please see comment in g1CollectedHeap.hpp and
2999         // G1CollectedHeap::ref_processing_init() to see how
3000         // reference processing currently works in G1.
3001         _ref_processor_stw-&gt;enable_discovery();
3002 
3003         // We want to temporarily turn off discovery by the
3004         // CM ref processor, if necessary, and turn it back on
3005         // on again later if we do. Using a scoped
3006         // NoRefDiscovery object will do this.
3007         NoRefDiscovery no_cm_discovery(_ref_processor_cm);
3008 
3009         policy()-&gt;record_collection_pause_start(sample_start_time_sec);
3010 
3011         // Forget the current allocation region (we might even choose it to be part
3012         // of the collection set!).
3013         _allocator-&gt;release_mutator_alloc_regions();
3014 
3015         calculate_collection_set(evacuation_info, target_pause_time_ms);
3016 
3017         G1RedirtyCardsQueueSet rdcqs(G1BarrierSet::dirty_card_queue_set().allocator());
3018         G1ParScanThreadStateSet per_thread_states(this,
3019                                                   &amp;rdcqs,
3020                                                   workers()-&gt;active_workers(),
3021                                                   collection_set()-&gt;young_region_length(),
3022                                                   collection_set()-&gt;optional_region_length());
3023         pre_evacuate_collection_set(evacuation_info, &amp;per_thread_states);
3024 
3025         // Actually do the work...
3026         evacuate_initial_collection_set(&amp;per_thread_states);
3027 
3028         if (_collection_set.optional_region_length() != 0) {
3029           evacuate_optional_collection_set(&amp;per_thread_states);
3030         }
3031         post_evacuate_collection_set(evacuation_info, &amp;rdcqs, &amp;per_thread_states);
3032 
3033         start_new_collection_set();
3034 
3035         _survivor_evac_stats.adjust_desired_plab_sz();
3036         _old_evac_stats.adjust_desired_plab_sz();
3037 
3038         if (should_start_conc_mark) {
3039           // We have to do this before we notify the CM threads that
3040           // they can start working to make sure that all the
3041           // appropriate initialization is done on the CM object.
3042           concurrent_mark()-&gt;post_concurrent_start();
3043           // Note that we don't actually trigger the CM thread at
3044           // this point. We do that later when we're sure that
3045           // the current thread has completed its logging output.
3046         }
3047 
3048         allocate_dummy_regions();
3049 
3050         _allocator-&gt;init_mutator_alloc_regions();
3051 
3052         expand_heap_after_young_collection();
3053 
3054         double sample_end_time_sec = os::elapsedTime();
3055         double pause_time_ms = (sample_end_time_sec - sample_start_time_sec) * MILLIUNITS;
3056         policy()-&gt;record_collection_pause_end(pause_time_ms);
3057       }
3058 
3059       verify_after_young_collection(verify_type);
3060 
3061       gc_epilogue(false);
3062     }
3063 
3064     // Print the remainder of the GC log output.
3065     if (evacuation_failed()) {
3066       log_info(gc)("To-space exhausted");
3067     }
3068 
3069     policy()-&gt;print_phases();
3070     heap_transition.print();
3071 
3072     _hrm-&gt;verify_optional();
3073     _verifier-&gt;verify_region_sets_optional();
3074 
3075     TASKQUEUE_STATS_ONLY(print_taskqueue_stats());
3076     TASKQUEUE_STATS_ONLY(reset_taskqueue_stats());
3077 
3078     print_heap_after_gc();
3079     print_heap_regions();
3080     trace_heap_after_gc(_gc_tracer_stw);
3081 
3082     // We must call G1MonitoringSupport::update_sizes() in the same scoping level
3083     // as an active TraceMemoryManagerStats object (i.e. before the destructor for the
3084     // TraceMemoryManagerStats is called) so that the G1 memory pools are updated
3085     // before any GC notifications are raised.
3086     g1mm()-&gt;update_sizes();
3087 
3088     _gc_tracer_stw-&gt;report_evacuation_info(&amp;evacuation_info);
3089     _gc_tracer_stw-&gt;report_tenuring_threshold(_policy-&gt;tenuring_threshold());
3090     _gc_timer_stw-&gt;register_gc_end();
3091     _gc_tracer_stw-&gt;report_gc_end(_gc_timer_stw-&gt;gc_end(), _gc_timer_stw-&gt;time_partitions());
3092   }
3093   // It should now be safe to tell the concurrent mark thread to start
3094   // without its logging output interfering with the logging output
3095   // that came from the pause.
3096 
3097   if (should_start_conc_mark) {
3098     // CAUTION: after the doConcurrentMark() call below, the concurrent marking
3099     // thread(s) could be running concurrently with us. Make sure that anything
3100     // after this point does not assume that we are the only GC thread running.
3101     // Note: of course, the actual marking work will not start until the safepoint
3102     // itself is released in SuspendibleThreadSet::desynchronize().
3103     do_concurrent_mark();
3104     ConcurrentGCBreakpoints::notify_idle_to_active();
3105   }
3106 }
3107 
3108 void G1CollectedHeap::remove_self_forwarding_pointers(G1RedirtyCardsQueueSet* rdcqs) {
3109   G1ParRemoveSelfForwardPtrsTask rsfp_task(rdcqs);
3110   workers()-&gt;run_task(&amp;rsfp_task);
3111 }
3112 
3113 void G1CollectedHeap::restore_after_evac_failure(G1RedirtyCardsQueueSet* rdcqs) {
3114   double remove_self_forwards_start = os::elapsedTime();
3115 
3116   remove_self_forwarding_pointers(rdcqs);
3117   _preserved_marks_set.restore(workers());
3118 
3119   phase_times()-&gt;record_evac_fail_remove_self_forwards((os::elapsedTime() - remove_self_forwards_start) * 1000.0);
3120 }
3121 
3122 void G1CollectedHeap::preserve_mark_during_evac_failure(uint worker_id, oop obj, markWord m) {
3123   if (!_evacuation_failed) {
3124     _evacuation_failed = true;
3125   }
3126 
3127   _evacuation_failed_info_array[worker_id].register_copy_failure(obj-&gt;size());
3128   _preserved_marks_set.get(worker_id)-&gt;push_if_necessary(obj, m);
3129 }
3130 
3131 bool G1ParEvacuateFollowersClosure::offer_termination() {
3132   EventGCPhaseParallel event;
3133   G1ParScanThreadState* const pss = par_scan_state();
3134   start_term_time();
3135   const bool res = terminator()-&gt;offer_termination();
3136   end_term_time();
3137   event.commit(GCId::current(), pss-&gt;worker_id(), G1GCPhaseTimes::phase_name(G1GCPhaseTimes::Termination));
3138   return res;
3139 }
3140 
3141 void G1ParEvacuateFollowersClosure::do_void() {
3142   EventGCPhaseParallel event;
3143   G1ParScanThreadState* const pss = par_scan_state();
3144   pss-&gt;trim_queue();
3145   event.commit(GCId::current(), pss-&gt;worker_id(), G1GCPhaseTimes::phase_name(_phase));
3146   do {
3147     EventGCPhaseParallel event;
3148     pss-&gt;steal_and_trim_queue(queues());
3149     event.commit(GCId::current(), pss-&gt;worker_id(), G1GCPhaseTimes::phase_name(_phase));
3150   } while (!offer_termination());
3151 }
3152 
3153 void G1CollectedHeap::complete_cleaning(BoolObjectClosure* is_alive,
3154                                         bool class_unloading_occurred) {
3155   uint num_workers = workers()-&gt;active_workers();
3156   G1ParallelCleaningTask unlink_task(is_alive, num_workers, class_unloading_occurred, false);
3157   workers()-&gt;run_task(&amp;unlink_task);
3158 }
3159 
3160 // Clean string dedup data structures.
3161 // Ideally we would prefer to use a StringDedupCleaningTask here, but we want to
3162 // record the durations of the phases. Hence the almost-copy.
3163 class G1StringDedupCleaningTask : public AbstractGangTask {
3164   BoolObjectClosure* _is_alive;
3165   OopClosure* _keep_alive;
3166   G1GCPhaseTimes* _phase_times;
3167 
3168 public:
3169   G1StringDedupCleaningTask(BoolObjectClosure* is_alive,
3170                             OopClosure* keep_alive,
3171                             G1GCPhaseTimes* phase_times) :
3172     AbstractGangTask("Partial Cleaning Task"),
3173     _is_alive(is_alive),
3174     _keep_alive(keep_alive),
3175     _phase_times(phase_times)
3176   {
3177     assert(G1StringDedup::is_enabled(), "String deduplication disabled.");
3178     StringDedup::gc_prologue(true);
3179   }
3180 
3181   ~G1StringDedupCleaningTask() {
3182     StringDedup::gc_epilogue();
3183   }
3184 
3185   void work(uint worker_id) {
3186     StringDedupUnlinkOrOopsDoClosure cl(_is_alive, _keep_alive);
3187     {
3188       G1GCParPhaseTimesTracker x(_phase_times, G1GCPhaseTimes::StringDedupQueueFixup, worker_id);
3189       StringDedupQueue::unlink_or_oops_do(&amp;cl);
3190     }
3191     {
3192       G1GCParPhaseTimesTracker x(_phase_times, G1GCPhaseTimes::StringDedupTableFixup, worker_id);
3193       StringDedupTable::unlink_or_oops_do(&amp;cl, worker_id);
3194     }
3195   }
3196 };
3197 
3198 void G1CollectedHeap::string_dedup_cleaning(BoolObjectClosure* is_alive,
3199                                             OopClosure* keep_alive,
3200                                             G1GCPhaseTimes* phase_times) {
3201   G1StringDedupCleaningTask cl(is_alive, keep_alive, phase_times);
3202   workers()-&gt;run_task(&amp;cl);
3203 }
3204 
3205 class G1RedirtyLoggedCardsTask : public AbstractGangTask {
3206  private:
3207   G1RedirtyCardsQueueSet* _qset;
3208   G1CollectedHeap* _g1h;
3209   BufferNode* volatile _nodes;
3210 
3211   void par_apply(RedirtyLoggedCardTableEntryClosure* cl, uint worker_id) {
3212     size_t buffer_size = _qset-&gt;buffer_size();
3213     BufferNode* next = Atomic::load(&amp;_nodes);
3214     while (next != NULL) {
3215       BufferNode* node = next;
3216       next = Atomic::cmpxchg(&amp;_nodes, node, node-&gt;next());
3217       if (next == node) {
3218         cl-&gt;apply_to_buffer(node, buffer_size, worker_id);
3219         next = node-&gt;next();
3220       }
3221     }
3222   }
3223 
3224  public:
3225   G1RedirtyLoggedCardsTask(G1RedirtyCardsQueueSet* qset, G1CollectedHeap* g1h) :
3226     AbstractGangTask("Redirty Cards"),
3227     _qset(qset), _g1h(g1h), _nodes(qset-&gt;all_completed_buffers()) { }
3228 
3229   virtual void work(uint worker_id) {
3230     G1GCPhaseTimes* p = _g1h-&gt;phase_times();
3231     G1GCParPhaseTimesTracker x(p, G1GCPhaseTimes::RedirtyCards, worker_id);
3232 
3233     RedirtyLoggedCardTableEntryClosure cl(_g1h);
3234     par_apply(&amp;cl, worker_id);
3235 
3236     p-&gt;record_thread_work_item(G1GCPhaseTimes::RedirtyCards, worker_id, cl.num_dirtied());
3237   }
3238 };
3239 
3240 void G1CollectedHeap::redirty_logged_cards(G1RedirtyCardsQueueSet* rdcqs) {
3241   double redirty_logged_cards_start = os::elapsedTime();
3242 
3243   G1RedirtyLoggedCardsTask redirty_task(rdcqs, this);
3244   workers()-&gt;run_task(&amp;redirty_task);
3245 
3246   G1DirtyCardQueueSet&amp; dcq = G1BarrierSet::dirty_card_queue_set();
3247   dcq.merge_bufferlists(rdcqs);
3248 
3249   phase_times()-&gt;record_redirty_logged_cards_time_ms((os::elapsedTime() - redirty_logged_cards_start) * 1000.0);
3250 }
3251 
3252 // Weak Reference Processing support
3253 
3254 bool G1STWIsAliveClosure::do_object_b(oop p) {
3255   // An object is reachable if it is outside the collection set,
3256   // or is inside and copied.
3257   return !_g1h-&gt;is_in_cset(p) || p-&gt;is_forwarded();
3258 }
3259 
3260 bool G1STWSubjectToDiscoveryClosure::do_object_b(oop obj) {
3261   assert(obj != NULL, "must not be NULL");
3262   assert(_g1h-&gt;is_in_reserved(obj), "Trying to discover obj " PTR_FORMAT " not in heap", p2i(obj));
3263   // The areas the CM and STW ref processor manage must be disjoint. The is_in_cset() below
3264   // may falsely indicate that this is not the case here: however the collection set only
3265   // contains old regions when concurrent mark is not running.
3266   return _g1h-&gt;is_in_cset(obj) || _g1h-&gt;heap_region_containing(obj)-&gt;is_survivor();
3267 }
3268 
3269 // Non Copying Keep Alive closure
3270 class G1KeepAliveClosure: public OopClosure {
3271   G1CollectedHeap*_g1h;
3272 public:
3273   G1KeepAliveClosure(G1CollectedHeap* g1h) :_g1h(g1h) {}
3274   void do_oop(narrowOop* p) { guarantee(false, "Not needed"); }
3275   void do_oop(oop* p) {
3276     oop obj = *p;
3277     assert(obj != NULL, "the caller should have filtered out NULL values");
3278 
3279     const G1HeapRegionAttr region_attr =_g1h-&gt;region_attr(obj);
3280     if (!region_attr.is_in_cset_or_humongous()) {
3281       return;
3282     }
3283     if (region_attr.is_in_cset()) {
3284       assert( obj-&gt;is_forwarded(), "invariant" );
3285       *p = obj-&gt;forwardee();
3286     } else {
3287       assert(!obj-&gt;is_forwarded(), "invariant" );
3288       assert(region_attr.is_humongous(),
3289              "Only allowed G1HeapRegionAttr state is IsHumongous, but is %d", region_attr.type());
3290      _g1h-&gt;set_humongous_is_live(obj);
3291     }
3292   }
3293 };
3294 
3295 // Copying Keep Alive closure - can be called from both
3296 // serial and parallel code as long as different worker
3297 // threads utilize different G1ParScanThreadState instances
3298 // and different queues.
3299 
3300 class G1CopyingKeepAliveClosure: public OopClosure {
3301   G1CollectedHeap*         _g1h;
3302   G1ParScanThreadState*    _par_scan_state;
3303 
3304 public:
3305   G1CopyingKeepAliveClosure(G1CollectedHeap* g1h,
3306                             G1ParScanThreadState* pss):
3307     _g1h(g1h),
3308     _par_scan_state(pss)
3309   {}
3310 
3311   virtual void do_oop(narrowOop* p) { do_oop_work(p); }
3312   virtual void do_oop(      oop* p) { do_oop_work(p); }
3313 
3314   template &lt;class T&gt; void do_oop_work(T* p) {
3315     oop obj = RawAccess&lt;&gt;::oop_load(p);
3316 
3317     if (_g1h-&gt;is_in_cset_or_humongous(obj)) {
3318       // If the referent object has been forwarded (either copied
3319       // to a new location or to itself in the event of an
3320       // evacuation failure) then we need to update the reference
3321       // field and, if both reference and referent are in the G1
3322       // heap, update the RSet for the referent.
3323       //
3324       // If the referent has not been forwarded then we have to keep
3325       // it alive by policy. Therefore we have copy the referent.
3326       //
3327       // When the queue is drained (after each phase of reference processing)
3328       // the object and it's followers will be copied, the reference field set
3329       // to point to the new location, and the RSet updated.
3330       _par_scan_state-&gt;push_on_queue(ScannerTask(p));
3331     }
3332   }
3333 };
3334 
3335 // Serial drain queue closure. Called as the 'complete_gc'
3336 // closure for each discovered list in some of the
3337 // reference processing phases.
3338 
3339 class G1STWDrainQueueClosure: public VoidClosure {
3340 protected:
3341   G1CollectedHeap* _g1h;
3342   G1ParScanThreadState* _par_scan_state;
3343 
3344   G1ParScanThreadState*   par_scan_state() { return _par_scan_state; }
3345 
3346 public:
3347   G1STWDrainQueueClosure(G1CollectedHeap* g1h, G1ParScanThreadState* pss) :
3348     _g1h(g1h),
3349     _par_scan_state(pss)
3350   { }
3351 
3352   void do_void() {
3353     G1ParScanThreadState* const pss = par_scan_state();
3354     pss-&gt;trim_queue();
3355   }
3356 };
3357 
3358 // Parallel Reference Processing closures
3359 
3360 // Implementation of AbstractRefProcTaskExecutor for parallel reference
3361 // processing during G1 evacuation pauses.
3362 
3363 class G1STWRefProcTaskExecutor: public AbstractRefProcTaskExecutor {
3364 private:
3365   G1CollectedHeap*          _g1h;
3366   G1ParScanThreadStateSet*  _pss;
3367   G1ScannerTasksQueueSet*   _queues;
3368   WorkGang*                 _workers;
3369 
3370 public:
3371   G1STWRefProcTaskExecutor(G1CollectedHeap* g1h,
3372                            G1ParScanThreadStateSet* per_thread_states,
3373                            WorkGang* workers,
3374                            G1ScannerTasksQueueSet *task_queues) :
3375     _g1h(g1h),
3376     _pss(per_thread_states),
3377     _queues(task_queues),
3378     _workers(workers)
3379   {
3380     g1h-&gt;ref_processor_stw()-&gt;set_active_mt_degree(workers-&gt;active_workers());
3381   }
3382 
3383   // Executes the given task using concurrent marking worker threads.
3384   virtual void execute(ProcessTask&amp; task, uint ergo_workers);
3385 };
3386 
3387 // Gang task for possibly parallel reference processing
3388 
3389 class G1STWRefProcTaskProxy: public AbstractGangTask {
3390   typedef AbstractRefProcTaskExecutor::ProcessTask ProcessTask;
3391   ProcessTask&amp;     _proc_task;
3392   G1CollectedHeap* _g1h;
3393   G1ParScanThreadStateSet* _pss;
3394   G1ScannerTasksQueueSet* _task_queues;
3395   TaskTerminator* _terminator;
3396 
3397 public:
3398   G1STWRefProcTaskProxy(ProcessTask&amp; proc_task,
3399                         G1CollectedHeap* g1h,
3400                         G1ParScanThreadStateSet* per_thread_states,
3401                         G1ScannerTasksQueueSet *task_queues,
3402                         TaskTerminator* terminator) :
3403     AbstractGangTask("Process reference objects in parallel"),
3404     _proc_task(proc_task),
3405     _g1h(g1h),
3406     _pss(per_thread_states),
3407     _task_queues(task_queues),
3408     _terminator(terminator)
3409   {}
3410 
3411   virtual void work(uint worker_id) {
3412     // The reference processing task executed by a single worker.
3413     ResourceMark rm;
3414 
3415     G1STWIsAliveClosure is_alive(_g1h);
3416 
3417     G1ParScanThreadState* pss = _pss-&gt;state_for_worker(worker_id);
3418     pss-&gt;set_ref_discoverer(NULL);
3419 
3420     // Keep alive closure.
3421     G1CopyingKeepAliveClosure keep_alive(_g1h, pss);
3422 
3423     // Complete GC closure
3424     G1ParEvacuateFollowersClosure drain_queue(_g1h, pss, _task_queues, _terminator, G1GCPhaseTimes::ObjCopy);
3425 
3426     // Call the reference processing task's work routine.
3427     _proc_task.work(worker_id, is_alive, keep_alive, drain_queue);
3428 
3429     // Note we cannot assert that the refs array is empty here as not all
3430     // of the processing tasks (specifically phase2 - pp2_work) execute
3431     // the complete_gc closure (which ordinarily would drain the queue) so
3432     // the queue may not be empty.
3433   }
3434 };
3435 
3436 // Driver routine for parallel reference processing.
3437 // Creates an instance of the ref processing gang
3438 // task and has the worker threads execute it.
3439 void G1STWRefProcTaskExecutor::execute(ProcessTask&amp; proc_task, uint ergo_workers) {
3440   assert(_workers != NULL, "Need parallel worker threads.");
3441 
3442   assert(_workers-&gt;active_workers() &gt;= ergo_workers,
3443          "Ergonomically chosen workers (%u) should be less than or equal to active workers (%u)",
3444          ergo_workers, _workers-&gt;active_workers());
3445   TaskTerminator terminator(ergo_workers, _queues);
3446   G1STWRefProcTaskProxy proc_task_proxy(proc_task, _g1h, _pss, _queues, &amp;terminator);
3447 
3448   _workers-&gt;run_task(&amp;proc_task_proxy, ergo_workers);
3449 }
3450 
3451 // End of weak reference support closures
3452 
3453 void G1CollectedHeap::process_discovered_references(G1ParScanThreadStateSet* per_thread_states) {
3454   double ref_proc_start = os::elapsedTime();
3455 
3456   ReferenceProcessor* rp = _ref_processor_stw;
3457   assert(rp-&gt;discovery_enabled(), "should have been enabled");
3458 
3459   // Closure to test whether a referent is alive.
3460   G1STWIsAliveClosure is_alive(this);
3461 
3462   // Even when parallel reference processing is enabled, the processing
3463   // of JNI refs is serial and performed serially by the current thread
3464   // rather than by a worker. The following PSS will be used for processing
3465   // JNI refs.
3466 
3467   // Use only a single queue for this PSS.
3468   G1ParScanThreadState*          pss = per_thread_states-&gt;state_for_worker(0);
3469   pss-&gt;set_ref_discoverer(NULL);
3470   assert(pss-&gt;queue_is_empty(), "pre-condition");
3471 
3472   // Keep alive closure.
3473   G1CopyingKeepAliveClosure keep_alive(this, pss);
3474 
3475   // Serial Complete GC closure
3476   G1STWDrainQueueClosure drain_queue(this, pss);
3477 
3478   // Setup the soft refs policy...
3479   rp-&gt;setup_policy(false);
3480 
3481   ReferenceProcessorPhaseTimes* pt = phase_times()-&gt;ref_phase_times();
3482 
3483   ReferenceProcessorStats stats;
3484   if (!rp-&gt;processing_is_mt()) {
3485     // Serial reference processing...
3486     stats = rp-&gt;process_discovered_references(&amp;is_alive,
3487                                               &amp;keep_alive,
3488                                               &amp;drain_queue,
3489                                               NULL,
3490                                               pt);
3491   } else {
3492     uint no_of_gc_workers = workers()-&gt;active_workers();
3493 
3494     // Parallel reference processing
3495     assert(no_of_gc_workers &lt;= rp-&gt;max_num_queues(),
3496            "Mismatch between the number of GC workers %u and the maximum number of Reference process queues %u",
3497            no_of_gc_workers,  rp-&gt;max_num_queues());
3498 
3499     G1STWRefProcTaskExecutor par_task_executor(this, per_thread_states, workers(), _task_queues);
3500     stats = rp-&gt;process_discovered_references(&amp;is_alive,
3501                                               &amp;keep_alive,
3502                                               &amp;drain_queue,
3503                                               &amp;par_task_executor,
3504                                               pt);
3505   }
3506 
3507   _gc_tracer_stw-&gt;report_gc_reference_stats(stats);
3508 
3509   // We have completed copying any necessary live referent objects.
3510   assert(pss-&gt;queue_is_empty(), "both queue and overflow should be empty");
3511 
3512   make_pending_list_reachable();
3513 
3514   assert(!rp-&gt;discovery_enabled(), "Postcondition");
3515   rp-&gt;verify_no_references_recorded();
3516 
3517   double ref_proc_time = os::elapsedTime() - ref_proc_start;
3518   phase_times()-&gt;record_ref_proc_time(ref_proc_time * 1000.0);
3519 }
3520 
3521 void G1CollectedHeap::make_pending_list_reachable() {
3522   if (collector_state()-&gt;in_concurrent_start_gc()) {
3523     oop pll_head = Universe::reference_pending_list();
3524     if (pll_head != NULL) {
3525       // Any valid worker id is fine here as we are in the VM thread and single-threaded.
3526       _cm-&gt;mark_in_next_bitmap(0 /* worker_id */, pll_head);
3527     }
3528   }
3529 }
3530 
3531 void G1CollectedHeap::merge_per_thread_state_info(G1ParScanThreadStateSet* per_thread_states) {
3532   Ticks start = Ticks::now();
3533   per_thread_states-&gt;flush();
3534   phase_times()-&gt;record_or_add_time_secs(G1GCPhaseTimes::MergePSS, 0 /* worker_id */, (Ticks::now() - start).seconds());
3535 }
3536 
3537 class G1PrepareEvacuationTask : public AbstractGangTask {
3538   class G1PrepareRegionsClosure : public HeapRegionClosure {
3539     G1CollectedHeap* _g1h;
3540     G1PrepareEvacuationTask* _parent_task;
3541     size_t _worker_humongous_total;
3542     size_t _worker_humongous_candidates;
3543 
3544     bool humongous_region_is_candidate(HeapRegion* region) const {
3545       assert(region-&gt;is_starts_humongous(), "Must start a humongous object");
3546 
3547       oop obj = oop(region-&gt;bottom());
3548 
3549       // Dead objects cannot be eager reclaim candidates. Due to class
3550       // unloading it is unsafe to query their classes so we return early.
3551       if (_g1h-&gt;is_obj_dead(obj, region)) {
3552         return false;
3553       }
3554 
3555       // If we do not have a complete remembered set for the region, then we can
3556       // not be sure that we have all references to it.
3557       if (!region-&gt;rem_set()-&gt;is_complete()) {
3558         return false;
3559       }
3560       // Candidate selection must satisfy the following constraints
3561       // while concurrent marking is in progress:
3562       //
3563       // * In order to maintain SATB invariants, an object must not be
3564       // reclaimed if it was allocated before the start of marking and
3565       // has not had its references scanned.  Such an object must have
3566       // its references (including type metadata) scanned to ensure no
3567       // live objects are missed by the marking process.  Objects
3568       // allocated after the start of concurrent marking don't need to
3569       // be scanned.
3570       //
3571       // * An object must not be reclaimed if it is on the concurrent
3572       // mark stack.  Objects allocated after the start of concurrent
3573       // marking are never pushed on the mark stack.
3574       //
3575       // Nominating only objects allocated after the start of concurrent
3576       // marking is sufficient to meet both constraints.  This may miss
3577       // some objects that satisfy the constraints, but the marking data
3578       // structures don't support efficiently performing the needed
3579       // additional tests or scrubbing of the mark stack.
3580       //
3581       // However, we presently only nominate is_typeArray() objects.
3582       // A humongous object containing references induces remembered
3583       // set entries on other regions.  In order to reclaim such an
3584       // object, those remembered sets would need to be cleaned up.
3585       //
3586       // We also treat is_typeArray() objects specially, allowing them
3587       // to be reclaimed even if allocated before the start of
3588       // concurrent mark.  For this we rely on mark stack insertion to
3589       // exclude is_typeArray() objects, preventing reclaiming an object
3590       // that is in the mark stack.  We also rely on the metadata for
3591       // such objects to be built-in and so ensured to be kept live.
3592       // Frequent allocation and drop of large binary blobs is an
3593       // important use case for eager reclaim, and this special handling
3594       // may reduce needed headroom.
3595 
3596       return obj-&gt;is_typeArray() &amp;&amp;
3597              _g1h-&gt;is_potential_eager_reclaim_candidate(region);
3598     }
3599 
3600   public:
3601     G1PrepareRegionsClosure(G1CollectedHeap* g1h, G1PrepareEvacuationTask* parent_task) :
3602       _g1h(g1h),
3603       _parent_task(parent_task),
3604       _worker_humongous_total(0),
3605       _worker_humongous_candidates(0) { }
3606 
3607     ~G1PrepareRegionsClosure() {
3608       _parent_task-&gt;add_humongous_candidates(_worker_humongous_candidates);
3609       _parent_task-&gt;add_humongous_total(_worker_humongous_total);
3610     }
3611 
3612     virtual bool do_heap_region(HeapRegion* hr) {
3613       // First prepare the region for scanning
3614       _g1h-&gt;rem_set()-&gt;prepare_region_for_scan(hr);
3615 
3616       // Now check if region is a humongous candidate
3617       if (!hr-&gt;is_starts_humongous()) {
3618         _g1h-&gt;register_region_with_region_attr(hr);
3619         return false;
3620       }
3621 
3622       uint index = hr-&gt;hrm_index();
3623       if (humongous_region_is_candidate(hr)) {
3624         _g1h-&gt;set_humongous_reclaim_candidate(index, true);
3625         _g1h-&gt;register_humongous_region_with_region_attr(index);
3626         _worker_humongous_candidates++;
3627         // We will later handle the remembered sets of these regions.
3628       } else {
3629         _g1h-&gt;set_humongous_reclaim_candidate(index, false);
3630         _g1h-&gt;register_region_with_region_attr(hr);
3631       }
3632       _worker_humongous_total++;
3633 
3634       return false;
3635     }
3636   };
3637 
3638   G1CollectedHeap* _g1h;
3639   HeapRegionClaimer _claimer;
3640   volatile size_t _humongous_total;
3641   volatile size_t _humongous_candidates;
3642 public:
3643   G1PrepareEvacuationTask(G1CollectedHeap* g1h) :
3644     AbstractGangTask("Prepare Evacuation"),
3645     _g1h(g1h),
3646     _claimer(_g1h-&gt;workers()-&gt;active_workers()),
3647     _humongous_total(0),
3648     _humongous_candidates(0) { }
3649 
3650   ~G1PrepareEvacuationTask() {
3651     _g1h-&gt;set_has_humongous_reclaim_candidate(_humongous_candidates &gt; 0);
3652   }
3653 
3654   void work(uint worker_id) {
3655     G1PrepareRegionsClosure cl(_g1h, this);
3656     _g1h-&gt;heap_region_par_iterate_from_worker_offset(&amp;cl, &amp;_claimer, worker_id);
3657   }
3658 
3659   void add_humongous_candidates(size_t candidates) {
3660     Atomic::add(&amp;_humongous_candidates, candidates);
3661   }
3662 
3663   void add_humongous_total(size_t total) {
3664     Atomic::add(&amp;_humongous_total, total);
3665   }
3666 
3667   size_t humongous_candidates() {
3668     return _humongous_candidates;
3669   }
3670 
3671   size_t humongous_total() {
3672     return _humongous_total;
3673   }
3674 };
3675 
3676 void G1CollectedHeap::pre_evacuate_collection_set(G1EvacuationInfo&amp; evacuation_info, G1ParScanThreadStateSet* per_thread_states) {
3677   _bytes_used_during_gc = 0;
3678 
3679   _expand_heap_after_alloc_failure = true;
3680   _evacuation_failed = false;
3681 
3682   // Disable the hot card cache.
3683   _hot_card_cache-&gt;reset_hot_cache_claimed_index();
3684   _hot_card_cache-&gt;set_use_cache(false);
3685 
3686   // Initialize the GC alloc regions.
3687   _allocator-&gt;init_gc_alloc_regions(evacuation_info);
3688 
3689   {
3690     Ticks start = Ticks::now();
3691     rem_set()-&gt;prepare_for_scan_heap_roots();
3692     phase_times()-&gt;record_prepare_heap_roots_time_ms((Ticks::now() - start).seconds() * 1000.0);
3693   }
3694 
3695   {
3696     G1PrepareEvacuationTask g1_prep_task(this);
3697     Tickspan task_time = run_task(&amp;g1_prep_task);
3698 
3699     phase_times()-&gt;record_register_regions(task_time.seconds() * 1000.0,
3700                                            g1_prep_task.humongous_total(),
3701                                            g1_prep_task.humongous_candidates());
3702   }
3703 
3704   assert(_verifier-&gt;check_region_attr_table(), "Inconsistency in the region attributes table.");
3705   _preserved_marks_set.assert_empty();
3706 
3707 #if COMPILER2_OR_JVMCI
3708   DerivedPointerTable::clear();
3709 #endif
3710 
3711   // Concurrent start needs claim bits to keep track of the marked-through CLDs.
3712   if (collector_state()-&gt;in_concurrent_start_gc()) {
3713     concurrent_mark()-&gt;pre_concurrent_start();
3714 
3715     double start_clear_claimed_marks = os::elapsedTime();
3716 
3717     ClassLoaderDataGraph::clear_claimed_marks();
3718 
3719     double recorded_clear_claimed_marks_time_ms = (os::elapsedTime() - start_clear_claimed_marks) * 1000.0;
3720     phase_times()-&gt;record_clear_claimed_marks_time_ms(recorded_clear_claimed_marks_time_ms);
3721   }
3722 
3723   // Should G1EvacuationFailureALot be in effect for this GC?
3724   NOT_PRODUCT(set_evacuation_failure_alot_for_current_gc();)
3725 }
3726 
3727 class G1EvacuateRegionsBaseTask : public AbstractGangTask {
3728 protected:
3729   G1CollectedHeap* _g1h;
3730   G1ParScanThreadStateSet* _per_thread_states;
3731   G1ScannerTasksQueueSet* _task_queues;
3732   TaskTerminator _terminator;
3733   uint _num_workers;
3734 
3735   void evacuate_live_objects(G1ParScanThreadState* pss,
3736                              uint worker_id,
3737                              G1GCPhaseTimes::GCParPhases objcopy_phase,
3738                              G1GCPhaseTimes::GCParPhases termination_phase) {
3739     G1GCPhaseTimes* p = _g1h-&gt;phase_times();
3740 
3741     Ticks start = Ticks::now();
3742     G1ParEvacuateFollowersClosure cl(_g1h, pss, _task_queues, &amp;_terminator, objcopy_phase);
3743     cl.do_void();
3744 
3745     assert(pss-&gt;queue_is_empty(), "should be empty");
3746 
3747     Tickspan evac_time = (Ticks::now() - start);
3748     p-&gt;record_or_add_time_secs(objcopy_phase, worker_id, evac_time.seconds() - cl.term_time());
3749 
3750     if (termination_phase == G1GCPhaseTimes::Termination) {
3751       p-&gt;record_time_secs(termination_phase, worker_id, cl.term_time());
3752       p-&gt;record_thread_work_item(termination_phase, worker_id, cl.term_attempts());
3753     } else {
3754       p-&gt;record_or_add_time_secs(termination_phase, worker_id, cl.term_time());
3755       p-&gt;record_or_add_thread_work_item(termination_phase, worker_id, cl.term_attempts());
3756     }
3757     assert(pss-&gt;trim_ticks().seconds() == 0.0, "Unexpected partial trimming during evacuation");
3758   }
3759 
3760   virtual void start_work(uint worker_id) { }
3761 
3762   virtual void end_work(uint worker_id) { }
3763 
3764   virtual void scan_roots(G1ParScanThreadState* pss, uint worker_id) = 0;
3765 
3766   virtual void evacuate_live_objects(G1ParScanThreadState* pss, uint worker_id) = 0;
3767 
3768 public:
3769   G1EvacuateRegionsBaseTask(const char* name,
3770                             G1ParScanThreadStateSet* per_thread_states,
3771                             G1ScannerTasksQueueSet* task_queues,
3772                             uint num_workers) :
3773     AbstractGangTask(name),
3774     _g1h(G1CollectedHeap::heap()),
3775     _per_thread_states(per_thread_states),
3776     _task_queues(task_queues),
3777     _terminator(num_workers, _task_queues),
3778     _num_workers(num_workers)
3779   { }
3780 
3781   void work(uint worker_id) {
3782     start_work(worker_id);
3783 
3784     {
3785       ResourceMark rm;
3786 
3787       G1ParScanThreadState* pss = _per_thread_states-&gt;state_for_worker(worker_id);
3788       pss-&gt;set_ref_discoverer(_g1h-&gt;ref_processor_stw());
3789 
3790       scan_roots(pss, worker_id);
3791       evacuate_live_objects(pss, worker_id);
3792     }
3793 
3794     end_work(worker_id);
3795   }
3796 };
3797 
3798 class G1EvacuateRegionsTask : public G1EvacuateRegionsBaseTask {
3799   G1RootProcessor* _root_processor;
3800 
3801   void scan_roots(G1ParScanThreadState* pss, uint worker_id) {
3802     _root_processor-&gt;evacuate_roots(pss, worker_id);
3803     _g1h-&gt;rem_set()-&gt;scan_heap_roots(pss, worker_id, G1GCPhaseTimes::ScanHR, G1GCPhaseTimes::ObjCopy);
3804     _g1h-&gt;rem_set()-&gt;scan_collection_set_regions(pss, worker_id, G1GCPhaseTimes::ScanHR, G1GCPhaseTimes::CodeRoots, G1GCPhaseTimes::ObjCopy);
3805   }
3806 
3807   void evacuate_live_objects(G1ParScanThreadState* pss, uint worker_id) {
3808     G1EvacuateRegionsBaseTask::evacuate_live_objects(pss, worker_id, G1GCPhaseTimes::ObjCopy, G1GCPhaseTimes::Termination);
3809   }
3810 
3811   void start_work(uint worker_id) {
3812     _g1h-&gt;phase_times()-&gt;record_time_secs(G1GCPhaseTimes::GCWorkerStart, worker_id, Ticks::now().seconds());
3813   }
3814 
3815   void end_work(uint worker_id) {
3816     _g1h-&gt;phase_times()-&gt;record_time_secs(G1GCPhaseTimes::GCWorkerEnd, worker_id, Ticks::now().seconds());
3817   }
3818 
3819 public:
3820   G1EvacuateRegionsTask(G1CollectedHeap* g1h,
3821                         G1ParScanThreadStateSet* per_thread_states,
3822                         G1ScannerTasksQueueSet* task_queues,
3823                         G1RootProcessor* root_processor,
3824                         uint num_workers) :
3825     G1EvacuateRegionsBaseTask("G1 Evacuate Regions", per_thread_states, task_queues, num_workers),
3826     _root_processor(root_processor)
3827   { }
3828 };
3829 
3830 void G1CollectedHeap::evacuate_initial_collection_set(G1ParScanThreadStateSet* per_thread_states) {
3831   G1GCPhaseTimes* p = phase_times();
3832 
3833   {
3834     Ticks start = Ticks::now();
3835     rem_set()-&gt;merge_heap_roots(true /* initial_evacuation */);
3836     p-&gt;record_merge_heap_roots_time((Ticks::now() - start).seconds() * 1000.0);
3837   }
3838 
3839   Tickspan task_time;
3840   const uint num_workers = workers()-&gt;active_workers();
3841 
3842   Ticks start_processing = Ticks::now();
3843   {
3844     G1RootProcessor root_processor(this, num_workers);
3845     G1EvacuateRegionsTask g1_par_task(this, per_thread_states, _task_queues, &amp;root_processor, num_workers);
3846     task_time = run_task(&amp;g1_par_task);
3847     // Closing the inner scope will execute the destructor for the G1RootProcessor object.
3848     // To extract its code root fixup time we measure total time of this scope and
3849     // subtract from the time the WorkGang task took.
3850   }
3851   Tickspan total_processing = Ticks::now() - start_processing;
3852 
3853   p-&gt;record_initial_evac_time(task_time.seconds() * 1000.0);
3854   p-&gt;record_or_add_code_root_fixup_time((total_processing - task_time).seconds() * 1000.0);
3855 }
3856 
3857 class G1EvacuateOptionalRegionsTask : public G1EvacuateRegionsBaseTask {
3858 
3859   void scan_roots(G1ParScanThreadState* pss, uint worker_id) {
3860     _g1h-&gt;rem_set()-&gt;scan_heap_roots(pss, worker_id, G1GCPhaseTimes::OptScanHR, G1GCPhaseTimes::OptObjCopy);
3861     _g1h-&gt;rem_set()-&gt;scan_collection_set_regions(pss, worker_id, G1GCPhaseTimes::OptScanHR, G1GCPhaseTimes::OptCodeRoots, G1GCPhaseTimes::OptObjCopy);
3862   }
3863 
3864   void evacuate_live_objects(G1ParScanThreadState* pss, uint worker_id) {
3865     G1EvacuateRegionsBaseTask::evacuate_live_objects(pss, worker_id, G1GCPhaseTimes::OptObjCopy, G1GCPhaseTimes::OptTermination);
3866   }
3867 
3868 public:
3869   G1EvacuateOptionalRegionsTask(G1ParScanThreadStateSet* per_thread_states,
3870                                 G1ScannerTasksQueueSet* queues,
3871                                 uint num_workers) :
3872     G1EvacuateRegionsBaseTask("G1 Evacuate Optional Regions", per_thread_states, queues, num_workers) {
3873   }
3874 };
3875 
3876 void G1CollectedHeap::evacuate_next_optional_regions(G1ParScanThreadStateSet* per_thread_states) {
3877   class G1MarkScope : public MarkScope { };
3878 
3879   Tickspan task_time;
3880 
3881   Ticks start_processing = Ticks::now();
3882   {
3883     G1MarkScope code_mark_scope;
3884     G1EvacuateOptionalRegionsTask task(per_thread_states, _task_queues, workers()-&gt;active_workers());
3885     task_time = run_task(&amp;task);
3886     // See comment in evacuate_collection_set() for the reason of the scope.
3887   }
3888   Tickspan total_processing = Ticks::now() - start_processing;
3889 
3890   G1GCPhaseTimes* p = phase_times();
3891   p-&gt;record_or_add_code_root_fixup_time((total_processing - task_time).seconds() * 1000.0);
3892 }
3893 
3894 void G1CollectedHeap::evacuate_optional_collection_set(G1ParScanThreadStateSet* per_thread_states) {
3895   const double gc_start_time_ms = phase_times()-&gt;cur_collection_start_sec() * 1000.0;
3896 
3897   while (!evacuation_failed() &amp;&amp; _collection_set.optional_region_length() &gt; 0) {
3898 
3899     double time_used_ms = os::elapsedTime() * 1000.0 - gc_start_time_ms;
3900     double time_left_ms = MaxGCPauseMillis - time_used_ms;
3901 
3902     if (time_left_ms &lt; 0 ||
3903         !_collection_set.finalize_optional_for_evacuation(time_left_ms * policy()-&gt;optional_evacuation_fraction())) {
3904       log_trace(gc, ergo, cset)("Skipping evacuation of %u optional regions, no more regions can be evacuated in %.3fms",
3905                                 _collection_set.optional_region_length(), time_left_ms);
3906       break;
3907     }
3908 
3909     {
3910       Ticks start = Ticks::now();
3911       rem_set()-&gt;merge_heap_roots(false /* initial_evacuation */);
3912       phase_times()-&gt;record_or_add_optional_merge_heap_roots_time((Ticks::now() - start).seconds() * 1000.0);
3913     }
3914 
3915     {
3916       Ticks start = Ticks::now();
3917       evacuate_next_optional_regions(per_thread_states);
3918       phase_times()-&gt;record_or_add_optional_evac_time((Ticks::now() - start).seconds() * 1000.0);
3919     }
3920   }
3921 
3922   _collection_set.abandon_optional_collection_set(per_thread_states);
3923 }
3924 
3925 void G1CollectedHeap::post_evacuate_collection_set(G1EvacuationInfo&amp; evacuation_info,
3926                                                    G1RedirtyCardsQueueSet* rdcqs,
3927                                                    G1ParScanThreadStateSet* per_thread_states) {
3928   G1GCPhaseTimes* p = phase_times();
3929 
3930   rem_set()-&gt;cleanup_after_scan_heap_roots();
3931 
3932   // Process any discovered reference objects - we have
3933   // to do this _before_ we retire the GC alloc regions
3934   // as we may have to copy some 'reachable' referent
3935   // objects (and their reachable sub-graphs) that were
3936   // not copied during the pause.
3937   process_discovered_references(per_thread_states);
3938 
3939   G1STWIsAliveClosure is_alive(this);
3940   G1KeepAliveClosure keep_alive(this);
3941 
3942   WeakProcessor::weak_oops_do(workers(), &amp;is_alive, &amp;keep_alive, p-&gt;weak_phase_times());
3943 
3944   if (G1StringDedup::is_enabled()) {
3945     double string_dedup_time_ms = os::elapsedTime();
3946 
3947     string_dedup_cleaning(&amp;is_alive, &amp;keep_alive, p);
3948 
3949     double string_cleanup_time_ms = (os::elapsedTime() - string_dedup_time_ms) * 1000.0;
3950     p-&gt;record_string_deduplication_time(string_cleanup_time_ms);
3951   }
3952 
3953   _allocator-&gt;release_gc_alloc_regions(evacuation_info);
3954 
3955   if (evacuation_failed()) {
3956     restore_after_evac_failure(rdcqs);
3957 
3958     // Reset the G1EvacuationFailureALot counters and flags
3959     NOT_PRODUCT(reset_evacuation_should_fail();)
3960 
3961     double recalculate_used_start = os::elapsedTime();
3962     set_used(recalculate_used());
3963     p-&gt;record_evac_fail_recalc_used_time((os::elapsedTime() - recalculate_used_start) * 1000.0);
3964 
3965     if (_archive_allocator != NULL) {
3966       _archive_allocator-&gt;clear_used();
3967     }
3968     for (uint i = 0; i &lt; ParallelGCThreads; i++) {
3969       if (_evacuation_failed_info_array[i].has_failed()) {
3970         _gc_tracer_stw-&gt;report_evacuation_failed(_evacuation_failed_info_array[i]);
3971       }
3972     }
3973   } else {
3974     // The "used" of the the collection set have already been subtracted
3975     // when they were freed.  Add in the bytes used.
3976     increase_used(_bytes_used_during_gc);
3977   }
3978 
3979   _preserved_marks_set.assert_empty();
3980 
3981   merge_per_thread_state_info(per_thread_states);
3982 
3983   // Reset and re-enable the hot card cache.
3984   // Note the counts for the cards in the regions in the
3985   // collection set are reset when the collection set is freed.
3986   _hot_card_cache-&gt;reset_hot_cache();
3987   _hot_card_cache-&gt;set_use_cache(true);
3988 
3989   purge_code_root_memory();
3990 
3991   redirty_logged_cards(rdcqs);
3992 
3993   free_collection_set(&amp;_collection_set, evacuation_info, per_thread_states-&gt;surviving_young_words());
3994 
3995   eagerly_reclaim_humongous_regions();
3996 
3997   record_obj_copy_mem_stats();
3998 
3999   evacuation_info.set_collectionset_used_before(collection_set()-&gt;bytes_used_before());
4000   evacuation_info.set_bytes_used(_bytes_used_during_gc);
4001 
4002 #if COMPILER2_OR_JVMCI
4003   double start = os::elapsedTime();
4004   DerivedPointerTable::update_pointers();
4005   phase_times()-&gt;record_derived_pointer_table_update_time((os::elapsedTime() - start) * 1000.0);
4006 #endif
4007   policy()-&gt;print_age_table();
4008 }
4009 
4010 void G1CollectedHeap::record_obj_copy_mem_stats() {
4011   policy()-&gt;old_gen_alloc_tracker()-&gt;
4012     add_allocated_bytes_since_last_gc(_old_evac_stats.allocated() * HeapWordSize);
4013 
4014   _gc_tracer_stw-&gt;report_evacuation_statistics(create_g1_evac_summary(&amp;_survivor_evac_stats),
4015                                                create_g1_evac_summary(&amp;_old_evac_stats));
4016 }
4017 
4018 void G1CollectedHeap::free_region(HeapRegion* hr, FreeRegionList* free_list) {
4019   assert(!hr-&gt;is_free(), "the region should not be free");
4020   assert(!hr-&gt;is_empty(), "the region should not be empty");
4021   assert(_hrm-&gt;is_available(hr-&gt;hrm_index()), "region should be committed");
4022 
4023   if (G1VerifyBitmaps) {
4024     MemRegion mr(hr-&gt;bottom(), hr-&gt;end());
4025     concurrent_mark()-&gt;clear_range_in_prev_bitmap(mr);
4026   }
4027 
4028   // Clear the card counts for this region.
4029   // Note: we only need to do this if the region is not young
4030   // (since we don't refine cards in young regions).
4031   if (!hr-&gt;is_young()) {
4032     _hot_card_cache-&gt;reset_card_counts(hr);
4033   }
4034 
4035   // Reset region metadata to allow reuse.
4036   hr-&gt;hr_clear(true /* clear_space */);
4037   _policy-&gt;remset_tracker()-&gt;update_at_free(hr);
4038 
4039   if (free_list != NULL) {
4040     free_list-&gt;add_ordered(hr);
4041   }
4042 }
4043 
4044 void G1CollectedHeap::free_humongous_region(HeapRegion* hr,
4045                                             FreeRegionList* free_list) {
4046   assert(hr-&gt;is_humongous(), "this is only for humongous regions");
4047   assert(free_list != NULL, "pre-condition");
4048   hr-&gt;clear_humongous();
4049   free_region(hr, free_list);
4050 }
4051 
4052 void G1CollectedHeap::remove_from_old_sets(const uint old_regions_removed,
4053                                            const uint humongous_regions_removed) {
4054   if (old_regions_removed &gt; 0 || humongous_regions_removed &gt; 0) {
4055     MutexLocker x(OldSets_lock, Mutex::_no_safepoint_check_flag);
4056     _old_set.bulk_remove(old_regions_removed);
4057     _humongous_set.bulk_remove(humongous_regions_removed);
4058   }
4059 
4060 }
4061 
4062 void G1CollectedHeap::prepend_to_freelist(FreeRegionList* list) {
4063   assert(list != NULL, "list can't be null");
4064   if (!list-&gt;is_empty()) {
4065     MutexLocker x(FreeList_lock, Mutex::_no_safepoint_check_flag);
4066     _hrm-&gt;insert_list_into_free_list(list);
4067   }
4068 }
4069 
4070 void G1CollectedHeap::decrement_summary_bytes(size_t bytes) {
4071   decrease_used(bytes);
4072 }
4073 
4074 class G1FreeCollectionSetTask : public AbstractGangTask {
4075   // Helper class to keep statistics for the collection set freeing
4076   class FreeCSetStats {
4077     size_t _before_used_bytes;   // Usage in regions successfully evacutate
4078     size_t _after_used_bytes;    // Usage in regions failing evacuation
4079     size_t _bytes_allocated_in_old_since_last_gc; // Size of young regions turned into old
4080     size_t _failure_used_words;  // Live size in failed regions
4081     size_t _failure_waste_words; // Wasted size in failed regions
4082     size_t _rs_length;           // Remembered set size
4083     uint _regions_freed;         // Number of regions freed
4084   public:
4085     FreeCSetStats() :
4086         _before_used_bytes(0),
4087         _after_used_bytes(0),
4088         _bytes_allocated_in_old_since_last_gc(0),
4089         _failure_used_words(0),
4090         _failure_waste_words(0),
4091         _rs_length(0),
4092         _regions_freed(0) { }
4093 
4094     void merge_stats(FreeCSetStats* other) {
4095       assert(other != NULL, "invariant");
4096       _before_used_bytes += other-&gt;_before_used_bytes;
4097       _after_used_bytes += other-&gt;_after_used_bytes;
4098       _bytes_allocated_in_old_since_last_gc += other-&gt;_bytes_allocated_in_old_since_last_gc;
4099       _failure_used_words += other-&gt;_failure_used_words;
4100       _failure_waste_words += other-&gt;_failure_waste_words;
4101       _rs_length += other-&gt;_rs_length;
4102       _regions_freed += other-&gt;_regions_freed;
4103     }
4104 
4105     void report(G1CollectedHeap* g1h, G1EvacuationInfo* evacuation_info) {
4106       evacuation_info-&gt;set_regions_freed(_regions_freed);
4107       evacuation_info-&gt;increment_collectionset_used_after(_after_used_bytes);
4108 
4109       g1h-&gt;decrement_summary_bytes(_before_used_bytes);
4110       g1h-&gt;alloc_buffer_stats(G1HeapRegionAttr::Old)-&gt;add_failure_used_and_waste(_failure_used_words, _failure_waste_words);
4111 
4112       G1Policy *policy = g1h-&gt;policy();
4113       policy-&gt;old_gen_alloc_tracker()-&gt;add_allocated_bytes_since_last_gc(_bytes_allocated_in_old_since_last_gc);
4114       policy-&gt;record_rs_length(_rs_length);
4115       policy-&gt;cset_regions_freed();
4116     }
4117 
4118     void account_failed_region(HeapRegion* r) {
4119       size_t used_words = r-&gt;marked_bytes() / HeapWordSize;
4120       _failure_used_words += used_words;
4121       _failure_waste_words += HeapRegion::GrainWords - used_words;
4122       _after_used_bytes += r-&gt;used();
4123 
4124       // When moving a young gen region to old gen, we "allocate" that whole
4125       // region there. This is in addition to any already evacuated objects.
4126       // Notify the policy about that. Old gen regions do not cause an
4127       // additional allocation: both the objects still in the region and the
4128       // ones already moved are accounted for elsewhere.
4129       if (r-&gt;is_young()) {
4130         _bytes_allocated_in_old_since_last_gc += HeapRegion::GrainBytes;
4131       }
4132     }
4133 
4134     void account_evacuated_region(HeapRegion* r) {
4135       _before_used_bytes += r-&gt;used();
4136       _regions_freed += 1;
4137     }
4138 
4139     void account_rs_length(HeapRegion* r) {
4140       _rs_length += r-&gt;rem_set()-&gt;occupied();
4141     }
4142   };
4143 
4144   // Closure applied to all regions in the collection set.
4145   class FreeCSetClosure : public HeapRegionClosure {
4146     // Helper to send JFR events for regions.
4147     class JFREventForRegion {
4148       EventGCPhaseParallel _event;
4149     public:
4150       JFREventForRegion(HeapRegion* region, uint worker_id) : _event() {
4151         _event.set_gcId(GCId::current());
4152         _event.set_gcWorkerId(worker_id);
4153         if (region-&gt;is_young()) {
4154           _event.set_name(G1GCPhaseTimes::phase_name(G1GCPhaseTimes::YoungFreeCSet));
4155         } else {
4156           _event.set_name(G1GCPhaseTimes::phase_name(G1GCPhaseTimes::NonYoungFreeCSet));
4157         }
4158       }
4159 
4160       ~JFREventForRegion() {
4161         _event.commit();
4162       }
4163     };
4164 
4165     // Helper to do timing for region work.
4166     class TimerForRegion {
4167       Tickspan&amp; _time;
4168       Ticks     _start_time;
4169     public:
4170       TimerForRegion(Tickspan&amp; time) : _time(time), _start_time(Ticks::now()) { }
4171       ~TimerForRegion() {
4172         _time += Ticks::now() - _start_time;
4173       }
4174     };
4175 
4176     // FreeCSetClosure members
4177     G1CollectedHeap* _g1h;
4178     const size_t*    _surviving_young_words;
4179     uint             _worker_id;
4180     Tickspan         _young_time;
4181     Tickspan         _non_young_time;
4182     FreeCSetStats*   _stats;
4183 
4184     void assert_in_cset(HeapRegion* r) {
4185       assert(r-&gt;young_index_in_cset() != 0 &amp;&amp;
4186              (uint)r-&gt;young_index_in_cset() &lt;= _g1h-&gt;collection_set()-&gt;young_region_length(),
4187              "Young index %u is wrong for region %u of type %s with %u young regions",
4188              r-&gt;young_index_in_cset(), r-&gt;hrm_index(), r-&gt;get_type_str(), _g1h-&gt;collection_set()-&gt;young_region_length());
4189     }
4190 
4191     void handle_evacuated_region(HeapRegion* r) {
4192       assert(!r-&gt;is_empty(), "Region %u is an empty region in the collection set.", r-&gt;hrm_index());
4193       stats()-&gt;account_evacuated_region(r);
4194 
4195       // Free the region and and its remembered set.
4196       _g1h-&gt;free_region(r, NULL);
4197     }
4198 
4199     void handle_failed_region(HeapRegion* r) {
4200       // Do some allocation statistics accounting. Regions that failed evacuation
4201       // are always made old, so there is no need to update anything in the young
4202       // gen statistics, but we need to update old gen statistics.
4203       stats()-&gt;account_failed_region(r);
4204 
4205       // Update the region state due to the failed evacuation.
4206       r-&gt;handle_evacuation_failure();
4207 
4208       // Add region to old set, need to hold lock.
4209       MutexLocker x(OldSets_lock, Mutex::_no_safepoint_check_flag);
4210       _g1h-&gt;old_set_add(r);
4211     }
4212 
4213     Tickspan&amp; timer_for_region(HeapRegion* r) {
4214       return r-&gt;is_young() ? _young_time : _non_young_time;
4215     }
4216 
4217     FreeCSetStats* stats() {
4218       return _stats;
4219     }
4220   public:
4221     FreeCSetClosure(const size_t* surviving_young_words,
4222                     uint worker_id,
4223                     FreeCSetStats* stats) :
4224         HeapRegionClosure(),
4225         _g1h(G1CollectedHeap::heap()),
4226         _surviving_young_words(surviving_young_words),
4227         _worker_id(worker_id),
4228         _young_time(),
4229         _non_young_time(),
4230         _stats(stats) { }
4231 
4232     virtual bool do_heap_region(HeapRegion* r) {
4233       assert(r-&gt;in_collection_set(), "Invariant: %u missing from CSet", r-&gt;hrm_index());
4234       JFREventForRegion event(r, _worker_id);
4235       TimerForRegion timer(timer_for_region(r));
4236 
4237       _g1h-&gt;clear_region_attr(r);
4238       stats()-&gt;account_rs_length(r);
4239 
4240       if (r-&gt;is_young()) {
4241         assert_in_cset(r);
4242         r-&gt;record_surv_words_in_group(_surviving_young_words[r-&gt;young_index_in_cset()]);
4243       }
4244 
4245       if (r-&gt;evacuation_failed()) {
4246         handle_failed_region(r);
4247       } else {
4248         handle_evacuated_region(r);
4249       }
4250       assert(!_g1h-&gt;is_on_master_free_list(r), "sanity");
4251 
4252       return false;
4253     }
4254 
4255     void report_timing(Tickspan parallel_time) {
4256       G1GCPhaseTimes* pt = _g1h-&gt;phase_times();
4257       pt-&gt;record_time_secs(G1GCPhaseTimes::ParFreeCSet, _worker_id, parallel_time.seconds());
4258       if (_young_time.value() &gt; 0) {
4259         pt-&gt;record_time_secs(G1GCPhaseTimes::YoungFreeCSet, _worker_id, _young_time.seconds());
4260       }
4261       if (_non_young_time.value() &gt; 0) {
4262         pt-&gt;record_time_secs(G1GCPhaseTimes::NonYoungFreeCSet, _worker_id, _non_young_time.seconds());
4263       }
4264     }
4265   };
4266 
4267   // G1FreeCollectionSetTask members
4268   G1CollectedHeap*  _g1h;
4269   G1EvacuationInfo* _evacuation_info;
4270   FreeCSetStats*    _worker_stats;
4271   HeapRegionClaimer _claimer;
4272   const size_t*     _surviving_young_words;
4273   uint              _active_workers;
4274 
4275   FreeCSetStats* worker_stats(uint worker) {
4276     return &amp;_worker_stats[worker];
4277   }
4278 
4279   void report_statistics() {
4280     // Merge the accounting
4281     FreeCSetStats total_stats;
4282     for (uint worker = 0; worker &lt; _active_workers; worker++) {
4283       total_stats.merge_stats(worker_stats(worker));
4284     }
4285     total_stats.report(_g1h, _evacuation_info);
4286   }
4287 
4288 public:
4289   G1FreeCollectionSetTask(G1EvacuationInfo* evacuation_info, const size_t* surviving_young_words, uint active_workers) :
4290       AbstractGangTask("G1 Free Collection Set"),
4291       _g1h(G1CollectedHeap::heap()),
4292       _evacuation_info(evacuation_info),
4293       _worker_stats(NEW_C_HEAP_ARRAY(FreeCSetStats, active_workers, mtGC)),
4294       _claimer(active_workers),
4295       _surviving_young_words(surviving_young_words),
4296       _active_workers(active_workers) {
4297     for (uint worker = 0; worker &lt; active_workers; worker++) {
4298       ::new (&amp;_worker_stats[worker]) FreeCSetStats();
4299     }
4300   }
4301 
4302   ~G1FreeCollectionSetTask() {
4303     Ticks serial_time = Ticks::now();
4304     report_statistics();
4305     for (uint worker = 0; worker &lt; _active_workers; worker++) {
4306       _worker_stats[worker].~FreeCSetStats();
4307     }
4308     FREE_C_HEAP_ARRAY(FreeCSetStats, _worker_stats);
4309     _g1h-&gt;phase_times()-&gt;record_serial_free_cset_time_ms((Ticks::now() - serial_time).seconds() * 1000.0);
4310   }
4311 
4312   virtual void work(uint worker_id) {
4313     EventGCPhaseParallel event;
4314     Ticks start = Ticks::now();
4315     FreeCSetClosure cl(_surviving_young_words, worker_id, worker_stats(worker_id));
4316     _g1h-&gt;collection_set_par_iterate_all(&amp;cl, &amp;_claimer, worker_id);
4317 
4318     // Report the total parallel time along with some more detailed metrics.
4319     cl.report_timing(Ticks::now() - start);
4320     event.commit(GCId::current(), worker_id, G1GCPhaseTimes::phase_name(G1GCPhaseTimes::ParFreeCSet));
4321   }
4322 };
4323 
4324 void G1CollectedHeap::free_collection_set(G1CollectionSet* collection_set, G1EvacuationInfo&amp; evacuation_info, const size_t* surviving_young_words) {
4325   _eden.clear();
4326 
4327   // The free collections set is split up in two tasks, the first
4328   // frees the collection set and records what regions are free,
4329   // and the second one rebuilds the free list. This proved to be
4330   // more efficient than adding a sorted list to another.
4331 
4332   Ticks free_cset_start_time = Ticks::now();
4333   {
4334     uint const num_cs_regions = _collection_set.region_length();
4335     uint const num_workers = clamp(num_cs_regions, 1u, workers()-&gt;active_workers());
4336     G1FreeCollectionSetTask cl(&amp;evacuation_info, surviving_young_words, num_workers);
4337 
4338     log_debug(gc, ergo)("Running %s using %u workers for collection set length %u (%u)",
4339                         cl.name(), num_workers, num_cs_regions, num_regions());
4340     workers()-&gt;run_task(&amp;cl, num_workers);
4341   }
4342 
4343   Ticks free_cset_end_time = Ticks::now();
4344   phase_times()-&gt;record_total_free_cset_time_ms((free_cset_end_time - free_cset_start_time).seconds() * 1000.0);
4345 
4346   // Now rebuild the free region list.
4347   hrm()-&gt;rebuild_free_list(workers());
4348   phase_times()-&gt;record_total_rebuild_freelist_time_ms((Ticks::now() - free_cset_end_time).seconds() * 1000.0);
4349 
4350   collection_set-&gt;clear();
4351 }
4352 
4353 class G1FreeHumongousRegionClosure : public HeapRegionClosure {
4354  private:
4355   FreeRegionList* _free_region_list;
4356   HeapRegionSet* _proxy_set;
4357   uint _humongous_objects_reclaimed;
4358   uint _humongous_regions_reclaimed;
4359   size_t _freed_bytes;
4360  public:
4361 
4362   G1FreeHumongousRegionClosure(FreeRegionList* free_region_list) :
4363     _free_region_list(free_region_list), _proxy_set(NULL), _humongous_objects_reclaimed(0), _humongous_regions_reclaimed(0), _freed_bytes(0) {
4364   }
4365 
4366   virtual bool do_heap_region(HeapRegion* r) {
4367     if (!r-&gt;is_starts_humongous()) {
4368       return false;
4369     }
4370 
4371     G1CollectedHeap* g1h = G1CollectedHeap::heap();
4372 
4373     oop obj = (oop)r-&gt;bottom();
4374     G1CMBitMap* next_bitmap = g1h-&gt;concurrent_mark()-&gt;next_mark_bitmap();
4375 
4376     // The following checks whether the humongous object is live are sufficient.
4377     // The main additional check (in addition to having a reference from the roots
4378     // or the young gen) is whether the humongous object has a remembered set entry.
4379     //
4380     // A humongous object cannot be live if there is no remembered set for it
4381     // because:
4382     // - there can be no references from within humongous starts regions referencing
4383     // the object because we never allocate other objects into them.
4384     // (I.e. there are no intra-region references that may be missed by the
4385     // remembered set)
4386     // - as soon there is a remembered set entry to the humongous starts region
4387     // (i.e. it has "escaped" to an old object) this remembered set entry will stay
4388     // until the end of a concurrent mark.
4389     //
4390     // It is not required to check whether the object has been found dead by marking
4391     // or not, in fact it would prevent reclamation within a concurrent cycle, as
4392     // all objects allocated during that time are considered live.
4393     // SATB marking is even more conservative than the remembered set.
4394     // So if at this point in the collection there is no remembered set entry,
4395     // nobody has a reference to it.
4396     // At the start of collection we flush all refinement logs, and remembered sets
4397     // are completely up-to-date wrt to references to the humongous object.
4398     //
4399     // Other implementation considerations:
4400     // - never consider object arrays at this time because they would pose
4401     // considerable effort for cleaning up the the remembered sets. This is
4402     // required because stale remembered sets might reference locations that
4403     // are currently allocated into.
4404     uint region_idx = r-&gt;hrm_index();
4405     if (!g1h-&gt;is_humongous_reclaim_candidate(region_idx) ||
4406         !r-&gt;rem_set()-&gt;is_empty()) {
4407       log_debug(gc, humongous)("Live humongous region %u object size " SIZE_FORMAT " start " PTR_FORMAT "  with remset " SIZE_FORMAT " code roots " SIZE_FORMAT " is marked %d reclaim candidate %d type array %d",
4408                                region_idx,
4409                                (size_t)obj-&gt;size() * HeapWordSize,
4410                                p2i(r-&gt;bottom()),
4411                                r-&gt;rem_set()-&gt;occupied(),
4412                                r-&gt;rem_set()-&gt;strong_code_roots_list_length(),
4413                                next_bitmap-&gt;is_marked(r-&gt;bottom()),
4414                                g1h-&gt;is_humongous_reclaim_candidate(region_idx),
4415                                obj-&gt;is_typeArray()
4416                               );
4417       return false;
4418     }
4419 
4420     guarantee(obj-&gt;is_typeArray(),
4421               "Only eagerly reclaiming type arrays is supported, but the object "
4422               PTR_FORMAT " is not.", p2i(r-&gt;bottom()));
4423 
4424     log_debug(gc, humongous)("Dead humongous region %u object size " SIZE_FORMAT " start " PTR_FORMAT " with remset " SIZE_FORMAT " code roots " SIZE_FORMAT " is marked %d reclaim candidate %d type array %d",
4425                              region_idx,
4426                              (size_t)obj-&gt;size() * HeapWordSize,
4427                              p2i(r-&gt;bottom()),
4428                              r-&gt;rem_set()-&gt;occupied(),
4429                              r-&gt;rem_set()-&gt;strong_code_roots_list_length(),
4430                              next_bitmap-&gt;is_marked(r-&gt;bottom()),
4431                              g1h-&gt;is_humongous_reclaim_candidate(region_idx),
4432                              obj-&gt;is_typeArray()
4433                             );
4434 
4435     G1ConcurrentMark* const cm = g1h-&gt;concurrent_mark();
4436     cm-&gt;humongous_object_eagerly_reclaimed(r);
4437     assert(!cm-&gt;is_marked_in_prev_bitmap(obj) &amp;&amp; !cm-&gt;is_marked_in_next_bitmap(obj),
4438            "Eagerly reclaimed humongous region %u should not be marked at all but is in prev %s next %s",
4439            region_idx,
4440            BOOL_TO_STR(cm-&gt;is_marked_in_prev_bitmap(obj)),
4441            BOOL_TO_STR(cm-&gt;is_marked_in_next_bitmap(obj)));
4442     _humongous_objects_reclaimed++;
4443     do {
4444       HeapRegion* next = g1h-&gt;next_region_in_humongous(r);
4445       _freed_bytes += r-&gt;used();
4446       r-&gt;set_containing_set(NULL);
4447       _humongous_regions_reclaimed++;
4448       g1h-&gt;free_humongous_region(r, _free_region_list);
4449       r = next;
4450     } while (r != NULL);
4451 
4452     return false;
4453   }
4454 
4455   uint humongous_objects_reclaimed() {
4456     return _humongous_objects_reclaimed;
4457   }
4458 
4459   uint humongous_regions_reclaimed() {
4460     return _humongous_regions_reclaimed;
4461   }
4462 
4463   size_t bytes_freed() const {
4464     return _freed_bytes;
4465   }
4466 };
4467 
4468 void G1CollectedHeap::eagerly_reclaim_humongous_regions() {
4469   assert_at_safepoint_on_vm_thread();
4470 
4471   if (!G1EagerReclaimHumongousObjects ||
4472       (!_has_humongous_reclaim_candidates &amp;&amp; !log_is_enabled(Debug, gc, humongous))) {
4473     phase_times()-&gt;record_fast_reclaim_humongous_time_ms(0.0, 0);
4474     return;
4475   }
4476 
4477   double start_time = os::elapsedTime();
4478 
4479   FreeRegionList local_cleanup_list("Local Humongous Cleanup List");
4480 
4481   G1FreeHumongousRegionClosure cl(&amp;local_cleanup_list);
4482   heap_region_iterate(&amp;cl);
4483 
4484   remove_from_old_sets(0, cl.humongous_regions_reclaimed());
4485 
4486   G1HRPrinter* hrp = hr_printer();
4487   if (hrp-&gt;is_active()) {
4488     FreeRegionListIterator iter(&amp;local_cleanup_list);
4489     while (iter.more_available()) {
4490       HeapRegion* hr = iter.get_next();
4491       hrp-&gt;cleanup(hr);
4492     }
4493   }
4494 
4495   prepend_to_freelist(&amp;local_cleanup_list);
4496   decrement_summary_bytes(cl.bytes_freed());
4497 
4498   phase_times()-&gt;record_fast_reclaim_humongous_time_ms((os::elapsedTime() - start_time) * 1000.0,
4499                                                        cl.humongous_objects_reclaimed());
4500 }
4501 
4502 class G1AbandonCollectionSetClosure : public HeapRegionClosure {
4503 public:
4504   virtual bool do_heap_region(HeapRegion* r) {
4505     assert(r-&gt;in_collection_set(), "Region %u must have been in collection set", r-&gt;hrm_index());
4506     G1CollectedHeap::heap()-&gt;clear_region_attr(r);
4507     r-&gt;clear_young_index_in_cset();
4508     return false;
4509   }
4510 };
4511 
4512 void G1CollectedHeap::abandon_collection_set(G1CollectionSet* collection_set) {
4513   G1AbandonCollectionSetClosure cl;
4514   collection_set_iterate_all(&amp;cl);
4515 
4516   collection_set-&gt;clear();
4517   collection_set-&gt;stop_incremental_building();
4518 }
4519 
4520 bool G1CollectedHeap::is_old_gc_alloc_region(HeapRegion* hr) {
4521   return _allocator-&gt;is_retained_old_region(hr);
4522 }
4523 
4524 void G1CollectedHeap::set_region_short_lived_locked(HeapRegion* hr) {
4525   _eden.add(hr);
4526   _policy-&gt;set_region_eden(hr);
4527 }
4528 
4529 #ifdef ASSERT
4530 
4531 class NoYoungRegionsClosure: public HeapRegionClosure {
4532 private:
4533   bool _success;
4534 public:
4535   NoYoungRegionsClosure() : _success(true) { }
4536   bool do_heap_region(HeapRegion* r) {
4537     if (r-&gt;is_young()) {
4538       log_error(gc, verify)("Region [" PTR_FORMAT ", " PTR_FORMAT ") tagged as young",
4539                             p2i(r-&gt;bottom()), p2i(r-&gt;end()));
4540       _success = false;
4541     }
4542     return false;
4543   }
4544   bool success() { return _success; }
4545 };
4546 
4547 bool G1CollectedHeap::check_young_list_empty() {
4548   bool ret = (young_regions_count() == 0);
4549 
4550   NoYoungRegionsClosure closure;
4551   heap_region_iterate(&amp;closure);
4552   ret = ret &amp;&amp; closure.success();
4553 
4554   return ret;
4555 }
4556 
4557 #endif // ASSERT
4558 
4559 class TearDownRegionSetsClosure : public HeapRegionClosure {
4560   HeapRegionSet *_old_set;
4561 
4562 public:
4563   TearDownRegionSetsClosure(HeapRegionSet* old_set) : _old_set(old_set) { }
4564 
4565   bool do_heap_region(HeapRegion* r) {
4566     if (r-&gt;is_old()) {
4567       _old_set-&gt;remove(r);
4568     } else if(r-&gt;is_young()) {
4569       r-&gt;uninstall_surv_rate_group();
4570     } else {
4571       // We ignore free regions, we'll empty the free list afterwards.
4572       // We ignore humongous and archive regions, we're not tearing down these
4573       // sets.
4574       assert(r-&gt;is_archive() || r-&gt;is_free() || r-&gt;is_humongous(),
4575              "it cannot be another type");
4576     }
4577     return false;
4578   }
4579 
4580   ~TearDownRegionSetsClosure() {
4581     assert(_old_set-&gt;is_empty(), "post-condition");
4582   }
4583 };
4584 
4585 void G1CollectedHeap::tear_down_region_sets(bool free_list_only) {
4586   assert_at_safepoint_on_vm_thread();
4587 
4588   if (!free_list_only) {
4589     TearDownRegionSetsClosure cl(&amp;_old_set);
4590     heap_region_iterate(&amp;cl);
4591 
4592     // Note that emptying the _young_list is postponed and instead done as
4593     // the first step when rebuilding the regions sets again. The reason for
4594     // this is that during a full GC string deduplication needs to know if
4595     // a collected region was young or old when the full GC was initiated.
4596   }
4597   _hrm-&gt;remove_all_free_regions();
4598 }
4599 
4600 void G1CollectedHeap::increase_used(size_t bytes) {
4601   _summary_bytes_used += bytes;
4602 }
4603 
4604 void G1CollectedHeap::decrease_used(size_t bytes) {
4605   assert(_summary_bytes_used &gt;= bytes,
4606          "invariant: _summary_bytes_used: " SIZE_FORMAT " should be &gt;= bytes: " SIZE_FORMAT,
4607          _summary_bytes_used, bytes);
4608   _summary_bytes_used -= bytes;
4609 }
4610 
4611 void G1CollectedHeap::set_used(size_t bytes) {
4612   _summary_bytes_used = bytes;
4613 }
4614 
4615 class RebuildRegionSetsClosure : public HeapRegionClosure {
4616 private:
4617   bool _free_list_only;
4618 
4619   HeapRegionSet* _old_set;
4620   HeapRegionManager* _hrm;
4621 
4622   size_t _total_used;
4623 
4624 public:
4625   RebuildRegionSetsClosure(bool free_list_only,
4626                            HeapRegionSet* old_set,
4627                            HeapRegionManager* hrm) :
4628     _free_list_only(free_list_only),
4629     _old_set(old_set), _hrm(hrm), _total_used(0) {
4630     assert(_hrm-&gt;num_free_regions() == 0, "pre-condition");
4631     if (!free_list_only) {
4632       assert(_old_set-&gt;is_empty(), "pre-condition");
4633     }
4634   }
4635 
4636   bool do_heap_region(HeapRegion* r) {
4637     if (r-&gt;is_empty()) {
4638       assert(r-&gt;rem_set()-&gt;is_empty(), "Empty regions should have empty remembered sets.");
4639       // Add free regions to the free list
4640       r-&gt;set_free();
4641       _hrm-&gt;insert_into_free_list(r);
4642     } else if (!_free_list_only) {
4643       assert(r-&gt;rem_set()-&gt;is_empty(), "At this point remembered sets must have been cleared.");
4644 
4645       if (r-&gt;is_archive() || r-&gt;is_humongous()) {
4646         // We ignore archive and humongous regions. We left these sets unchanged.
4647       } else {
4648         assert(r-&gt;is_young() || r-&gt;is_free() || r-&gt;is_old(), "invariant");
4649         // We now move all (non-humongous, non-old, non-archive) regions to old gen, and register them as such.
4650         r-&gt;move_to_old();
4651         _old_set-&gt;add(r);
4652       }
4653       _total_used += r-&gt;used();
4654     }
4655 
4656     return false;
4657   }
4658 
4659   size_t total_used() {
4660     return _total_used;
4661   }
4662 };
4663 
4664 void G1CollectedHeap::rebuild_region_sets(bool free_list_only) {
4665   assert_at_safepoint_on_vm_thread();
4666 
4667   if (!free_list_only) {
4668     _eden.clear();
4669     _survivor.clear();
4670   }
4671 
4672   RebuildRegionSetsClosure cl(free_list_only, &amp;_old_set, _hrm);
4673   heap_region_iterate(&amp;cl);
4674 
4675   if (!free_list_only) {
4676     set_used(cl.total_used());
4677     if (_archive_allocator != NULL) {
4678       _archive_allocator-&gt;clear_used();
4679     }
4680   }
4681   assert_used_and_recalculate_used_equal(this);
4682 }
4683 
4684 // Methods for the mutator alloc region
4685 
4686 HeapRegion* G1CollectedHeap::new_mutator_alloc_region(size_t word_size,
4687                                                       bool force,
4688                                                       uint node_index) {
4689   assert_heap_locked_or_at_safepoint(true /* should_be_vm_thread */);
4690   bool should_allocate = policy()-&gt;should_allocate_mutator_region();
4691   if (force || should_allocate) {
4692     HeapRegion* new_alloc_region = new_region(word_size,
4693                                               HeapRegionType::Eden,
4694                                               false /* do_expand */,
4695                                               node_index);
4696     if (new_alloc_region != NULL) {
4697       set_region_short_lived_locked(new_alloc_region);
4698       _hr_printer.alloc(new_alloc_region, !should_allocate);
4699       _verifier-&gt;check_bitmaps("Mutator Region Allocation", new_alloc_region);
4700       _policy-&gt;remset_tracker()-&gt;update_at_allocate(new_alloc_region);
4701       return new_alloc_region;
4702     }
4703   }
4704   return NULL;
4705 }
4706 
4707 void G1CollectedHeap::retire_mutator_alloc_region(HeapRegion* alloc_region,
4708                                                   size_t allocated_bytes) {
4709   assert_heap_locked_or_at_safepoint(true /* should_be_vm_thread */);
4710   assert(alloc_region-&gt;is_eden(), "all mutator alloc regions should be eden");
4711 
4712   collection_set()-&gt;add_eden_region(alloc_region);
4713   increase_used(allocated_bytes);
4714   _eden.add_used_bytes(allocated_bytes);
4715   _hr_printer.retire(alloc_region);
4716 
4717   // We update the eden sizes here, when the region is retired,
4718   // instead of when it's allocated, since this is the point that its
4719   // used space has been recorded in _summary_bytes_used.
4720   g1mm()-&gt;update_eden_size();
4721 }
4722 
4723 // Methods for the GC alloc regions
4724 
4725 bool G1CollectedHeap::has_more_regions(G1HeapRegionAttr dest) {
4726   if (dest.is_old()) {
4727     return true;
4728   } else {
4729     return survivor_regions_count() &lt; policy()-&gt;max_survivor_regions();
4730   }
4731 }
4732 
4733 HeapRegion* G1CollectedHeap::new_gc_alloc_region(size_t word_size, G1HeapRegionAttr dest, uint node_index) {
4734   assert(FreeList_lock-&gt;owned_by_self(), "pre-condition");
4735 
4736   if (!has_more_regions(dest)) {
4737     return NULL;
4738   }
4739 
4740   HeapRegionType type;
4741   if (dest.is_young()) {
4742     type = HeapRegionType::Survivor;
4743   } else {
4744     type = HeapRegionType::Old;
4745   }
4746 
4747   HeapRegion* new_alloc_region = new_region(word_size,
4748                                             type,
4749                                             true /* do_expand */,
4750                                             node_index);
4751 
4752   if (new_alloc_region != NULL) {
4753     if (type.is_survivor()) {
4754       new_alloc_region-&gt;set_survivor();
4755       _survivor.add(new_alloc_region);
4756       _verifier-&gt;check_bitmaps("Survivor Region Allocation", new_alloc_region);
4757     } else {
4758       new_alloc_region-&gt;set_old();
4759       _verifier-&gt;check_bitmaps("Old Region Allocation", new_alloc_region);
4760     }
4761     _policy-&gt;remset_tracker()-&gt;update_at_allocate(new_alloc_region);
4762     register_region_with_region_attr(new_alloc_region);
4763     _hr_printer.alloc(new_alloc_region);
4764     return new_alloc_region;
4765   }
4766   return NULL;
4767 }
4768 
4769 void G1CollectedHeap::retire_gc_alloc_region(HeapRegion* alloc_region,
4770                                              size_t allocated_bytes,
4771                                              G1HeapRegionAttr dest) {
4772   _bytes_used_during_gc += allocated_bytes;
4773   if (dest.is_old()) {
4774     old_set_add(alloc_region);
4775   } else {
4776     assert(dest.is_young(), "Retiring alloc region should be young (%d)", dest.type());
4777     _survivor.add_used_bytes(allocated_bytes);
4778   }
4779 
4780   bool const during_im = collector_state()-&gt;in_concurrent_start_gc();
4781   if (during_im &amp;&amp; allocated_bytes &gt; 0) {
4782     _cm-&gt;root_regions()-&gt;add(alloc_region-&gt;next_top_at_mark_start(), alloc_region-&gt;top());
4783   }
4784   _hr_printer.retire(alloc_region);
4785 }
4786 
4787 HeapRegion* G1CollectedHeap::alloc_highest_free_region() {
4788   bool expanded = false;
4789   uint index = _hrm-&gt;find_highest_free(&amp;expanded);
4790 
4791   if (index != G1_NO_HRM_INDEX) {
4792     if (expanded) {
4793       log_debug(gc, ergo, heap)("Attempt heap expansion (requested address range outside heap bounds). region size: " SIZE_FORMAT "B",
4794                                 HeapRegion::GrainWords * HeapWordSize);
4795     }
4796     return _hrm-&gt;allocate_free_regions_starting_at(index, 1);
4797   }
4798   return NULL;
4799 }
4800 
4801 // Optimized nmethod scanning
4802 
4803 class RegisterNMethodOopClosure: public OopClosure {
4804   G1CollectedHeap* _g1h;
4805   nmethod* _nm;
4806 
4807   template &lt;class T&gt; void do_oop_work(T* p) {
4808     T heap_oop = RawAccess&lt;&gt;::oop_load(p);
4809     if (!CompressedOops::is_null(heap_oop)) {
4810       oop obj = CompressedOops::decode_not_null(heap_oop);
4811       HeapRegion* hr = _g1h-&gt;heap_region_containing(obj);
4812       assert(!hr-&gt;is_continues_humongous(),
4813              "trying to add code root " PTR_FORMAT " in continuation of humongous region " HR_FORMAT
4814              " starting at " HR_FORMAT,
4815              p2i(_nm), HR_FORMAT_PARAMS(hr), HR_FORMAT_PARAMS(hr-&gt;humongous_start_region()));
4816 
4817       // HeapRegion::add_strong_code_root_locked() avoids adding duplicate entries.
4818       hr-&gt;add_strong_code_root_locked(_nm);
4819     }
4820   }
4821 
4822 public:
4823   RegisterNMethodOopClosure(G1CollectedHeap* g1h, nmethod* nm) :
4824     _g1h(g1h), _nm(nm) {}
4825 
4826   void do_oop(oop* p)       { do_oop_work(p); }
4827   void do_oop(narrowOop* p) { do_oop_work(p); }
4828 };
4829 
4830 class UnregisterNMethodOopClosure: public OopClosure {
4831   G1CollectedHeap* _g1h;
4832   nmethod* _nm;
4833 
4834   template &lt;class T&gt; void do_oop_work(T* p) {
4835     T heap_oop = RawAccess&lt;&gt;::oop_load(p);
4836     if (!CompressedOops::is_null(heap_oop)) {
4837       oop obj = CompressedOops::decode_not_null(heap_oop);
4838       HeapRegion* hr = _g1h-&gt;heap_region_containing(obj);
4839       assert(!hr-&gt;is_continues_humongous(),
4840              "trying to remove code root " PTR_FORMAT " in continuation of humongous region " HR_FORMAT
4841              " starting at " HR_FORMAT,
4842              p2i(_nm), HR_FORMAT_PARAMS(hr), HR_FORMAT_PARAMS(hr-&gt;humongous_start_region()));
4843 
4844       hr-&gt;remove_strong_code_root(_nm);
4845     }
4846   }
4847 
4848 public:
4849   UnregisterNMethodOopClosure(G1CollectedHeap* g1h, nmethod* nm) :
4850     _g1h(g1h), _nm(nm) {}
4851 
4852   void do_oop(oop* p)       { do_oop_work(p); }
4853   void do_oop(narrowOop* p) { do_oop_work(p); }
4854 };
4855 
4856 void G1CollectedHeap::register_nmethod(nmethod* nm) {
4857   guarantee(nm != NULL, "sanity");
4858   RegisterNMethodOopClosure reg_cl(this, nm);
4859   nm-&gt;oops_do(&amp;reg_cl);
4860 }
4861 
4862 void G1CollectedHeap::unregister_nmethod(nmethod* nm) {
4863   guarantee(nm != NULL, "sanity");
4864   UnregisterNMethodOopClosure reg_cl(this, nm);
4865   nm-&gt;oops_do(&amp;reg_cl, true);
4866 }
4867 
4868 void G1CollectedHeap::purge_code_root_memory() {
4869   double purge_start = os::elapsedTime();
4870   G1CodeRootSet::purge();
4871   double purge_time_ms = (os::elapsedTime() - purge_start) * 1000.0;
4872   phase_times()-&gt;record_strong_code_root_purge_time(purge_time_ms);
4873 }
4874 
4875 class RebuildStrongCodeRootClosure: public CodeBlobClosure {
4876   G1CollectedHeap* _g1h;
4877 
4878 public:
4879   RebuildStrongCodeRootClosure(G1CollectedHeap* g1h) :
4880     _g1h(g1h) {}
4881 
4882   void do_code_blob(CodeBlob* cb) {
4883     nmethod* nm = (cb != NULL) ? cb-&gt;as_nmethod_or_null() : NULL;
4884     if (nm == NULL) {
4885       return;
4886     }
4887 
4888     _g1h-&gt;register_nmethod(nm);
4889   }
4890 };
4891 
4892 void G1CollectedHeap::rebuild_strong_code_roots() {
4893   RebuildStrongCodeRootClosure blob_cl(this);
4894   CodeCache::blobs_do(&amp;blob_cl);
4895 }
4896 
4897 void G1CollectedHeap::initialize_serviceability() {
4898   _g1mm-&gt;initialize_serviceability();
4899 }
4900 
4901 MemoryUsage G1CollectedHeap::memory_usage() {
4902   return _g1mm-&gt;memory_usage();
4903 }
4904 
4905 GrowableArray&lt;GCMemoryManager*&gt; G1CollectedHeap::memory_managers() {
4906   return _g1mm-&gt;memory_managers();
4907 }
4908 
4909 GrowableArray&lt;MemoryPool*&gt; G1CollectedHeap::memory_pools() {
4910   return _g1mm-&gt;memory_pools();
4911 }
<a name="2" id="anc2"></a><b style="font-size: large; color: red">--- EOF ---</b>















































































</pre><form name="eof"><input name="value" value="2" type="hidden" /></form></body></html>
